{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNe_RPhx3Cx"
   },
   "source": [
    "## Makemore Part 2: MLP\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Idea from paper: C is a lookup table (matrix) for embeddings vector of each of the words in V e.g |V|=17k\n",
    "- C: (17000x30)\n",
    "- one-hot encoding of a word: (1x17000)\n",
    "- enc @ C -> (1x30) embedding vector\n",
    "\n",
    "We can get these vectors for multiple input words (words that came before).\n",
    "\n",
    "Pass them to neurons with n_in=30, those neurons are fully connected to a hidden layer, then goes through tanh -> softmax for probabilities.\n",
    "\n",
    "Final output: 17k vector of probabilities for the 17k words in vocab.\n",
    "- Train by using actual next word's index, get the predicted prob, do -log(prob) and backprop etc.\n",
    "\n",
    "---\n",
    "\n",
    "We can use this idea for a character-level model as well.\n",
    "\n",
    "## Architecture\n",
    "- Start with indexes from the chars/words in context\n",
    "- Convert them to respective embeddings vectors in C (lookup)\n",
    "- Stack the vectors' values together, do forward pass with weights matrix and add bias\n",
    "- Activate through tanh\n",
    "- Pass through one more layer to get 27 outputs (one for each unique char in our vocab)\n",
    "- Softmax to get probabilities\n",
    "\n",
    "## Building dataset\n",
    "Hyperparameter: BLOCK_SIZE = 3\n",
    "- block_size is the number of previous chars we consider when predicting next\n",
    "\n",
    "For each word, we add to X with block_size=3 char windows, Y has the char that comes after\n",
    "- e.g '.emma': (..., e), (..e, m), (.em, m), (emm, a), (mma, .)\n",
    "- So each word contributes n+1 examples as before, n = len(word)\n",
    "\n",
    "## Lookup table\n",
    "Lookup table: C = (27,2) random init\n",
    "- In paper, they compress 17k words of vocab into Rn of n=30\n",
    "- So we do similar here for 27 unique chars -> embeddings of size 2\n",
    "\n",
    "Previously, we used one-hot encoding to lookup with enc @ W\n",
    "- But this is just the same as doing W[idx] due to all the zeroes\n",
    "\n",
    "In PyTorch, we can just do C[X] and it will work\n",
    "- produces (32,3,2) - one 2D vector for each encoded char\n",
    "- Or another way to think about it, one (3x2) vector for each row in X. 3 because BLOCK_SIZE=3, so each row in X has 3 elements. For each of those chars, we want one 2D vector - its embedding\n",
    "\n",
    "## F.cross_entropy\n",
    "F.cross_entropy(logits, targets) is the same as doing:\n",
    "\n",
    "```python\n",
    "logits = logits.softmax(dim=1)\n",
    "logits = logits[torch.arange(M), Y]\n",
    "nll_loss = -logits.prob().mean()\n",
    "```\n",
    "i.e same as softmax -> select corresponding probabilities for targets in Y -> get NLL Loss (mean)\n",
    "\n",
    "F.cross_entropy is **better** because PyTorch can optimise and not create new memory, and it can use **fused kernels** to cluster ops together and run them at the same time.\n",
    "- Also more numerically stable. logits can be subtracted or added by any number. when logits are too high, e^(high number) becomes inf, then we get nans. But PyTorch can internally subtract the data by the max number to prevent this\n",
    "\n",
    "## Batching\n",
    "Instead of fwd + backward on whole dataset which is slow, we can pick a random batch each time and fwd + backward on that.\n",
    "Just select BATCH_SIZE of random indices within [0,M), then use X[batch_indices] and Y[batch_indices]\n",
    "\n",
    "## Finding good learning rate\n",
    "1. Find min and max bounds by trial and error\n",
    "- Set very low LR, low is if it barely changes\n",
    "- High is if loss grows\n",
    "2. Create torch.linspace between low and high using exponents for the lrs e.g -3, 0 for 10^-3 -> 10^0=1\n",
    "3. During loop, ith iteration uses ith LR in list for the batch. Track the loss for that LR\n",
    "4. Graph it and find the valley in the graph - the middle should be a good LR.\n",
    "\n",
    "## Train, test, validation split (80,10,10)\n",
    "Train split: Used to optimise model parameters with gradient descent\n",
    "\n",
    "Validation split: Used to tune hyperparameters: e.g outputs in hidden layers, size of embeddings in C\n",
    "  \n",
    "Test split: Used to evaluate the performance of the model at the **end**\n",
    "- Why: achieving low loss on the same set we used to train isn't necessarily a good thing. Because when we use bigger models, they are able to just memorise the dataset, then doesn't generalise to new inputs well.\n",
    "\n",
    "Training: we tune hyperparameters by checking repeatedly on dev split, then at the end check loss against test split just one time and this is the final number we report.\n",
    "\n",
    "## Overfitting, underfitting\n",
    "Underfitting: when train loss and dev or test loss are both high and close to each other\n",
    "- Means the model isn't big enough / powerful enough to fit the data\n",
    "\n",
    "Overfitting: when train loss is low, but dev/test loss are much higher\n",
    "- Means the model is memorising the training data but doesn't generalise well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "executionInfo": {
     "elapsed": 10072,
     "status": "ok",
     "timestamp": 1727498203827,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "E1zlN5r9xtTz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl1lLlcpJtBM"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1727488190995,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "INrlJJdu70z3",
    "outputId": "4fa9ff58-4349-46f0-fbb5-5b7f861df269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 32033\n",
      "['yuheng', 'diondre', 'xavien', 'jori', 'juanluis']\n",
      "25626 28829\n"
     ]
    }
   ],
   "source": [
    "# Open up names\n",
    "names = []\n",
    "with open('names.txt', 'r') as names_file:\n",
    "  names = names_file.read().splitlines()\n",
    "\n",
    "print(\"Total length:\", len(names))\n",
    "\n",
    "# build lookups\n",
    "uniq = ['.'] + sorted(list(set(''.join(names))))\n",
    "stoi = { char: idx for idx, char in enumerate(uniq)}\n",
    "itos = { idx: char for char,idx in stoi.items() }\n",
    "\n",
    "# Hyperparameters\n",
    "BLOCK_SIZE = 3\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "def build_dataset(names):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in names:\n",
    "      # print(word)\n",
    "      word = word + '.'\n",
    "      block = [0] * BLOCK_SIZE # ... (empty context at start)\n",
    "    \n",
    "      for char in word:\n",
    "        char_idx = stoi[char]\n",
    "        X.append(block)\n",
    "        Y.append(char_idx)\n",
    "    \n",
    "        block_str = ''.join(list(map(lambda i: itos[i], block)))\n",
    "        # print(f'{block_str} -> {char}')\n",
    "    \n",
    "        # update block: roll over sliding window\n",
    "        block = block[1:] + [char_idx]\n",
    "\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# train, test, dev split: 80,10,10\n",
    "from random import Random\n",
    "Random(42).shuffle(names)\n",
    "\n",
    "print(names[0:5])\n",
    "\n",
    "n_80 = int(0.8*len(names))\n",
    "n_90 = int(0.9*len(names))\n",
    "print(n_80, n_90)\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n_80]) # 80%\n",
    "Xdev, Ydev = build_dataset(names[n_80:n_90]) # 10%\n",
    "Xtest, Ytest = build_dataset(names[n_90:]) # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228146\n",
      "0.8004742577121667\n",
      "0.09930044795876325\n",
      "0.10022529432906999\n",
      "182625 22655 22866\n"
     ]
    }
   ],
   "source": [
    "total = len(Xtr) + len(Xdev) + len(Xtest)\n",
    "print(total)\n",
    "print(len(Xtr) / total)\n",
    "print(len(Xdev) / total)\n",
    "print(len(Xtest) / total)\n",
    "print(len(Xtr) , len(Xdev), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1727489615781,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "vUo1EJwqJzs0",
    "outputId": "b6926d3a-389f-4be1-9eae-bf58b6f6b454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625]) torch.int64 torch.int64\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 25],\n",
      "        [ 0, 25, 21],\n",
      "        ...,\n",
      "        [15, 12,  4],\n",
      "        [12,  4,  1],\n",
      "        [ 4,  1, 14]])\n",
      "---\n",
      "tensor([25, 21,  8,  ...,  1, 14,  0])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Ytr.shape, Xtr.dtype, Ytr.dtype)\n",
    "print(Xtr)\n",
    "print(\"---\")\n",
    "print(Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_c8JD9Z8Bd"
   },
   "source": [
    "## Lookup table C, embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1727490140478,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "J7DEHynvMtq7",
    "outputId": "f79ae109-2ddb-40ef-895e-6553c96d2fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5013, -0.8621,  0.1141, -0.4797,  1.0472,  1.5424, -1.2521, -1.6304,\n",
      "          0.9521, -0.3236],\n",
      "        [-0.5833, -0.0969, -0.5720,  1.8011, -0.0412, -0.1098, -0.3862, -0.5910,\n",
      "          0.3759,  0.5625],\n",
      "        [ 1.4190, -0.6763,  1.7523,  0.7620,  1.0095,  0.6250,  1.4192,  0.3493,\n",
      "         -0.8872,  0.1808],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -1.5889,  0.9987,  0.3235,  0.5826,\n",
      "         -1.2369, -0.5786],\n",
      "        [-0.9113,  1.4787,  0.3230, -1.6748, -0.3587,  0.8224, -0.5762, -0.1008,\n",
      "          0.6782, -0.5032],\n",
      "        [ 0.0545, -0.4910, -1.7107, -0.1619,  1.3593, -0.4439,  1.0834,  0.0320,\n",
      "         -0.5812,  0.0108],\n",
      "        [-0.0482, -0.1170, -0.0892,  0.2336, -0.4153,  0.0076,  0.7098,  0.6353,\n",
      "          0.2616, -1.0070],\n",
      "        [-0.7181,  0.8360, -0.9140, -0.3356,  0.6749, -0.6606,  0.0102, -0.3768,\n",
      "         -0.3413,  1.1586],\n",
      "        [-0.5271, -1.4327,  0.5918,  1.5556, -0.8974,  0.6933,  1.5235, -0.0697,\n",
      "          1.2950, -1.4680],\n",
      "        [ 0.0252,  0.8468,  0.1797, -1.2906,  0.7836,  0.3613,  0.0547,  1.3996,\n",
      "         -0.2099,  1.9860],\n",
      "        [ 0.1808,  0.3776, -0.8380,  1.0456, -0.9487,  0.4099,  0.3002,  0.3258,\n",
      "         -1.2162, -0.9100],\n",
      "        [-1.2349, -0.6287,  0.2815,  1.0406,  0.0178,  0.1483,  1.0566,  0.2054,\n",
      "         -0.3947, -0.1226],\n",
      "        [-0.2265,  0.1289, -0.4223,  0.8507,  0.0965, -1.7318,  0.3284, -1.7837,\n",
      "         -1.1700, -0.1490],\n",
      "        [-0.0549,  0.3628, -0.5278, -0.4945, -0.2542, -0.1471, -0.8297, -0.7266,\n",
      "         -0.4838, -1.7872],\n",
      "        [ 0.3676, -0.8236,  2.4072, -1.0029,  0.8090,  1.9090, -1.2104, -0.3956,\n",
      "          0.8433,  0.3986],\n",
      "        [-0.4483,  0.1495, -1.5291,  1.0313,  1.2232,  0.3802, -0.0795, -0.6631,\n",
      "          0.0809,  2.3321],\n",
      "        [ 0.8383,  0.8010, -0.4180,  0.3827, -0.6356, -1.2418, -2.5622,  0.4679,\n",
      "          0.3027, -1.5873],\n",
      "        [ 1.3803, -0.9970,  0.1950, -0.7574,  2.2395, -1.2319, -1.8203, -1.7171,\n",
      "          0.5226,  1.5027],\n",
      "        [ 2.1024,  0.3466, -0.6218, -0.9768,  0.5534, -1.1111, -0.8860,  0.7296,\n",
      "         -1.8385,  0.6464],\n",
      "        [-0.2872,  0.8906, -0.3067,  0.7088, -1.8585,  0.4963, -1.0555, -0.3514,\n",
      "         -1.6551, -0.5993],\n",
      "        [ 0.1360,  0.1325, -0.0439,  0.5588,  1.4310, -0.6275, -0.5256, -0.3855,\n",
      "         -1.4038, -0.1902],\n",
      "        [-0.9724, -2.4353, -0.8207,  0.7638, -0.6430, -1.6277,  0.6287,  0.2157,\n",
      "         -0.7200,  1.8399],\n",
      "        [ 0.9315,  0.6752, -0.4389, -0.6038,  0.4971,  1.3256,  0.2866,  0.2505,\n",
      "         -0.4102, -0.3827],\n",
      "        [-0.0496, -0.3635, -0.0551, -0.6078, -0.0129,  1.4252, -0.0224,  0.8542,\n",
      "         -1.8022,  0.0875],\n",
      "        [-1.1542,  0.0965,  0.5871, -2.1649, -0.5822,  0.8462,  1.0826, -0.5823,\n",
      "         -0.5806, -0.6051],\n",
      "        [ 0.7079, -1.1227, -1.5542, -0.9094, -0.9417, -0.3635, -1.0071, -0.3720,\n",
      "         -0.4000,  0.2393],\n",
      "        [ 0.8201,  0.7582,  0.3197,  0.6152, -0.1345,  0.7046,  0.1137,  0.5721,\n",
      "         -0.8211, -0.3797]]) \n",
      "------\n",
      "\n",
      "Embeddings:\n",
      "torch.Size([182625, 3, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393],\n",
       "         [-0.9724, -2.4353, -0.8207,  ...,  0.2157, -0.7200,  1.8399]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4483,  0.1495, -1.5291,  ..., -0.6631,  0.0809,  2.3321],\n",
       "         [-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032]],\n",
       "\n",
       "        [[-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625]],\n",
       "\n",
       "        [[-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625],\n",
       "         [ 0.3676, -0.8236,  2.4072,  ..., -0.3956,  0.8433,  0.3986]]])"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(1097)\n",
    "\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "print(C, \"\\n------\\n\")\n",
    "\n",
    "# Embedding\n",
    "    # (32,3,2)\n",
    "emb = C[Xtr]\n",
    "\n",
    "# For each size 3 block in X (each row), we get a (3x2) matrix which has all the 2D embeddings for the chars that make up the block\n",
    "  # e.g X[2] = [0,5,13], so emb[2] is [C[0], C[5], C[13]] stacked vertically\n",
    "# print(emb[2])\n",
    "# example = torch.cat((C[0], C[5], C[13]), dim=0).view((BLOCK_SIZE ,EMBEDDING_SIZE))\n",
    "# same as emb[2]\n",
    "# print(example)\n",
    "# print(\"\\n------\\n\")\n",
    "\n",
    "\n",
    "print(\"Embeddings:\")\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck__BTsebPl5"
   },
   "source": [
    "## Different ways to flatten\n",
    "(32,3,2) -> (32,6) so we can keep W to (6, 100)\n",
    "- 3x2=6 determined by block size and embedding size\n",
    "\n",
    "torch.cat, torch.unbind creates new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1727490193315,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "5TGsS6o-Q2HD",
    "outputId": "48929d69-be45-45d1-8069-373cc42e1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor([[ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621, -0.5833, -0.0969],\n",
      "        [ 0.5013, -0.8621, -0.5833, -0.0969,  1.4192,  0.3493],\n",
      "        [-0.5833, -0.0969,  1.4192,  0.3493,  1.4192,  0.3493],\n",
      "        [ 1.4192,  0.3493,  1.4192,  0.3493,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.9260,  0.9209],\n",
      "        [ 0.5013, -0.8621,  1.9260,  0.9209,  1.0095,  0.6250],\n",
      "        [ 1.9260,  0.9209,  1.0095,  0.6250,  0.3759,  0.5625],\n",
      "        [ 1.0095,  0.6250,  0.3759,  0.5625,  1.1586, -1.2548],\n",
      "        [ 0.3759,  0.5625,  1.1586, -1.2548,  0.3759,  0.5625],\n",
      "        [ 1.1586, -1.2548,  0.3759,  0.5625,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.1141, -0.4797,  1.1586, -1.2548],\n",
      "        [ 0.1141, -0.4797,  1.1586, -1.2548,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.3759,  0.5625],\n",
      "        [ 0.5013, -0.8621,  0.3759,  0.5625,  1.3593, -0.4439],\n",
      "        [ 0.3759,  0.5625,  1.3593, -0.4439,  0.1141, -0.4797],\n",
      "        [ 1.3593, -0.4439,  0.1141, -0.4797,  1.0472,  1.5424],\n",
      "        [ 0.1141, -0.4797,  1.0472,  1.5424, -0.5833, -0.0969],\n",
      "        [ 1.0472,  1.5424, -0.5833, -0.0969,  1.0095,  0.6250],\n",
      "        [-0.5833, -0.0969,  1.0095,  0.6250,  1.0095,  0.6250],\n",
      "        [ 1.0095,  0.6250,  1.0095,  0.6250,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.3593, -0.4439],\n",
      "        [ 0.5013, -0.8621,  1.3593, -0.4439,  1.9260,  0.9209],\n",
      "        [ 1.3593, -0.4439,  1.9260,  0.9209, -0.6208, -2.2271],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -0.3862, -0.5910],\n",
      "        [-0.6208, -2.2271, -0.3862, -0.5910,  0.3759,  0.5625],\n",
      "        [-0.3862, -0.5910,  0.3759,  0.5625,  0.1141, -0.4797]])\n"
     ]
    }
   ],
   "source": [
    "### with unbind + cat\n",
    "  # unbind: removes a dimension and returns tuple of each slice\n",
    "  # e.g dim=1, so we slice along 0,1,2 for emb\n",
    "  # get: emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], ...\n",
    "# cat: concatenate\n",
    "sliced = torch.unbind(emb, dim=1)\n",
    "cat_ver = torch.cat(sliced , dim=1)\n",
    "\n",
    "# 32x3x2\n",
    "    # flatten by dim=(1,2), meaning flatten the (3x2) -> 6 each\n",
    "flattened = torch.flatten(emb, 1, 2)\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "emb_view = emb.view((32, BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# all of the above are equivalent\n",
    "print(cat_ver.logical_and(flattened).flatten().all())\n",
    "print(flattened.logical_and(cat_ver).flatten().all())\n",
    "\n",
    "# [C[0], C[0], C[0]]\n",
    "# [C[0], C[0], C[5]]\n",
    "# [C[0], C[5], C[13]]\n",
    "# ...and so on\n",
    "    # we are just putting the embeddings per example into one row, before it was 3 rows\n",
    "print(emb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "In the matrix multiplication emb_view @ W, each row of emb_view represents a flattened embedding (e.g 6 values for a context of size 3, with each character having 2D embeddings). \n",
    "\n",
    "The matrix W has e.g 6 rows (one for each input value) and W_OUT columns (or more, depending on the number of neurons). Each column in W represents the weights for one neuron. \n",
    "\n",
    "The result of the multiplication is a matrix where each row corresponds to the linear combination (dot product) of the input embeddings with each neuron's weights. \n",
    "\n",
    "Bias\n",
    "- Each neuron - column of W - has its own bias\n",
    "- So bias also has W_OUT columns for the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: tensor([ 2.0187e-01,  1.4710e+00,  1.9650e+00,  9.3848e-02, -2.3441e-01,\n",
      "        -4.7062e-01,  1.7981e+00,  1.3198e-01,  2.7150e+00, -7.4946e-01,\n",
      "        -5.1111e-01,  3.9081e-01,  1.6414e+00, -8.1366e-01, -9.3341e-01,\n",
      "        -7.3788e-01,  8.5509e-02, -1.7808e+00,  1.1113e+00, -5.5110e-01,\n",
      "         5.0704e-01, -6.7913e-01, -1.0101e+00,  9.1796e-01,  7.6946e-01,\n",
      "        -4.4623e-01,  1.2764e+00, -1.2066e-01, -4.7289e-01, -1.0512e+00,\n",
      "        -4.3958e-03,  1.6660e-01,  2.3627e+00, -2.6778e-01,  1.3920e+00,\n",
      "        -2.6669e-02, -8.4985e-01,  1.5683e-01,  5.6315e-01,  4.2754e-01,\n",
      "         5.4978e-01,  6.3640e-01,  3.1889e-01, -6.2480e-01, -4.2308e-01,\n",
      "        -1.9012e+00,  1.8052e-01,  7.1911e-01,  1.6683e+00,  2.8072e+00,\n",
      "         2.7626e-01, -5.8916e-01,  7.7466e-02, -5.5283e-01,  1.4628e+00,\n",
      "        -1.7897e-02,  5.1046e-02,  1.7784e+00, -4.5596e-01, -5.8732e-01,\n",
      "         5.9618e-01,  1.8748e-01, -1.0953e+00, -1.1454e-02, -1.4278e+00,\n",
      "        -5.5494e-01,  2.6929e-02, -8.8307e-01,  1.4867e+00,  1.3102e+00,\n",
      "         7.8010e-01,  8.0268e-01, -7.5872e-02, -3.8843e-01,  8.5350e-01,\n",
      "         2.7264e-01,  5.7208e-01, -2.8076e+00, -6.2541e-01,  3.3910e-01,\n",
      "         1.2967e+00,  2.2458e+00, -6.3808e-01, -1.1681e+00, -3.9329e-01,\n",
      "         6.5543e-01, -1.2356e+00, -3.7094e-01,  5.9898e-01, -6.9444e-01,\n",
      "         1.1406e+00, -2.7965e-03, -1.9256e+00,  7.9234e-01, -1.7960e-01,\n",
      "         1.3692e-01, -1.3680e+00,  1.0828e+00,  1.1273e+00,  4.7051e-01]) torch.Size([100])\n",
      "Weights: tensor([[-7.1810e-01,  8.3600e-01, -1.0791e+00,  1.2057e+00,  2.6973e-01,\n",
      "          5.2423e-01, -6.1063e-02, -6.9666e-01, -3.4132e-01,  1.1586e+00,\n",
      "         -1.0619e+00, -1.1610e+00,  4.3032e-01, -1.9583e+00,  9.3387e-02,\n",
      "         -1.0691e+00,  1.5235e+00, -6.9664e-02, -5.6455e-01, -7.9108e-01,\n",
      "          1.0705e-01,  2.7462e-01, -1.1039e+00, -1.4048e+00,  7.8364e-01,\n",
      "          3.6128e-01,  6.5409e-01, -1.1603e+00,  4.6247e-02, -2.9984e-01,\n",
      "         -2.1354e-01,  1.2460e+00, -8.3801e-01,  1.0456e+00,  1.5562e+00,\n",
      "         -2.6961e-01,  7.9354e-01, -8.5949e-02, -1.5612e+00,  3.5904e-01,\n",
      "         -1.2349e+00, -6.2868e-01,  6.5242e-01,  1.4185e-01,  4.7228e-01,\n",
      "          2.1225e-01,  7.2978e-01, -1.7944e+00, -3.9467e-01, -1.2257e-01,\n",
      "         -1.3193e+00,  1.1875e-02,  1.2833e+00,  2.6286e-01,  6.6034e-02,\n",
      "         -1.1686e+00,  3.2837e-01, -1.7837e+00, -1.6474e+00,  1.9871e-01,\n",
      "          1.0863e+00, -6.5903e-01,  1.5766e-01,  1.2503e+00, -2.5422e-01,\n",
      "         -1.4714e-01,  2.1997e-01,  1.3157e+00,  2.2878e-01,  1.1145e+00,\n",
      "         -3.2283e-01, -8.0235e-02,  2.4072e+00, -1.0029e+00, -1.3323e+00,\n",
      "         -8.9409e-01, -1.6066e+00,  1.3429e+00,  9.6337e-01,  1.4746e+00,\n",
      "         -4.4829e-01,  1.4953e-01, -8.8030e-01,  1.1096e-01, -5.8594e-01,\n",
      "         -8.9357e-01,  9.2534e-01, -1.0908e+00,  8.0932e-02,  2.3321e+00,\n",
      "          1.6550e+00, -3.8977e-01, -7.7072e-01,  3.8948e-01,  2.7477e+00,\n",
      "         -1.5581e+00, -2.5622e+00,  4.6791e-01,  1.7387e-01,  1.3312e+00],\n",
      "        [-7.1088e-01,  2.5429e-02,  1.0890e+00,  1.1829e-01,  2.2395e+00,\n",
      "         -1.2319e+00, -2.8846e-01, -1.3064e+00,  4.4597e-01, -1.8175e+00,\n",
      "         -1.3924e-01,  1.3249e+00, -6.2185e-01, -9.7683e-01,  1.0342e+00,\n",
      "          1.7657e-01,  8.3350e-01,  1.5388e-02,  5.7120e-01,  8.5034e-02,\n",
      "         -2.8718e-01,  8.9056e-01,  3.7022e-01,  1.3262e+00, -1.7049e+00,\n",
      "          3.8610e-01, -2.0026e+00,  6.0783e-01, -1.6551e+00, -5.9929e-01,\n",
      "         -3.6991e-01,  2.3647e-01, -1.1753e+00,  5.3637e-01, -9.0471e-01,\n",
      "         -1.2309e+00, -5.2558e-01, -3.8553e-01, -1.0000e+00,  6.1530e-02,\n",
      "         -1.4148e-01, -3.2006e-02, -2.1585e-01,  8.3349e-01, -6.4297e-01,\n",
      "         -1.6277e+00,  7.8763e-01, -5.3737e-01,  1.3131e+00, -5.9624e-01,\n",
      "          8.3240e-01, -3.0251e-01, -4.3890e-01, -6.0381e-01,  7.1719e-01,\n",
      "         -7.8713e-01,  3.3999e-01, -8.1010e-01,  4.6389e-01,  3.9152e-01,\n",
      "         -4.9597e-02, -3.6354e-01, -2.6023e+00,  1.1007e+00, -1.7645e+00,\n",
      "          6.7286e-01, -1.7120e-02, -8.4894e-01, -1.8022e+00,  8.7541e-02,\n",
      "         -1.1445e+00,  2.0378e+00,  2.6010e+00, -2.2385e-01, -1.1921e+00,\n",
      "         -2.0477e-01,  1.0826e+00, -5.8233e-01, -1.2191e+00, -1.2410e+00,\n",
      "         -5.1301e-01,  1.8155e-01, -1.4117e+00, -2.0157e+00, -4.1464e-01,\n",
      "          1.4782e+00,  4.7040e-01, -1.2020e+00,  1.9502e-01, -1.2976e+00,\n",
      "          6.0715e-01, -8.0226e-02, -9.8605e-01,  7.5322e-02, -9.2196e-01,\n",
      "         -1.0853e-01,  1.2442e+00,  1.2436e+00, -1.1895e+00, -3.2544e-01],\n",
      "        [ 8.1460e-01,  4.8126e-01,  6.4170e-01,  7.8046e-01,  7.1301e-01,\n",
      "          2.4376e+00,  7.4449e-02,  9.6073e-01, -8.6273e-01, -8.3088e-02,\n",
      "          5.9447e-02, -7.6873e-01, -6.5726e-01, -5.6905e-01,  1.9764e-01,\n",
      "         -1.7378e+00, -6.6617e-01, -8.9936e-01, -1.2121e+00,  4.4946e-01,\n",
      "          8.8620e-01, -3.0014e-01,  2.0293e+00,  1.4800e+00,  6.6429e-01,\n",
      "          1.1615e+00, -6.1566e-01, -9.6866e-01,  1.0670e+00,  9.4479e-01,\n",
      "          1.4494e+00, -7.3998e-01,  4.7223e-01, -5.5969e-01, -1.4609e-01,\n",
      "         -4.3357e-01, -1.4038e+00,  2.1768e+00, -6.2962e-01, -4.4866e-01,\n",
      "         -2.7296e-02, -2.4778e-01, -5.1016e-01, -1.3836e+00,  7.8202e-01,\n",
      "         -1.0901e+00, -1.3537e+00,  4.2172e-01,  1.6125e-01,  6.1557e-01,\n",
      "          2.7566e-01, -4.2143e-01, -9.7029e-01,  1.9328e+00, -9.1598e-01,\n",
      "         -1.0598e+00,  1.5106e+00,  6.7990e-01, -7.2256e-01, -8.4603e-01,\n",
      "          2.5762e-01, -4.8791e-01,  1.1588e+00,  9.8472e-01,  4.2132e-01,\n",
      "          1.3541e+00,  9.6074e-01,  1.0960e+00,  8.0618e-01, -5.7572e-01,\n",
      "          2.0584e+00,  8.5097e-01, -3.5352e-01,  3.7652e-01, -3.9866e-01,\n",
      "          5.2502e-02,  7.4158e-01, -5.7739e-01, -3.3450e-01, -8.8310e-01,\n",
      "         -1.3411e+00,  1.0866e+00, -1.0402e+00,  1.2738e+00, -2.3889e-01,\n",
      "          1.6581e+00,  9.3606e-02, -1.0642e+00,  5.2892e-01, -1.3660e+00,\n",
      "          8.4573e-01, -4.6098e-02,  1.0687e+00, -1.6301e+00, -9.5060e-01,\n",
      "          3.9265e-01, -1.4449e+00, -1.8935e+00,  8.7514e-01,  1.4432e+00],\n",
      "        [ 1.4128e+00,  5.7770e-01,  3.9623e-01, -1.4493e-01, -5.7405e-01,\n",
      "         -1.2095e-01,  1.3899e+00, -1.2030e+00,  2.4030e-01, -1.6544e-01,\n",
      "          2.0322e+00, -8.2616e-01, -5.3636e-01,  1.3758e+00, -2.3940e-01,\n",
      "         -8.7633e-01, -8.3108e-01,  2.8242e-01,  1.0010e+00, -2.7377e-01,\n",
      "          1.1995e+00,  2.8674e-01,  2.1740e+00, -4.7309e-01, -5.2840e-01,\n",
      "          3.0895e-01, -5.4864e-01,  8.3270e-01, -3.5875e-01,  7.1682e-01,\n",
      "         -5.3256e-01,  7.4503e-01, -4.4002e-01,  6.9609e-01, -2.2072e-01,\n",
      "          1.3650e+00,  2.1544e-01, -9.3033e-02, -9.6285e-01,  3.8327e-01,\n",
      "          6.3439e-01, -4.1400e-01, -1.9272e-01,  1.4764e+00,  8.8626e-01,\n",
      "         -2.9724e-01,  4.7020e-01, -1.0364e+00,  6.8510e-01, -1.9297e-01,\n",
      "          6.4189e-01,  5.6032e-01,  1.2352e+00,  1.8413e+00, -2.2121e+00,\n",
      "          5.7039e-02, -2.0588e-01, -9.3632e-01, -1.7652e+00, -3.7639e-02,\n",
      "         -7.4322e-01, -7.6209e-01, -1.3490e+00,  1.2202e+00,  3.7299e-01,\n",
      "         -4.4424e-01, -3.5138e-01, -3.1173e-02,  3.4857e-01, -2.0550e+00,\n",
      "          9.3395e-01,  3.3232e-01,  3.5048e-02,  1.9604e-02,  4.7809e-01,\n",
      "         -7.2230e-01,  1.6689e+00,  1.2793e+00, -4.1101e-01, -8.7899e-01,\n",
      "          3.2840e-01, -3.4789e-01,  8.5007e-01,  7.6753e-01, -7.4803e-03,\n",
      "          3.2052e-01,  2.0832e-01,  2.2560e-01, -7.7178e-01,  1.0816e+00,\n",
      "         -2.1803e+00,  2.3792e-01, -5.1057e-01, -6.6335e-01,  8.1924e-01,\n",
      "          7.5172e-01,  3.9705e-01,  9.2073e-01, -8.5089e-01, -5.3331e-02],\n",
      "        [ 3.6215e-01, -2.6254e-01,  6.1347e-01,  8.0390e-01,  3.2292e+00,\n",
      "          2.8667e-01, -1.8176e+00, -1.6987e+00,  2.2694e-01, -5.3206e-01,\n",
      "          2.8164e-01,  6.6339e-01,  1.7068e+00,  8.2487e-02, -1.5641e+00,\n",
      "         -5.7801e-01, -4.7712e-01, -9.4715e-01, -2.8117e-01, -4.9719e-01,\n",
      "          7.6347e-01,  5.1177e-01, -4.4473e-01, -1.6531e+00, -7.2736e-01,\n",
      "          1.4863e-01, -1.6997e-01,  3.4705e-01, -2.4451e-02, -5.5490e-01,\n",
      "         -1.4115e+00,  1.4792e+00, -1.8170e+00, -6.6434e-01,  5.7631e-01,\n",
      "          1.7041e+00, -1.0011e+00,  7.5991e-01, -7.4984e-01,  5.8652e-01,\n",
      "         -7.6236e-01,  6.8469e-01, -7.1102e-01, -2.8819e-01, -7.1024e-02,\n",
      "         -1.9569e-01, -2.2118e-01,  1.4781e+00,  5.0949e-01,  7.0875e-01,\n",
      "         -2.3041e+00, -5.2504e-01,  6.0890e-01, -2.3092e+00, -7.2373e-01,\n",
      "          4.3035e-01, -1.1462e+00,  3.7116e-01,  1.3328e+00, -1.9126e-01,\n",
      "         -4.0249e-02, -1.1455e+00,  1.8672e+00, -1.3552e+00, -2.0998e+00,\n",
      "         -8.5932e-01,  9.8540e-01, -2.6434e+00,  2.6961e-01,  1.2891e-01,\n",
      "         -4.6070e-01, -9.3153e-01, -4.2360e-02,  1.4518e+00, -7.7915e-01,\n",
      "         -2.3505e+00, -9.9457e-01,  1.8400e-01, -1.8827e+00, -1.7448e+00,\n",
      "          1.4177e+00,  3.2395e-01, -6.2231e-02,  3.0489e-01,  1.2175e-01,\n",
      "          5.5836e-01, -4.4466e-01,  3.4690e-01, -2.6629e-01,  4.9420e-01,\n",
      "          3.8629e-01, -2.5782e+00,  2.2763e+00,  9.1525e-01,  3.8684e-01,\n",
      "          2.1511e+00,  3.7783e-01,  6.1362e-01, -4.9867e-01, -2.9690e-01],\n",
      "        [ 1.8192e+00,  7.5188e-01, -2.5624e-02, -2.3873e-01,  3.0330e-02,\n",
      "          1.6655e-01, -7.2749e-01, -4.5378e-01,  1.3745e-01, -7.3632e-01,\n",
      "         -2.0109e-01, -1.1377e+00,  7.4136e-01,  1.1940e+00, -1.3213e-01,\n",
      "         -7.9466e-01,  1.5852e-02,  7.4152e-01,  4.9078e-03,  1.7905e+00,\n",
      "          4.2722e-01,  2.9182e-01, -1.1522e+00,  2.9243e-01, -1.4846e-01,\n",
      "         -7.7229e-02, -1.8028e+00, -8.4563e-01, -7.5941e-01,  1.3174e+00,\n",
      "         -1.0508e+00, -2.3263e-01, -3.1366e-01,  1.3844e+00,  3.6892e-01,\n",
      "         -9.4231e-01,  8.4514e-01,  1.6384e+00,  5.4606e-01,  9.9718e-01,\n",
      "         -1.0060e+00, -3.3119e-01,  6.2402e-01, -1.3950e-01, -3.4087e-01,\n",
      "          1.8141e-01,  4.2441e-01, -1.0516e+00,  2.1074e+00, -2.9034e-01,\n",
      "         -9.8687e-01, -1.1533e+00,  1.1101e+00,  1.4474e-01,  4.5651e-01,\n",
      "          1.0404e+00, -8.0491e-01, -1.4995e+00, -2.8694e-01, -1.4160e+00,\n",
      "          1.0657e+00, -1.3080e+00,  1.6061e+00,  7.3484e-01, -5.9020e-01,\n",
      "          1.1070e+00, -1.7763e+00, -7.2777e-01,  1.4331e+00,  9.5111e-01,\n",
      "         -1.2630e+00, -4.2146e-01,  5.7142e-01, -6.2862e-01,  1.6844e+00,\n",
      "         -4.0412e-01, -1.6352e+00, -1.0059e+00,  6.9011e-04, -1.9917e+00,\n",
      "         -1.4201e+00, -1.0875e+00, -2.0102e+00, -1.1761e+00,  2.2165e-01,\n",
      "         -1.2756e+00,  1.9618e+00, -1.6436e+00,  9.9192e-01, -2.2536e+00,\n",
      "         -3.3507e-01, -1.3871e+00, -1.5678e+00, -1.6249e+00,  1.2603e+00,\n",
      "         -1.0253e+00,  3.1168e-01,  1.2904e+00, -1.4584e+00, -1.4469e-01]])\n"
     ]
    }
   ],
   "source": [
    "W_OUT = 100\n",
    "\n",
    "# (6,100)\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN)\n",
    "# (1,100)\n",
    "    # 1 bias per neuron (per column)\n",
    "bias = torch.randn(W_OUT, generator=GEN)\n",
    "print(\"Bias:\", bias, bias.shape)\n",
    "\n",
    "\n",
    "# bias = torch.randn()\n",
    "print(\"Weights:\", W)\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "# h for hidden layer\n",
    "h = emb_view @ W\n",
    "\n",
    "# why this works: bias is (100,)\n",
    "# broadcast: \n",
    "    # h: (32, 100)\n",
    "    # b: (  , 100) -> (1,100), 1 is inferred\n",
    "    # so it copies bias and adds to each row as expected - each column gets a different bias value, for each neuron\n",
    "h = h + bias\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [6.8365e-14, 4.1561e-10, 2.0476e-05, 1.9685e-01, 3.6644e-09, 2.5611e-04,\n",
       "         2.3277e-09, 4.3530e-05, 4.4425e-05, 1.3900e-09, 2.9489e-05, 2.1639e-03,\n",
       "         8.2406e-09, 9.1830e-03, 5.0304e-05, 1.9712e-07, 1.0256e-03, 6.0565e-08,\n",
       "         1.5621e-02, 1.3701e-06, 3.1891e-06, 1.0589e-05, 4.8030e-07, 2.0710e-04,\n",
       "         2.5028e-02, 6.3929e-02, 6.8553e-01],\n",
       "        [1.3785e-10, 7.1116e-05, 1.9685e-11, 3.7020e-08, 3.8112e-07, 6.1939e-05,\n",
       "         1.0589e-03, 2.4612e-02, 5.9960e-05, 8.2954e-06, 2.8608e-03, 1.1369e-06,\n",
       "         2.0626e-08, 1.2665e-06, 1.3825e-03, 6.0687e-05, 5.0792e-06, 2.9640e-02,\n",
       "         2.6060e-12, 1.0480e-08, 8.6222e-06, 5.8362e-08, 3.8414e-02, 6.9616e-01,\n",
       "         1.3612e-06, 2.0560e-01, 4.1925e-08],\n",
       "        [1.1296e-08, 9.9169e-01, 8.2763e-05, 4.7307e-08, 5.2716e-07, 1.1432e-07,\n",
       "         4.5556e-08, 1.5861e-03, 2.5982e-06, 3.0303e-08, 4.2567e-05, 4.3100e-10,\n",
       "         4.2078e-09, 4.7673e-07, 1.3123e-06, 3.5119e-03, 1.8188e-09, 9.2137e-10,\n",
       "         1.0143e-03, 5.2248e-10, 1.5106e-04, 5.9120e-09, 2.2771e-06, 1.9160e-03,\n",
       "         6.2971e-09, 5.7147e-09, 1.9878e-06],\n",
       "        [2.4788e-08, 4.1117e-05, 1.1683e-08, 8.8031e-05, 3.0312e-08, 1.8094e-03,\n",
       "         1.0015e-07, 1.4834e-08, 9.9315e-11, 1.4141e-08, 5.7513e-06, 4.2593e-04,\n",
       "         1.1685e-07, 2.8681e-01, 9.1562e-07, 3.4057e-02, 3.5671e-09, 1.1302e-13,\n",
       "         6.4183e-06, 6.1721e-08, 7.6164e-03, 7.1072e-07, 7.6732e-09, 6.6902e-01,\n",
       "         1.1124e-04, 6.7153e-12, 1.0122e-05],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [1.3228e-10, 5.2603e-01, 6.8273e-09, 7.1021e-09, 1.1829e-07, 8.2693e-05,\n",
       "         7.0980e-05, 1.4303e-01, 4.5650e-06, 1.5805e-07, 7.8844e-03, 1.0340e-06,\n",
       "         8.8627e-08, 4.5004e-07, 4.7476e-03, 6.4546e-07, 3.2316e-07, 8.9055e-03,\n",
       "         1.6497e-09, 3.4401e-08, 1.4834e-07, 2.0662e-07, 2.8947e-03, 1.7122e-01,\n",
       "         8.3633e-07, 1.3513e-01, 6.4751e-07],\n",
       "        [1.2745e-10, 1.0000e+00, 1.6020e-13, 1.8645e-10, 5.1141e-12, 4.2361e-10,\n",
       "         1.4395e-13, 2.1033e-07, 6.0098e-10, 1.2448e-12, 7.6635e-11, 1.1237e-09,\n",
       "         1.3887e-10, 3.6748e-09, 2.8929e-10, 9.9027e-09, 4.6133e-13, 1.2200e-15,\n",
       "         2.7734e-08, 2.8084e-12, 1.5415e-08, 3.5499e-10, 2.3313e-09, 4.0335e-07,\n",
       "         4.8197e-09, 3.6785e-10, 4.0072e-07],\n",
       "        [1.5062e-10, 7.9447e-01, 6.8998e-12, 3.8265e-08, 6.0892e-12, 1.4840e-01,\n",
       "         1.6850e-08, 2.2217e-09, 7.1332e-12, 1.0418e-03, 6.1412e-04, 3.0613e-04,\n",
       "         2.4696e-10, 2.5273e-03, 7.4339e-04, 7.8838e-04, 2.7189e-09, 2.0782e-17,\n",
       "         2.9269e-08, 3.4234e-11, 5.7498e-07, 8.5817e-07, 1.0154e-07, 5.1110e-02,\n",
       "         3.4165e-10, 1.5504e-12, 9.0301e-09],\n",
       "        [9.9902e-10, 1.1785e-09, 3.0751e-11, 3.6634e-06, 2.3274e-05, 2.2613e-12,\n",
       "         8.7994e-07, 6.4068e-09, 9.5297e-13, 3.0796e-10, 1.9769e-11, 3.4849e-10,\n",
       "         4.9741e-11, 5.3920e-11, 2.3951e-08, 2.5569e-09, 5.8760e-14, 5.1701e-12,\n",
       "         6.6037e-09, 5.7791e-12, 9.9997e-01, 9.0664e-11, 2.0022e-11, 1.0661e-06,\n",
       "         1.4851e-11, 1.8723e-14, 5.8557e-14],\n",
       "        [1.1216e-13, 1.3773e-04, 2.3751e-10, 4.4798e-06, 1.0092e-08, 4.3905e-05,\n",
       "         9.5929e-10, 2.1847e-09, 9.9114e-11, 1.4299e-09, 9.3408e-08, 1.6257e-09,\n",
       "         5.2884e-12, 6.7871e-09, 2.2369e-06, 3.5295e-08, 2.5777e-08, 7.6389e-13,\n",
       "         9.9981e-01, 1.1991e-09, 6.5274e-09, 2.5702e-10, 3.7675e-09, 1.0881e-07,\n",
       "         1.2617e-08, 4.5884e-13, 8.0979e-09],\n",
       "        [1.3089e-11, 7.7038e-09, 5.3127e-08, 4.4668e-05, 5.2548e-07, 7.5878e-04,\n",
       "         1.7066e-07, 1.0781e-07, 7.4577e-06, 9.0856e-09, 7.0690e-08, 1.3405e-01,\n",
       "         1.4165e-05, 6.1325e-04, 3.4102e-01, 2.6888e-07, 2.6087e-07, 2.2872e-07,\n",
       "         8.1178e-08, 7.1988e-07, 5.2155e-01, 1.1549e-09, 1.6845e-04, 6.5212e-05,\n",
       "         5.1928e-05, 6.6453e-08, 1.6523e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [3.0727e-11, 7.0382e-08, 9.0770e-05, 2.8462e-01, 4.4219e-03, 5.0848e-03,\n",
       "         2.2958e-04, 1.3853e-02, 5.1958e-02, 5.3968e-07, 1.6313e-04, 6.4986e-02,\n",
       "         6.8246e-08, 3.2213e-03, 3.9932e-02, 3.9735e-05, 2.0214e-04, 7.5298e-05,\n",
       "         2.0479e-02, 1.0849e-05, 1.3969e-01, 4.4869e-05, 3.1109e-05, 1.6134e-01,\n",
       "         1.0194e-01, 4.7200e-03, 1.0287e-01],\n",
       "        [7.1443e-10, 8.3208e-06, 1.3397e-08, 1.1205e-06, 7.6466e-01, 1.5721e-07,\n",
       "         1.3244e-01, 1.1216e-03, 1.5530e-04, 9.2832e-08, 4.5542e-10, 3.3853e-07,\n",
       "         3.4060e-08, 5.6231e-07, 1.9254e-04, 2.3368e-07, 2.6900e-08, 2.8312e-04,\n",
       "         3.6823e-07, 1.0882e-07, 8.5466e-04, 3.7873e-08, 2.7222e-08, 1.0012e-01,\n",
       "         1.5806e-04, 1.8494e-08, 1.4281e-06],\n",
       "        [6.5291e-14, 1.5102e-07, 1.1729e-04, 5.7639e-02, 1.9839e-04, 3.0956e-04,\n",
       "         7.3039e-06, 6.2358e-05, 3.8526e-06, 3.4572e-09, 7.1664e-06, 7.9779e-06,\n",
       "         8.5429e-10, 5.1024e-06, 1.1610e-04, 1.6946e-04, 2.3829e-04, 1.8623e-07,\n",
       "         8.3640e-01, 2.6639e-08, 7.1354e-06, 5.8141e-07, 2.0660e-09, 9.9714e-02,\n",
       "         2.7150e-03, 1.2070e-06, 2.2771e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [5.2470e-11, 7.6938e-03, 1.0777e-05, 5.0911e-03, 1.0572e-06, 3.5413e-03,\n",
       "         6.2156e-06, 1.4944e-01, 1.4113e-01, 1.6016e-07, 2.7195e-02, 4.4333e-02,\n",
       "         2.4544e-03, 8.7723e-03, 4.1838e-03, 1.5251e-05, 1.1605e-03, 5.5702e-05,\n",
       "         6.4435e-04, 7.8044e-04, 9.9097e-06, 7.1613e-03, 1.7232e-01, 7.4297e-03,\n",
       "         1.7301e-01, 2.0271e-01, 4.0855e-02],\n",
       "        [4.2015e-10, 1.6522e-02, 1.3572e-10, 1.3594e-07, 6.9696e-03, 1.1380e-06,\n",
       "         4.4457e-07, 6.5977e-04, 1.6107e-03, 2.7494e-08, 3.9008e-07, 1.2763e-05,\n",
       "         1.0397e-07, 7.4413e-08, 1.6280e-02, 1.5473e-02, 1.3636e-07, 1.3538e-06,\n",
       "         1.4636e-08, 2.6405e-07, 2.7999e-01, 2.0693e-08, 1.5857e-03, 6.6085e-01,\n",
       "         2.5812e-06, 3.1863e-06, 4.2912e-05],\n",
       "        [1.9791e-11, 2.0595e-06, 9.9719e-03, 9.5977e-04, 2.9245e-04, 1.4362e-04,\n",
       "         5.3716e-08, 2.1452e-06, 7.4617e-09, 7.7082e-08, 1.6292e-04, 7.7487e-07,\n",
       "         1.6527e-10, 6.1820e-04, 7.4312e-06, 5.2019e-04, 3.0618e-06, 1.2894e-13,\n",
       "         8.4620e-01, 1.9118e-07, 1.0804e-03, 2.0602e-08, 1.7234e-09, 1.3877e-01,\n",
       "         1.2669e-03, 4.8764e-13, 2.6270e-08],\n",
       "        [3.8494e-13, 7.3627e-01, 1.5682e-08, 6.9540e-09, 1.6663e-13, 4.1843e-04,\n",
       "         2.2122e-04, 3.6886e-07, 4.4483e-06, 3.9973e-03, 1.1547e-05, 1.0779e-02,\n",
       "         2.1516e-04, 2.0246e-04, 4.0275e-04, 4.2318e-07, 1.5429e-07, 3.6351e-07,\n",
       "         1.0134e-12, 2.1719e-07, 4.3730e-10, 1.8890e-05, 9.8278e-02, 4.5051e-05,\n",
       "         2.2433e-11, 1.4914e-01, 4.5380e-07],\n",
       "        [3.4293e-06, 2.5420e-09, 7.8812e-06, 1.4604e-03, 5.9348e-10, 1.0571e-05,\n",
       "         4.2870e-12, 1.2451e-07, 1.5078e-08, 5.8704e-12, 3.9242e-09, 2.5691e-07,\n",
       "         5.6767e-10, 4.0119e-05, 3.8598e-05, 7.5505e-05, 8.8371e-11, 1.2825e-14,\n",
       "         2.6853e-05, 5.5664e-10, 2.1989e-02, 6.9340e-09, 2.2668e-06, 2.2171e-07,\n",
       "         1.0763e-04, 1.4859e-05, 9.7622e-01],\n",
       "        [9.5699e-08, 2.0733e-04, 3.2152e-09, 2.9141e-05, 3.3797e-10, 2.8711e-04,\n",
       "         8.9165e-01, 6.9198e-11, 4.6177e-12, 1.0739e-01, 8.8537e-05, 8.1354e-05,\n",
       "         4.6162e-08, 1.0546e-06, 7.2800e-07, 3.1520e-09, 1.0961e-10, 2.2440e-09,\n",
       "         2.5469e-12, 1.2000e-09, 3.3605e-07, 7.7052e-08, 3.1465e-06, 2.6781e-04,\n",
       "         3.4917e-11, 4.7030e-12, 2.6047e-12],\n",
       "        [2.8478e-08, 6.6653e-01, 1.0830e-05, 1.4612e-08, 1.2338e-08, 5.5271e-07,\n",
       "         3.3243e-08, 2.8572e-01, 8.4457e-04, 3.5494e-08, 1.8938e-04, 6.2184e-10,\n",
       "         8.0545e-09, 3.9368e-08, 1.7321e-04, 3.9506e-02, 6.0118e-07, 1.4032e-10,\n",
       "         1.8904e-04, 6.0291e-09, 8.0527e-05, 2.7112e-06, 9.3329e-05, 6.6551e-03,\n",
       "         4.2669e-08, 7.3339e-08, 7.3088e-07],\n",
       "        [7.5847e-06, 1.7647e-05, 3.9970e-07, 2.8866e-05, 1.8172e-08, 4.0296e-03,\n",
       "         5.9501e-07, 3.4804e-09, 1.2925e-11, 2.8633e-07, 2.7979e-04, 5.0431e-06,\n",
       "         7.6742e-09, 5.2283e-02, 2.0955e-04, 8.9330e-02, 1.8862e-09, 5.8623e-14,\n",
       "         2.1015e-06, 8.6655e-09, 2.9872e-01, 7.3246e-08, 1.0754e-07, 5.5508e-01,\n",
       "         6.8259e-07, 3.2069e-11, 8.3104e-09],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [8.1452e-10, 1.8016e-04, 6.5544e-08, 3.9486e-07, 1.7942e-03, 1.1060e-06,\n",
       "         2.1602e-04, 3.6675e-04, 6.3153e-06, 4.7813e-09, 2.2747e-06, 1.1809e-07,\n",
       "         4.0151e-08, 6.1695e-07, 7.7422e-05, 2.8504e-07, 1.5279e-07, 1.3777e-05,\n",
       "         1.0753e-07, 1.9894e-08, 5.9891e-06, 1.0691e-07, 8.8311e-07, 9.9588e-01,\n",
       "         1.4456e-03, 1.2169e-07, 1.1007e-05],\n",
       "        [7.7901e-12, 9.9992e-01, 6.1512e-12, 2.5135e-12, 1.2922e-09, 9.6660e-09,\n",
       "         1.6270e-10, 2.6213e-05, 5.4496e-08, 1.1987e-11, 2.1822e-06, 2.0637e-08,\n",
       "         1.6035e-09, 5.7746e-11, 1.4566e-08, 4.4576e-08, 2.2200e-10, 1.5848e-08,\n",
       "         1.6301e-09, 1.3031e-10, 1.2187e-08, 2.6812e-10, 5.4765e-09, 1.4868e-05,\n",
       "         5.2247e-10, 3.7397e-05, 1.2600e-06],\n",
       "        [5.7278e-12, 2.3306e-14, 7.1772e-07, 7.7967e-08, 3.1102e-05, 1.2452e-07,\n",
       "         6.8755e-11, 4.1059e-11, 1.9512e-09, 2.4815e-14, 1.4978e-14, 4.9167e-08,\n",
       "         1.4188e-12, 5.8159e-06, 1.3131e-04, 4.6438e-07, 7.4308e-13, 1.4926e-12,\n",
       "         3.6095e-06, 2.8189e-13, 9.9983e-01, 1.9827e-13, 8.4867e-13, 1.6814e-06,\n",
       "         3.0432e-10, 2.2336e-14, 3.1839e-08],\n",
       "        [2.0404e-10, 2.5702e-08, 1.2095e-08, 5.2292e-04, 3.6576e-11, 8.6851e-06,\n",
       "         9.9664e-01, 1.1695e-10, 3.9258e-14, 1.3928e-07, 4.2754e-07, 1.4162e-04,\n",
       "         6.5149e-10, 2.6736e-03, 7.3567e-09, 2.9535e-12, 1.2528e-07, 7.2461e-06,\n",
       "         3.3744e-11, 1.2678e-11, 1.5350e-08, 6.9142e-12, 5.0439e-12, 1.1578e-06,\n",
       "         2.3050e-11, 2.3835e-08, 6.5479e-14],\n",
       "        [8.7365e-15, 5.7650e-10, 5.0590e-03, 2.5860e-06, 9.1061e-07, 2.7300e-08,\n",
       "         6.1033e-05, 1.9516e-02, 3.6828e-02, 1.0858e-08, 3.8272e-09, 3.0933e-06,\n",
       "         1.9118e-12, 2.9697e-09, 8.6193e-06, 2.6454e-07, 2.5217e-03, 1.0188e-02,\n",
       "         7.1391e-08, 1.6662e-06, 2.4008e-08, 7.3626e-10, 1.2556e-04, 1.7289e-06,\n",
       "         7.3063e-01, 1.9229e-01, 2.7674e-03],\n",
       "        [1.1174e-09, 1.1508e-09, 3.5377e-05, 6.8658e-05, 1.1534e-04, 3.6285e-09,\n",
       "         4.2690e-08, 9.5894e-01, 1.8759e-03, 6.1524e-11, 4.6356e-08, 2.2982e-11,\n",
       "         5.4032e-12, 1.4064e-05, 1.7364e-02, 7.6190e-06, 5.5600e-07, 2.6349e-10,\n",
       "         5.0398e-04, 3.2318e-08, 2.0003e-02, 7.7636e-10, 1.1831e-07, 2.9509e-04,\n",
       "         7.5410e-04, 1.9720e-06, 2.1938e-05]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "res = h @ W2 + b2\n",
    "probs = res.softmax(dim=1)\n",
    "\n",
    "# ach row is normalized\n",
    "print(probs.sum(dim=1))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding probabilities for each y label, get NLL Loss\n",
    "torch.arange(m) -> 0,1,2,3...m-1 for row indices\n",
    "Y: e.g [0,5,13,13,6..] -> actual labels. so we end up getting\n",
    "- res[0,0], res[1,5], res[2,13] etc in a vector\n",
    "- This is the predicted probability based on current weights for that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8111e-04, 9.1830e-03, 1.2665e-06, 9.9169e-01, 2.4788e-08, 6.1128e-06,\n",
       "        8.8627e-08, 1.2448e-12, 1.0154e-07, 3.0796e-10, 1.3773e-04, 1.3089e-11,\n",
       "        7.5123e-07, 3.1109e-05, 8.3208e-06, 6.5291e-14, 1.6017e-06, 7.8044e-04,\n",
       "        1.6522e-02, 9.9719e-03, 4.1843e-04, 5.6767e-10, 4.6162e-08, 6.6653e-01,\n",
       "        7.5847e-06, 4.1817e-07, 2.8504e-07, 2.2200e-10, 1.9512e-09, 1.3928e-07,\n",
       "        5.7650e-10, 1.1174e-09])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(Y)\n",
    "probs[torch.arange(m), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5532)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = -probs.log().mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--SETUP--#\n",
    "\n",
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(2147483647)\n",
    "# Number of examples\n",
    "M = len(Xtr) \n",
    "\n",
    "# Params: C, W, bias, W2, b2\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "\n",
    "# Forward pass\n",
    "W_OUT = 200\n",
    "\n",
    "# First layer\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN) # (6,100)\n",
    "bias = torch.randn(W_OUT, generator=GEN) # (1,100): 1 bias per neuron (per column)\n",
    "\n",
    "# Second layer: W_OUT inputs, 27 outputs for log-counts (to become probabilities)\n",
    "W2 = torch.randn((W_OUT, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "parameters = [C, W, bias, W2, b2]\n",
    "for param in parameters:\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4491617679595947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x38eee8f70>]"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--TRAINING--#\n",
    "EPOCHS=50000\n",
    "LR=0.01\n",
    "BATCH_SIZE=32\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # select a random batch and use that for fwd + backward for faster iteration\n",
    "    # batch_indices = torch.randperm(len(Xtr), generator=GEN)\n",
    "    # batch_indices = batch_indices[:BATCH_SIZE]\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (32,), generator=GEN)\n",
    "\n",
    "    emb = C[Xtr[batch_indices]] # lookup embeddings corresponding to each row -> (M, BLOCK_SIZE, 2) i.e one 2D vector per char in a row of X\n",
    "\n",
    "    # view shares the same underlying elements, just rearranges logically\n",
    "        # (number of examples, number of elems per example)\n",
    "    emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "    \n",
    "    # forward pass: get linear combs + bias for each row, activate with tanh\n",
    "    h = emb_view @ W\n",
    "    h = h + bias\n",
    "    h = h.tanh()\n",
    "    \n",
    "    # forward pass 2nd layer to get 27 outputs per row\n",
    "    probs = h @ W2 + b2\n",
    "    \n",
    "    # this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "    nll_loss = F.cross_entropy(probs, Ytr[batch_indices])\n",
    "    \n",
    "    epochs.append(i)\n",
    "    losses.append(nll_loss.log10().item())\n",
    "    # --Update--#\n",
    "    \n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "    \n",
    "    # backward pass\n",
    "    nll_loss.backward()\n",
    "    \n",
    "    # update\n",
    "    with torch.no_grad():\n",
    "        for param in parameters:\n",
    "            param -= LR * param.grad\n",
    "            \n",
    "print(\"Loss:\", nll_loss.item())\n",
    "\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loss for dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2029, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ydev)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1730, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ytr)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melrendi\n",
      "taminressy\n",
      "cayhlusipo\n",
      "kace\n",
      "elli\n",
      "ton\n",
      "espunigolt\n",
      "sirellea\n",
      "kaveynaill\n",
      "baya\n",
      "fan\n",
      "fodd\n",
      "jyovan\n",
      "mckea\n",
      "barla\n",
      "zemusandin\n",
      "ovia\n",
      "majadelvin\n",
      "kensonna\n",
      "raylalara\n"
     ]
    }
   ],
   "source": [
    "def get_word(max_count=10):\n",
    "    current_block = [0] * BLOCK_SIZE\n",
    "    string = []\n",
    "\n",
    "    for i in range(max_count):\n",
    "        emb = C[current_block].view(1, BLOCK_SIZE * EMBEDDING_SIZE)\n",
    "        \n",
    "        # fwd pass\n",
    "        h = emb @ W + bias\n",
    "        h = h.tanh()\n",
    "    \n",
    "        probs = h @ W2 + b2\n",
    "        # turn into probabilities\n",
    "        probs = probs.softmax(dim=1)\n",
    "    \n",
    "        sampled = torch.multinomial(probs, num_samples=1, replacement=True, generator=GEN)\n",
    "        sampled_idx = sampled.item()\n",
    "        char = itos[sampled_idx]\n",
    "\n",
    "        if char == '.':\n",
    "            break\n",
    "\n",
    "        string.append(char)\n",
    "\n",
    "        current_block = current_block[1:] + [sampled_idx]\n",
    "\n",
    "\n",
    "    return ''.join(string)\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    word = get_word()\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'baya' in names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQDGcWn074pA7NdAWB8vTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env 28Sep)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
