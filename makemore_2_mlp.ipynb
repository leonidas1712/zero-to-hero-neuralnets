{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNe_RPhx3Cx"
   },
   "source": [
    "## Makemore Part 2: MLP\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Idea from paper: C is a lookup table (matrix) for embeddings vector of each of the words in V e.g |V|=17k\n",
    "- C: (17000x30)\n",
    "- one-hot encoding of a word: (1x17000)\n",
    "- enc @ C -> (1x30) embedding vector\n",
    "\n",
    "We can get these vectors for multiple input words (words that came before).\n",
    "\n",
    "Pass them to neurons with n_in=30, those neurons are fully connected to a hidden layer, then goes through tanh -> softmax for probabilities.\n",
    "\n",
    "Final output: 17k vector of probabilities for the 17k words in vocab.\n",
    "- Train by using actual next word's index, get the predicted prob, do -log(prob) and backprop etc.\n",
    "\n",
    "---\n",
    "\n",
    "We can use this idea for a character-level model as well.\n",
    "\n",
    "## Architecture\n",
    "- Start with indexes from the chars/words in context\n",
    "- Convert them to respective embeddings vectors in C (lookup)\n",
    "- Stack the vectors' values together, do forward pass with weights matrix and add bias\n",
    "- Activate through tanh\n",
    "- Pass through one more layer to get 27 outputs (one for each unique char in our vocab)\n",
    "- Softmax to get probabilities\n",
    "\n",
    "## Building dataset\n",
    "Hyperparameter: BLOCK_SIZE = 3\n",
    "- block_size is the number of previous chars we consider when predicting next\n",
    "\n",
    "For each word, we add to X with block_size=3 char windows, Y has the char that comes after\n",
    "- e.g '.emma': (..., e), (..e, m), (.em, m), (emm, a), (mma, .)\n",
    "- So each word contributes n+1 examples as before, n = len(word)\n",
    "\n",
    "## Lookup table\n",
    "Lookup table: C = (27,2) random init\n",
    "- In paper, they compress 17k words of vocab into Rn of n=30\n",
    "- So we do similar here for 27 unique chars -> embeddings of size 2\n",
    "\n",
    "Previously, we used one-hot encoding to lookup with enc @ W\n",
    "- But this is just the same as doing W[idx] due to all the zeroes\n",
    "\n",
    "In PyTorch, we can just do C[X] and it will work\n",
    "- produces (32,3,2) - one 2D vector for each encoded char\n",
    "- Or another way to think about it, one (3x2) vector for each row in X. 3 because BLOCK_SIZE=3, so each row in X has 3 elements. For each of those chars, we want one 2D vector - its embedding\n",
    "\n",
    "## F.cross_entropy\n",
    "F.cross_entropy(logits, targets) is the same as doing:\n",
    "\n",
    "```python\n",
    "logits = logits.softmax(dim=1)\n",
    "logits = logits[torch.arange(M), Y]\n",
    "nll_loss = -logits.prob().mean()\n",
    "```\n",
    "i.e same as softmax -> select corresponding probabilities for targets in Y -> get NLL Loss (mean)\n",
    "\n",
    "F.cross_entropy is **better** because PyTorch can optimise and not create new memory, and it can use **fused kernels** to cluster ops together and run them at the same time.\n",
    "- Also more numerically stable. logits can be subtracted or added by any number. when logits are too high, e^(high number) becomes inf, then we get nans. But PyTorch can internally subtract the data by the max number to prevent this\n",
    "\n",
    "## Batching\n",
    "Instead of fwd + backward on whole dataset which is slow, we can pick a random batch each time and fwd + backward on that.\n",
    "Just select BATCH_SIZE of random indices within [0,M), then use X[batch_indices] and Y[batch_indices]\n",
    "\n",
    "## Finding good learning rate\n",
    "1. Find min and max bounds by trial and error\n",
    "- Set very low LR, low is if it barely changes\n",
    "- High is if loss grows\n",
    "2. Create torch.linspace between low and high using exponents for the lrs e.g -3, 0 for 10^-3 -> 10^0=1\n",
    "3. During loop, ith iteration uses ith LR in list for the batch. Track the loss for that LR\n",
    "4. Graph it and find the valley in the graph - the middle should be a good LR.\n",
    "\n",
    "## Train, test, validation split (80,10,10)\n",
    "Train split: Used to optimise model parameters with gradient descent\n",
    "\n",
    "Validation split: Used to tune hyperparameters: e.g outputs in hidden layers, size of embeddings in C\n",
    "  \n",
    "Test split: Used to evaluate the performance of the model at the **end**\n",
    "- Why: achieving low loss on the same set we used to train isn't necessarily a good thing. Because when we use bigger models, they are able to just memorise the dataset, then doesn't generalise to new inputs well.\n",
    "\n",
    "## Overfitting, underfitting\n",
    "Underfitting: when train loss and dev or test loss are both high and close to each other\n",
    "- Means the model isn't big enough / powerful enough to fit the data\n",
    "\n",
    "Overfitting: when train loss is low, but dev/test loss are much higher\n",
    "- Means the model is memorising the training data but doesn't generalise well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "executionInfo": {
     "elapsed": 10072,
     "status": "ok",
     "timestamp": 1727498203827,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "E1zlN5r9xtTz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl1lLlcpJtBM"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1727488190995,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "INrlJJdu70z3",
    "outputId": "4fa9ff58-4349-46f0-fbb5-5b7f861df269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 32033\n",
      "['yuheng', 'diondre', 'xavien', 'jori', 'juanluis']\n",
      "25626 28829\n"
     ]
    }
   ],
   "source": [
    "# Open up names\n",
    "names = []\n",
    "with open('names.txt', 'r') as names_file:\n",
    "  names = names_file.read().splitlines()\n",
    "\n",
    "print(\"Total length:\", len(names))\n",
    "\n",
    "# build lookups\n",
    "uniq = ['.'] + sorted(list(set(''.join(names))))\n",
    "stoi = { char: idx for idx, char in enumerate(uniq)}\n",
    "itos = { idx: char for char,idx in stoi.items() }\n",
    "\n",
    "# Hyperparameters\n",
    "BLOCK_SIZE = 3\n",
    "EMBEDDING_SIZE = 2\n",
    "\n",
    "def build_dataset(names):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in names:\n",
    "      # print(word)\n",
    "      word = word + '.'\n",
    "      block = [0] * BLOCK_SIZE # ... (empty context at start)\n",
    "    \n",
    "      for char in word:\n",
    "        char_idx = stoi[char]\n",
    "        X.append(block)\n",
    "        Y.append(char_idx)\n",
    "    \n",
    "        block_str = ''.join(list(map(lambda i: itos[i], block)))\n",
    "        # print(f'{block_str} -> {char}')\n",
    "    \n",
    "        # update block: roll over sliding window\n",
    "        block = block[1:] + [char_idx]\n",
    "\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# train, test, dev split: 80,10,10\n",
    "from random import Random\n",
    "Random(42).shuffle(names)\n",
    "\n",
    "print(names[0:5])\n",
    "\n",
    "n_80 = int(0.8*len(names))\n",
    "n_90 = int(0.9*len(names))\n",
    "print(n_80, n_90)\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n_80]) # 80%\n",
    "Xdev, Ydev = build_dataset(names[n_80:n_90]) # 10%\n",
    "Xtest, Ytest = build_dataset(names[n_90:]) # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228146\n",
      "0.8004742577121667\n",
      "0.09930044795876325\n",
      "0.10022529432906999\n",
      "182625 22655 22866\n"
     ]
    }
   ],
   "source": [
    "total = len(Xtr) + len(Xdev) + len(Xtest)\n",
    "print(total)\n",
    "print(len(Xtr) / total)\n",
    "print(len(Xdev) / total)\n",
    "print(len(Xtest) / total)\n",
    "print(len(Xtr) , len(Xdev), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1727489615781,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "vUo1EJwqJzs0",
    "outputId": "b6926d3a-389f-4be1-9eae-bf58b6f6b454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625]) torch.int64 torch.int64\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 25],\n",
      "        [ 0, 25, 21],\n",
      "        ...,\n",
      "        [15, 12,  4],\n",
      "        [12,  4,  1],\n",
      "        [ 4,  1, 14]])\n",
      "---\n",
      "tensor([25, 21,  8,  ...,  1, 14,  0])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Ytr.shape, Xtr.dtype, Ytr.dtype)\n",
    "print(Xtr)\n",
    "print(\"---\")\n",
    "print(Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_c8JD9Z8Bd"
   },
   "source": [
    "## Lookup table C, embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1727490140478,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "J7DEHynvMtq7",
    "outputId": "f79ae109-2ddb-40ef-895e-6553c96d2fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5013, -0.8621],\n",
      "        [ 0.1141, -0.4797],\n",
      "        [ 1.0472,  1.5424],\n",
      "        [-1.2521, -1.6304],\n",
      "        [ 0.9521, -0.3236],\n",
      "        [-0.5833, -0.0969],\n",
      "        [-0.5720,  1.8011],\n",
      "        [-0.0412, -0.1098],\n",
      "        [-0.3862, -0.5910],\n",
      "        [ 0.3759,  0.5625],\n",
      "        [ 1.4190, -0.6763],\n",
      "        [ 1.7523,  0.7620],\n",
      "        [ 1.0095,  0.6250],\n",
      "        [ 1.4192,  0.3493],\n",
      "        [-0.8872,  0.1808],\n",
      "        [ 1.9260,  0.9209],\n",
      "        [-0.6208, -2.2271],\n",
      "        [-1.5889,  0.9987],\n",
      "        [ 0.3235,  0.5826],\n",
      "        [ 1.3593, -0.4439],\n",
      "        [-0.4701,  1.0922],\n",
      "        [-1.2478, -0.6459],\n",
      "        [ 1.1586, -1.2548],\n",
      "        [-0.0892,  0.2336],\n",
      "        [ 0.3606,  0.3916],\n",
      "        [ 1.1569,  0.9773],\n",
      "        [ 0.2489,  0.5062]]) \n",
      "------\n",
      "\n",
      "Embeddings:\n",
      "torch.Size([182625, 5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 1.1569,  0.9773]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 1.1569,  0.9773],\n",
       "         [-1.2478, -0.6459]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [-0.3862, -0.5910],\n",
       "         [ 1.9260,  0.9209],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 0.9521, -0.3236]],\n",
       "\n",
       "        [[-0.3862, -0.5910],\n",
       "         [ 1.9260,  0.9209],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 0.9521, -0.3236],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 1.9260,  0.9209],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 0.9521, -0.3236],\n",
       "         [ 0.1141, -0.4797],\n",
       "         [-0.8872,  0.1808]]])"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(1097)\n",
    "\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "print(C, \"\\n------\\n\")\n",
    "\n",
    "# Embedding\n",
    "    # (32,3,2)\n",
    "emb = C[Xtr]\n",
    "\n",
    "# For each size 3 block in X (each row), we get a (3x2) matrix which has all the 2D embeddings for the chars that make up the block\n",
    "  # e.g X[2] = [0,5,13], so emb[2] is [C[0], C[5], C[13]] stacked vertically\n",
    "# print(emb[2])\n",
    "# example = torch.cat((C[0], C[5], C[13]), dim=0).view((BLOCK_SIZE ,EMBEDDING_SIZE))\n",
    "# same as emb[2]\n",
    "# print(example)\n",
    "# print(\"\\n------\\n\")\n",
    "\n",
    "\n",
    "print(\"Embeddings:\")\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck__BTsebPl5"
   },
   "source": [
    "## Different ways to flatten\n",
    "(32,3,2) -> (32,6) so we can keep W to (6, 100)\n",
    "- 3x2=6 determined by block size and embedding size\n",
    "\n",
    "torch.cat, torch.unbind creates new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1727490193315,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "5TGsS6o-Q2HD",
    "outputId": "48929d69-be45-45d1-8069-373cc42e1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor([[ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621, -0.5833, -0.0969],\n",
      "        [ 0.5013, -0.8621, -0.5833, -0.0969,  1.4192,  0.3493],\n",
      "        [-0.5833, -0.0969,  1.4192,  0.3493,  1.4192,  0.3493],\n",
      "        [ 1.4192,  0.3493,  1.4192,  0.3493,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.9260,  0.9209],\n",
      "        [ 0.5013, -0.8621,  1.9260,  0.9209,  1.0095,  0.6250],\n",
      "        [ 1.9260,  0.9209,  1.0095,  0.6250,  0.3759,  0.5625],\n",
      "        [ 1.0095,  0.6250,  0.3759,  0.5625,  1.1586, -1.2548],\n",
      "        [ 0.3759,  0.5625,  1.1586, -1.2548,  0.3759,  0.5625],\n",
      "        [ 1.1586, -1.2548,  0.3759,  0.5625,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.1141, -0.4797,  1.1586, -1.2548],\n",
      "        [ 0.1141, -0.4797,  1.1586, -1.2548,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.3759,  0.5625],\n",
      "        [ 0.5013, -0.8621,  0.3759,  0.5625,  1.3593, -0.4439],\n",
      "        [ 0.3759,  0.5625,  1.3593, -0.4439,  0.1141, -0.4797],\n",
      "        [ 1.3593, -0.4439,  0.1141, -0.4797,  1.0472,  1.5424],\n",
      "        [ 0.1141, -0.4797,  1.0472,  1.5424, -0.5833, -0.0969],\n",
      "        [ 1.0472,  1.5424, -0.5833, -0.0969,  1.0095,  0.6250],\n",
      "        [-0.5833, -0.0969,  1.0095,  0.6250,  1.0095,  0.6250],\n",
      "        [ 1.0095,  0.6250,  1.0095,  0.6250,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.3593, -0.4439],\n",
      "        [ 0.5013, -0.8621,  1.3593, -0.4439,  1.9260,  0.9209],\n",
      "        [ 1.3593, -0.4439,  1.9260,  0.9209, -0.6208, -2.2271],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -0.3862, -0.5910],\n",
      "        [-0.6208, -2.2271, -0.3862, -0.5910,  0.3759,  0.5625],\n",
      "        [-0.3862, -0.5910,  0.3759,  0.5625,  0.1141, -0.4797]])\n"
     ]
    }
   ],
   "source": [
    "### with unbind + cat\n",
    "  # unbind: removes a dimension and returns tuple of each slice\n",
    "  # e.g dim=1, so we slice along 0,1,2 for emb\n",
    "  # get: emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], ...\n",
    "# cat: concatenate\n",
    "sliced = torch.unbind(emb, dim=1)\n",
    "cat_ver = torch.cat(sliced , dim=1)\n",
    "\n",
    "# 32x3x2\n",
    "    # flatten by dim=(1,2), meaning flatten the (3x2) -> 6 each\n",
    "flattened = torch.flatten(emb, 1, 2)\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "emb_view = emb.view((32, BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# all of the above are equivalent\n",
    "print(cat_ver.logical_and(flattened).flatten().all())\n",
    "print(flattened.logical_and(cat_ver).flatten().all())\n",
    "\n",
    "# [C[0], C[0], C[0]]\n",
    "# [C[0], C[0], C[5]]\n",
    "# [C[0], C[5], C[13]]\n",
    "# ...and so on\n",
    "    # we are just putting the embeddings per example into one row, before it was 3 rows\n",
    "print(emb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "In the matrix multiplication emb_view @ W, each row of emb_view represents a flattened embedding (e.g 6 values for a context of size 3, with each character having 2D embeddings). \n",
    "\n",
    "The matrix W has e.g 6 rows (one for each input value) and W_OUT columns (or more, depending on the number of neurons). Each column in W represents the weights for one neuron. \n",
    "\n",
    "The result of the multiplication is a matrix where each row corresponds to the linear combination (dot product) of the input embeddings with each neuron's weights. \n",
    "\n",
    "Bias\n",
    "- Each neuron - column of W - has its own bias\n",
    "- So bias also has W_OUT columns for the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: tensor([ 2.0187e-01,  1.4710e+00,  1.9650e+00,  9.3848e-02, -2.3441e-01,\n",
      "        -4.7062e-01,  1.7981e+00,  1.3198e-01,  2.7150e+00, -7.4946e-01,\n",
      "        -5.1111e-01,  3.9081e-01,  1.6414e+00, -8.1366e-01, -9.3341e-01,\n",
      "        -7.3788e-01,  8.5509e-02, -1.7808e+00,  1.1113e+00, -5.5110e-01,\n",
      "         5.0704e-01, -6.7913e-01, -1.0101e+00,  9.1796e-01,  7.6946e-01,\n",
      "        -4.4623e-01,  1.2764e+00, -1.2066e-01, -4.7289e-01, -1.0512e+00,\n",
      "        -4.3958e-03,  1.6660e-01,  2.3627e+00, -2.6778e-01,  1.3920e+00,\n",
      "        -2.6669e-02, -8.4985e-01,  1.5683e-01,  5.6315e-01,  4.2754e-01,\n",
      "         5.4978e-01,  6.3640e-01,  3.1889e-01, -6.2480e-01, -4.2308e-01,\n",
      "        -1.9012e+00,  1.8052e-01,  7.1911e-01,  1.6683e+00,  2.8072e+00,\n",
      "         2.7626e-01, -5.8916e-01,  7.7466e-02, -5.5283e-01,  1.4628e+00,\n",
      "        -1.7897e-02,  5.1046e-02,  1.7784e+00, -4.5596e-01, -5.8732e-01,\n",
      "         5.9618e-01,  1.8748e-01, -1.0953e+00, -1.1454e-02, -1.4278e+00,\n",
      "        -5.5494e-01,  2.6929e-02, -8.8307e-01,  1.4867e+00,  1.3102e+00,\n",
      "         7.8010e-01,  8.0268e-01, -7.5872e-02, -3.8843e-01,  8.5350e-01,\n",
      "         2.7264e-01,  5.7208e-01, -2.8076e+00, -6.2541e-01,  3.3910e-01,\n",
      "         1.2967e+00,  2.2458e+00, -6.3808e-01, -1.1681e+00, -3.9329e-01,\n",
      "         6.5543e-01, -1.2356e+00, -3.7094e-01,  5.9898e-01, -6.9444e-01,\n",
      "         1.1406e+00, -2.7965e-03, -1.9256e+00,  7.9234e-01, -1.7960e-01,\n",
      "         1.3692e-01, -1.3680e+00,  1.0828e+00,  1.1273e+00,  4.7051e-01]) torch.Size([100])\n",
      "Weights: tensor([[-7.1810e-01,  8.3600e-01, -1.0791e+00,  1.2057e+00,  2.6973e-01,\n",
      "          5.2423e-01, -6.1063e-02, -6.9666e-01, -3.4132e-01,  1.1586e+00,\n",
      "         -1.0619e+00, -1.1610e+00,  4.3032e-01, -1.9583e+00,  9.3387e-02,\n",
      "         -1.0691e+00,  1.5235e+00, -6.9664e-02, -5.6455e-01, -7.9108e-01,\n",
      "          1.0705e-01,  2.7462e-01, -1.1039e+00, -1.4048e+00,  7.8364e-01,\n",
      "          3.6128e-01,  6.5409e-01, -1.1603e+00,  4.6247e-02, -2.9984e-01,\n",
      "         -2.1354e-01,  1.2460e+00, -8.3801e-01,  1.0456e+00,  1.5562e+00,\n",
      "         -2.6961e-01,  7.9354e-01, -8.5949e-02, -1.5612e+00,  3.5904e-01,\n",
      "         -1.2349e+00, -6.2868e-01,  6.5242e-01,  1.4185e-01,  4.7228e-01,\n",
      "          2.1225e-01,  7.2978e-01, -1.7944e+00, -3.9467e-01, -1.2257e-01,\n",
      "         -1.3193e+00,  1.1875e-02,  1.2833e+00,  2.6286e-01,  6.6034e-02,\n",
      "         -1.1686e+00,  3.2837e-01, -1.7837e+00, -1.6474e+00,  1.9871e-01,\n",
      "          1.0863e+00, -6.5903e-01,  1.5766e-01,  1.2503e+00, -2.5422e-01,\n",
      "         -1.4714e-01,  2.1997e-01,  1.3157e+00,  2.2878e-01,  1.1145e+00,\n",
      "         -3.2283e-01, -8.0235e-02,  2.4072e+00, -1.0029e+00, -1.3323e+00,\n",
      "         -8.9409e-01, -1.6066e+00,  1.3429e+00,  9.6337e-01,  1.4746e+00,\n",
      "         -4.4829e-01,  1.4953e-01, -8.8030e-01,  1.1096e-01, -5.8594e-01,\n",
      "         -8.9357e-01,  9.2534e-01, -1.0908e+00,  8.0932e-02,  2.3321e+00,\n",
      "          1.6550e+00, -3.8977e-01, -7.7072e-01,  3.8948e-01,  2.7477e+00,\n",
      "         -1.5581e+00, -2.5622e+00,  4.6791e-01,  1.7387e-01,  1.3312e+00],\n",
      "        [-7.1088e-01,  2.5429e-02,  1.0890e+00,  1.1829e-01,  2.2395e+00,\n",
      "         -1.2319e+00, -2.8846e-01, -1.3064e+00,  4.4597e-01, -1.8175e+00,\n",
      "         -1.3924e-01,  1.3249e+00, -6.2185e-01, -9.7683e-01,  1.0342e+00,\n",
      "          1.7657e-01,  8.3350e-01,  1.5388e-02,  5.7120e-01,  8.5034e-02,\n",
      "         -2.8718e-01,  8.9056e-01,  3.7022e-01,  1.3262e+00, -1.7049e+00,\n",
      "          3.8610e-01, -2.0026e+00,  6.0783e-01, -1.6551e+00, -5.9929e-01,\n",
      "         -3.6991e-01,  2.3647e-01, -1.1753e+00,  5.3637e-01, -9.0471e-01,\n",
      "         -1.2309e+00, -5.2558e-01, -3.8553e-01, -1.0000e+00,  6.1530e-02,\n",
      "         -1.4148e-01, -3.2006e-02, -2.1585e-01,  8.3349e-01, -6.4297e-01,\n",
      "         -1.6277e+00,  7.8763e-01, -5.3737e-01,  1.3131e+00, -5.9624e-01,\n",
      "          8.3240e-01, -3.0251e-01, -4.3890e-01, -6.0381e-01,  7.1719e-01,\n",
      "         -7.8713e-01,  3.3999e-01, -8.1010e-01,  4.6389e-01,  3.9152e-01,\n",
      "         -4.9597e-02, -3.6354e-01, -2.6023e+00,  1.1007e+00, -1.7645e+00,\n",
      "          6.7286e-01, -1.7120e-02, -8.4894e-01, -1.8022e+00,  8.7541e-02,\n",
      "         -1.1445e+00,  2.0378e+00,  2.6010e+00, -2.2385e-01, -1.1921e+00,\n",
      "         -2.0477e-01,  1.0826e+00, -5.8233e-01, -1.2191e+00, -1.2410e+00,\n",
      "         -5.1301e-01,  1.8155e-01, -1.4117e+00, -2.0157e+00, -4.1464e-01,\n",
      "          1.4782e+00,  4.7040e-01, -1.2020e+00,  1.9502e-01, -1.2976e+00,\n",
      "          6.0715e-01, -8.0226e-02, -9.8605e-01,  7.5322e-02, -9.2196e-01,\n",
      "         -1.0853e-01,  1.2442e+00,  1.2436e+00, -1.1895e+00, -3.2544e-01],\n",
      "        [ 8.1460e-01,  4.8126e-01,  6.4170e-01,  7.8046e-01,  7.1301e-01,\n",
      "          2.4376e+00,  7.4449e-02,  9.6073e-01, -8.6273e-01, -8.3088e-02,\n",
      "          5.9447e-02, -7.6873e-01, -6.5726e-01, -5.6905e-01,  1.9764e-01,\n",
      "         -1.7378e+00, -6.6617e-01, -8.9936e-01, -1.2121e+00,  4.4946e-01,\n",
      "          8.8620e-01, -3.0014e-01,  2.0293e+00,  1.4800e+00,  6.6429e-01,\n",
      "          1.1615e+00, -6.1566e-01, -9.6866e-01,  1.0670e+00,  9.4479e-01,\n",
      "          1.4494e+00, -7.3998e-01,  4.7223e-01, -5.5969e-01, -1.4609e-01,\n",
      "         -4.3357e-01, -1.4038e+00,  2.1768e+00, -6.2962e-01, -4.4866e-01,\n",
      "         -2.7296e-02, -2.4778e-01, -5.1016e-01, -1.3836e+00,  7.8202e-01,\n",
      "         -1.0901e+00, -1.3537e+00,  4.2172e-01,  1.6125e-01,  6.1557e-01,\n",
      "          2.7566e-01, -4.2143e-01, -9.7029e-01,  1.9328e+00, -9.1598e-01,\n",
      "         -1.0598e+00,  1.5106e+00,  6.7990e-01, -7.2256e-01, -8.4603e-01,\n",
      "          2.5762e-01, -4.8791e-01,  1.1588e+00,  9.8472e-01,  4.2132e-01,\n",
      "          1.3541e+00,  9.6074e-01,  1.0960e+00,  8.0618e-01, -5.7572e-01,\n",
      "          2.0584e+00,  8.5097e-01, -3.5352e-01,  3.7652e-01, -3.9866e-01,\n",
      "          5.2502e-02,  7.4158e-01, -5.7739e-01, -3.3450e-01, -8.8310e-01,\n",
      "         -1.3411e+00,  1.0866e+00, -1.0402e+00,  1.2738e+00, -2.3889e-01,\n",
      "          1.6581e+00,  9.3606e-02, -1.0642e+00,  5.2892e-01, -1.3660e+00,\n",
      "          8.4573e-01, -4.6098e-02,  1.0687e+00, -1.6301e+00, -9.5060e-01,\n",
      "          3.9265e-01, -1.4449e+00, -1.8935e+00,  8.7514e-01,  1.4432e+00],\n",
      "        [ 1.4128e+00,  5.7770e-01,  3.9623e-01, -1.4493e-01, -5.7405e-01,\n",
      "         -1.2095e-01,  1.3899e+00, -1.2030e+00,  2.4030e-01, -1.6544e-01,\n",
      "          2.0322e+00, -8.2616e-01, -5.3636e-01,  1.3758e+00, -2.3940e-01,\n",
      "         -8.7633e-01, -8.3108e-01,  2.8242e-01,  1.0010e+00, -2.7377e-01,\n",
      "          1.1995e+00,  2.8674e-01,  2.1740e+00, -4.7309e-01, -5.2840e-01,\n",
      "          3.0895e-01, -5.4864e-01,  8.3270e-01, -3.5875e-01,  7.1682e-01,\n",
      "         -5.3256e-01,  7.4503e-01, -4.4002e-01,  6.9609e-01, -2.2072e-01,\n",
      "          1.3650e+00,  2.1544e-01, -9.3033e-02, -9.6285e-01,  3.8327e-01,\n",
      "          6.3439e-01, -4.1400e-01, -1.9272e-01,  1.4764e+00,  8.8626e-01,\n",
      "         -2.9724e-01,  4.7020e-01, -1.0364e+00,  6.8510e-01, -1.9297e-01,\n",
      "          6.4189e-01,  5.6032e-01,  1.2352e+00,  1.8413e+00, -2.2121e+00,\n",
      "          5.7039e-02, -2.0588e-01, -9.3632e-01, -1.7652e+00, -3.7639e-02,\n",
      "         -7.4322e-01, -7.6209e-01, -1.3490e+00,  1.2202e+00,  3.7299e-01,\n",
      "         -4.4424e-01, -3.5138e-01, -3.1173e-02,  3.4857e-01, -2.0550e+00,\n",
      "          9.3395e-01,  3.3232e-01,  3.5048e-02,  1.9604e-02,  4.7809e-01,\n",
      "         -7.2230e-01,  1.6689e+00,  1.2793e+00, -4.1101e-01, -8.7899e-01,\n",
      "          3.2840e-01, -3.4789e-01,  8.5007e-01,  7.6753e-01, -7.4803e-03,\n",
      "          3.2052e-01,  2.0832e-01,  2.2560e-01, -7.7178e-01,  1.0816e+00,\n",
      "         -2.1803e+00,  2.3792e-01, -5.1057e-01, -6.6335e-01,  8.1924e-01,\n",
      "          7.5172e-01,  3.9705e-01,  9.2073e-01, -8.5089e-01, -5.3331e-02],\n",
      "        [ 3.6215e-01, -2.6254e-01,  6.1347e-01,  8.0390e-01,  3.2292e+00,\n",
      "          2.8667e-01, -1.8176e+00, -1.6987e+00,  2.2694e-01, -5.3206e-01,\n",
      "          2.8164e-01,  6.6339e-01,  1.7068e+00,  8.2487e-02, -1.5641e+00,\n",
      "         -5.7801e-01, -4.7712e-01, -9.4715e-01, -2.8117e-01, -4.9719e-01,\n",
      "          7.6347e-01,  5.1177e-01, -4.4473e-01, -1.6531e+00, -7.2736e-01,\n",
      "          1.4863e-01, -1.6997e-01,  3.4705e-01, -2.4451e-02, -5.5490e-01,\n",
      "         -1.4115e+00,  1.4792e+00, -1.8170e+00, -6.6434e-01,  5.7631e-01,\n",
      "          1.7041e+00, -1.0011e+00,  7.5991e-01, -7.4984e-01,  5.8652e-01,\n",
      "         -7.6236e-01,  6.8469e-01, -7.1102e-01, -2.8819e-01, -7.1024e-02,\n",
      "         -1.9569e-01, -2.2118e-01,  1.4781e+00,  5.0949e-01,  7.0875e-01,\n",
      "         -2.3041e+00, -5.2504e-01,  6.0890e-01, -2.3092e+00, -7.2373e-01,\n",
      "          4.3035e-01, -1.1462e+00,  3.7116e-01,  1.3328e+00, -1.9126e-01,\n",
      "         -4.0249e-02, -1.1455e+00,  1.8672e+00, -1.3552e+00, -2.0998e+00,\n",
      "         -8.5932e-01,  9.8540e-01, -2.6434e+00,  2.6961e-01,  1.2891e-01,\n",
      "         -4.6070e-01, -9.3153e-01, -4.2360e-02,  1.4518e+00, -7.7915e-01,\n",
      "         -2.3505e+00, -9.9457e-01,  1.8400e-01, -1.8827e+00, -1.7448e+00,\n",
      "          1.4177e+00,  3.2395e-01, -6.2231e-02,  3.0489e-01,  1.2175e-01,\n",
      "          5.5836e-01, -4.4466e-01,  3.4690e-01, -2.6629e-01,  4.9420e-01,\n",
      "          3.8629e-01, -2.5782e+00,  2.2763e+00,  9.1525e-01,  3.8684e-01,\n",
      "          2.1511e+00,  3.7783e-01,  6.1362e-01, -4.9867e-01, -2.9690e-01],\n",
      "        [ 1.8192e+00,  7.5188e-01, -2.5624e-02, -2.3873e-01,  3.0330e-02,\n",
      "          1.6655e-01, -7.2749e-01, -4.5378e-01,  1.3745e-01, -7.3632e-01,\n",
      "         -2.0109e-01, -1.1377e+00,  7.4136e-01,  1.1940e+00, -1.3213e-01,\n",
      "         -7.9466e-01,  1.5852e-02,  7.4152e-01,  4.9078e-03,  1.7905e+00,\n",
      "          4.2722e-01,  2.9182e-01, -1.1522e+00,  2.9243e-01, -1.4846e-01,\n",
      "         -7.7229e-02, -1.8028e+00, -8.4563e-01, -7.5941e-01,  1.3174e+00,\n",
      "         -1.0508e+00, -2.3263e-01, -3.1366e-01,  1.3844e+00,  3.6892e-01,\n",
      "         -9.4231e-01,  8.4514e-01,  1.6384e+00,  5.4606e-01,  9.9718e-01,\n",
      "         -1.0060e+00, -3.3119e-01,  6.2402e-01, -1.3950e-01, -3.4087e-01,\n",
      "          1.8141e-01,  4.2441e-01, -1.0516e+00,  2.1074e+00, -2.9034e-01,\n",
      "         -9.8687e-01, -1.1533e+00,  1.1101e+00,  1.4474e-01,  4.5651e-01,\n",
      "          1.0404e+00, -8.0491e-01, -1.4995e+00, -2.8694e-01, -1.4160e+00,\n",
      "          1.0657e+00, -1.3080e+00,  1.6061e+00,  7.3484e-01, -5.9020e-01,\n",
      "          1.1070e+00, -1.7763e+00, -7.2777e-01,  1.4331e+00,  9.5111e-01,\n",
      "         -1.2630e+00, -4.2146e-01,  5.7142e-01, -6.2862e-01,  1.6844e+00,\n",
      "         -4.0412e-01, -1.6352e+00, -1.0059e+00,  6.9011e-04, -1.9917e+00,\n",
      "         -1.4201e+00, -1.0875e+00, -2.0102e+00, -1.1761e+00,  2.2165e-01,\n",
      "         -1.2756e+00,  1.9618e+00, -1.6436e+00,  9.9192e-01, -2.2536e+00,\n",
      "         -3.3507e-01, -1.3871e+00, -1.5678e+00, -1.6249e+00,  1.2603e+00,\n",
      "         -1.0253e+00,  3.1168e-01,  1.2904e+00, -1.4584e+00, -1.4469e-01]])\n"
     ]
    }
   ],
   "source": [
    "W_OUT = 100\n",
    "\n",
    "# (6,100)\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN)\n",
    "# (1,100)\n",
    "    # 1 bias per neuron (per column)\n",
    "bias = torch.randn(W_OUT, generator=GEN)\n",
    "print(\"Bias:\", bias, bias.shape)\n",
    "\n",
    "\n",
    "# bias = torch.randn()\n",
    "print(\"Weights:\", W)\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "# h for hidden layer\n",
    "h = emb_view @ W\n",
    "\n",
    "# why this works: bias is (100,)\n",
    "# broadcast: \n",
    "    # h: (32, 100)\n",
    "    # b: (  , 100) -> (1,100), 1 is inferred\n",
    "    # so it copies bias and adds to each row as expected - each column gets a different bias value, for each neuron\n",
    "h = h + bias\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [6.8365e-14, 4.1561e-10, 2.0476e-05, 1.9685e-01, 3.6644e-09, 2.5611e-04,\n",
       "         2.3277e-09, 4.3530e-05, 4.4425e-05, 1.3900e-09, 2.9489e-05, 2.1639e-03,\n",
       "         8.2406e-09, 9.1830e-03, 5.0304e-05, 1.9712e-07, 1.0256e-03, 6.0565e-08,\n",
       "         1.5621e-02, 1.3701e-06, 3.1891e-06, 1.0589e-05, 4.8030e-07, 2.0710e-04,\n",
       "         2.5028e-02, 6.3929e-02, 6.8553e-01],\n",
       "        [1.3785e-10, 7.1116e-05, 1.9685e-11, 3.7020e-08, 3.8112e-07, 6.1939e-05,\n",
       "         1.0589e-03, 2.4612e-02, 5.9960e-05, 8.2954e-06, 2.8608e-03, 1.1369e-06,\n",
       "         2.0626e-08, 1.2665e-06, 1.3825e-03, 6.0687e-05, 5.0792e-06, 2.9640e-02,\n",
       "         2.6060e-12, 1.0480e-08, 8.6222e-06, 5.8362e-08, 3.8414e-02, 6.9616e-01,\n",
       "         1.3612e-06, 2.0560e-01, 4.1925e-08],\n",
       "        [1.1296e-08, 9.9169e-01, 8.2763e-05, 4.7307e-08, 5.2716e-07, 1.1432e-07,\n",
       "         4.5556e-08, 1.5861e-03, 2.5982e-06, 3.0303e-08, 4.2567e-05, 4.3100e-10,\n",
       "         4.2078e-09, 4.7673e-07, 1.3123e-06, 3.5119e-03, 1.8188e-09, 9.2137e-10,\n",
       "         1.0143e-03, 5.2248e-10, 1.5106e-04, 5.9120e-09, 2.2771e-06, 1.9160e-03,\n",
       "         6.2971e-09, 5.7147e-09, 1.9878e-06],\n",
       "        [2.4788e-08, 4.1117e-05, 1.1683e-08, 8.8031e-05, 3.0312e-08, 1.8094e-03,\n",
       "         1.0015e-07, 1.4834e-08, 9.9315e-11, 1.4141e-08, 5.7513e-06, 4.2593e-04,\n",
       "         1.1685e-07, 2.8681e-01, 9.1562e-07, 3.4057e-02, 3.5671e-09, 1.1302e-13,\n",
       "         6.4183e-06, 6.1721e-08, 7.6164e-03, 7.1072e-07, 7.6732e-09, 6.6902e-01,\n",
       "         1.1124e-04, 6.7153e-12, 1.0122e-05],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [1.3228e-10, 5.2603e-01, 6.8273e-09, 7.1021e-09, 1.1829e-07, 8.2693e-05,\n",
       "         7.0980e-05, 1.4303e-01, 4.5650e-06, 1.5805e-07, 7.8844e-03, 1.0340e-06,\n",
       "         8.8627e-08, 4.5004e-07, 4.7476e-03, 6.4546e-07, 3.2316e-07, 8.9055e-03,\n",
       "         1.6497e-09, 3.4401e-08, 1.4834e-07, 2.0662e-07, 2.8947e-03, 1.7122e-01,\n",
       "         8.3633e-07, 1.3513e-01, 6.4751e-07],\n",
       "        [1.2745e-10, 1.0000e+00, 1.6020e-13, 1.8645e-10, 5.1141e-12, 4.2361e-10,\n",
       "         1.4395e-13, 2.1033e-07, 6.0098e-10, 1.2448e-12, 7.6635e-11, 1.1237e-09,\n",
       "         1.3887e-10, 3.6748e-09, 2.8929e-10, 9.9027e-09, 4.6133e-13, 1.2200e-15,\n",
       "         2.7734e-08, 2.8084e-12, 1.5415e-08, 3.5499e-10, 2.3313e-09, 4.0335e-07,\n",
       "         4.8197e-09, 3.6785e-10, 4.0072e-07],\n",
       "        [1.5062e-10, 7.9447e-01, 6.8998e-12, 3.8265e-08, 6.0892e-12, 1.4840e-01,\n",
       "         1.6850e-08, 2.2217e-09, 7.1332e-12, 1.0418e-03, 6.1412e-04, 3.0613e-04,\n",
       "         2.4696e-10, 2.5273e-03, 7.4339e-04, 7.8838e-04, 2.7189e-09, 2.0782e-17,\n",
       "         2.9269e-08, 3.4234e-11, 5.7498e-07, 8.5817e-07, 1.0154e-07, 5.1110e-02,\n",
       "         3.4165e-10, 1.5504e-12, 9.0301e-09],\n",
       "        [9.9902e-10, 1.1785e-09, 3.0751e-11, 3.6634e-06, 2.3274e-05, 2.2613e-12,\n",
       "         8.7994e-07, 6.4068e-09, 9.5297e-13, 3.0796e-10, 1.9769e-11, 3.4849e-10,\n",
       "         4.9741e-11, 5.3920e-11, 2.3951e-08, 2.5569e-09, 5.8760e-14, 5.1701e-12,\n",
       "         6.6037e-09, 5.7791e-12, 9.9997e-01, 9.0664e-11, 2.0022e-11, 1.0661e-06,\n",
       "         1.4851e-11, 1.8723e-14, 5.8557e-14],\n",
       "        [1.1216e-13, 1.3773e-04, 2.3751e-10, 4.4798e-06, 1.0092e-08, 4.3905e-05,\n",
       "         9.5929e-10, 2.1847e-09, 9.9114e-11, 1.4299e-09, 9.3408e-08, 1.6257e-09,\n",
       "         5.2884e-12, 6.7871e-09, 2.2369e-06, 3.5295e-08, 2.5777e-08, 7.6389e-13,\n",
       "         9.9981e-01, 1.1991e-09, 6.5274e-09, 2.5702e-10, 3.7675e-09, 1.0881e-07,\n",
       "         1.2617e-08, 4.5884e-13, 8.0979e-09],\n",
       "        [1.3089e-11, 7.7038e-09, 5.3127e-08, 4.4668e-05, 5.2548e-07, 7.5878e-04,\n",
       "         1.7066e-07, 1.0781e-07, 7.4577e-06, 9.0856e-09, 7.0690e-08, 1.3405e-01,\n",
       "         1.4165e-05, 6.1325e-04, 3.4102e-01, 2.6888e-07, 2.6087e-07, 2.2872e-07,\n",
       "         8.1178e-08, 7.1988e-07, 5.2155e-01, 1.1549e-09, 1.6845e-04, 6.5212e-05,\n",
       "         5.1928e-05, 6.6453e-08, 1.6523e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [3.0727e-11, 7.0382e-08, 9.0770e-05, 2.8462e-01, 4.4219e-03, 5.0848e-03,\n",
       "         2.2958e-04, 1.3853e-02, 5.1958e-02, 5.3968e-07, 1.6313e-04, 6.4986e-02,\n",
       "         6.8246e-08, 3.2213e-03, 3.9932e-02, 3.9735e-05, 2.0214e-04, 7.5298e-05,\n",
       "         2.0479e-02, 1.0849e-05, 1.3969e-01, 4.4869e-05, 3.1109e-05, 1.6134e-01,\n",
       "         1.0194e-01, 4.7200e-03, 1.0287e-01],\n",
       "        [7.1443e-10, 8.3208e-06, 1.3397e-08, 1.1205e-06, 7.6466e-01, 1.5721e-07,\n",
       "         1.3244e-01, 1.1216e-03, 1.5530e-04, 9.2832e-08, 4.5542e-10, 3.3853e-07,\n",
       "         3.4060e-08, 5.6231e-07, 1.9254e-04, 2.3368e-07, 2.6900e-08, 2.8312e-04,\n",
       "         3.6823e-07, 1.0882e-07, 8.5466e-04, 3.7873e-08, 2.7222e-08, 1.0012e-01,\n",
       "         1.5806e-04, 1.8494e-08, 1.4281e-06],\n",
       "        [6.5291e-14, 1.5102e-07, 1.1729e-04, 5.7639e-02, 1.9839e-04, 3.0956e-04,\n",
       "         7.3039e-06, 6.2358e-05, 3.8526e-06, 3.4572e-09, 7.1664e-06, 7.9779e-06,\n",
       "         8.5429e-10, 5.1024e-06, 1.1610e-04, 1.6946e-04, 2.3829e-04, 1.8623e-07,\n",
       "         8.3640e-01, 2.6639e-08, 7.1354e-06, 5.8141e-07, 2.0660e-09, 9.9714e-02,\n",
       "         2.7150e-03, 1.2070e-06, 2.2771e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [5.2470e-11, 7.6938e-03, 1.0777e-05, 5.0911e-03, 1.0572e-06, 3.5413e-03,\n",
       "         6.2156e-06, 1.4944e-01, 1.4113e-01, 1.6016e-07, 2.7195e-02, 4.4333e-02,\n",
       "         2.4544e-03, 8.7723e-03, 4.1838e-03, 1.5251e-05, 1.1605e-03, 5.5702e-05,\n",
       "         6.4435e-04, 7.8044e-04, 9.9097e-06, 7.1613e-03, 1.7232e-01, 7.4297e-03,\n",
       "         1.7301e-01, 2.0271e-01, 4.0855e-02],\n",
       "        [4.2015e-10, 1.6522e-02, 1.3572e-10, 1.3594e-07, 6.9696e-03, 1.1380e-06,\n",
       "         4.4457e-07, 6.5977e-04, 1.6107e-03, 2.7494e-08, 3.9008e-07, 1.2763e-05,\n",
       "         1.0397e-07, 7.4413e-08, 1.6280e-02, 1.5473e-02, 1.3636e-07, 1.3538e-06,\n",
       "         1.4636e-08, 2.6405e-07, 2.7999e-01, 2.0693e-08, 1.5857e-03, 6.6085e-01,\n",
       "         2.5812e-06, 3.1863e-06, 4.2912e-05],\n",
       "        [1.9791e-11, 2.0595e-06, 9.9719e-03, 9.5977e-04, 2.9245e-04, 1.4362e-04,\n",
       "         5.3716e-08, 2.1452e-06, 7.4617e-09, 7.7082e-08, 1.6292e-04, 7.7487e-07,\n",
       "         1.6527e-10, 6.1820e-04, 7.4312e-06, 5.2019e-04, 3.0618e-06, 1.2894e-13,\n",
       "         8.4620e-01, 1.9118e-07, 1.0804e-03, 2.0602e-08, 1.7234e-09, 1.3877e-01,\n",
       "         1.2669e-03, 4.8764e-13, 2.6270e-08],\n",
       "        [3.8494e-13, 7.3627e-01, 1.5682e-08, 6.9540e-09, 1.6663e-13, 4.1843e-04,\n",
       "         2.2122e-04, 3.6886e-07, 4.4483e-06, 3.9973e-03, 1.1547e-05, 1.0779e-02,\n",
       "         2.1516e-04, 2.0246e-04, 4.0275e-04, 4.2318e-07, 1.5429e-07, 3.6351e-07,\n",
       "         1.0134e-12, 2.1719e-07, 4.3730e-10, 1.8890e-05, 9.8278e-02, 4.5051e-05,\n",
       "         2.2433e-11, 1.4914e-01, 4.5380e-07],\n",
       "        [3.4293e-06, 2.5420e-09, 7.8812e-06, 1.4604e-03, 5.9348e-10, 1.0571e-05,\n",
       "         4.2870e-12, 1.2451e-07, 1.5078e-08, 5.8704e-12, 3.9242e-09, 2.5691e-07,\n",
       "         5.6767e-10, 4.0119e-05, 3.8598e-05, 7.5505e-05, 8.8371e-11, 1.2825e-14,\n",
       "         2.6853e-05, 5.5664e-10, 2.1989e-02, 6.9340e-09, 2.2668e-06, 2.2171e-07,\n",
       "         1.0763e-04, 1.4859e-05, 9.7622e-01],\n",
       "        [9.5699e-08, 2.0733e-04, 3.2152e-09, 2.9141e-05, 3.3797e-10, 2.8711e-04,\n",
       "         8.9165e-01, 6.9198e-11, 4.6177e-12, 1.0739e-01, 8.8537e-05, 8.1354e-05,\n",
       "         4.6162e-08, 1.0546e-06, 7.2800e-07, 3.1520e-09, 1.0961e-10, 2.2440e-09,\n",
       "         2.5469e-12, 1.2000e-09, 3.3605e-07, 7.7052e-08, 3.1465e-06, 2.6781e-04,\n",
       "         3.4917e-11, 4.7030e-12, 2.6047e-12],\n",
       "        [2.8478e-08, 6.6653e-01, 1.0830e-05, 1.4612e-08, 1.2338e-08, 5.5271e-07,\n",
       "         3.3243e-08, 2.8572e-01, 8.4457e-04, 3.5494e-08, 1.8938e-04, 6.2184e-10,\n",
       "         8.0545e-09, 3.9368e-08, 1.7321e-04, 3.9506e-02, 6.0118e-07, 1.4032e-10,\n",
       "         1.8904e-04, 6.0291e-09, 8.0527e-05, 2.7112e-06, 9.3329e-05, 6.6551e-03,\n",
       "         4.2669e-08, 7.3339e-08, 7.3088e-07],\n",
       "        [7.5847e-06, 1.7647e-05, 3.9970e-07, 2.8866e-05, 1.8172e-08, 4.0296e-03,\n",
       "         5.9501e-07, 3.4804e-09, 1.2925e-11, 2.8633e-07, 2.7979e-04, 5.0431e-06,\n",
       "         7.6742e-09, 5.2283e-02, 2.0955e-04, 8.9330e-02, 1.8862e-09, 5.8623e-14,\n",
       "         2.1015e-06, 8.6655e-09, 2.9872e-01, 7.3246e-08, 1.0754e-07, 5.5508e-01,\n",
       "         6.8259e-07, 3.2069e-11, 8.3104e-09],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [8.1452e-10, 1.8016e-04, 6.5544e-08, 3.9486e-07, 1.7942e-03, 1.1060e-06,\n",
       "         2.1602e-04, 3.6675e-04, 6.3153e-06, 4.7813e-09, 2.2747e-06, 1.1809e-07,\n",
       "         4.0151e-08, 6.1695e-07, 7.7422e-05, 2.8504e-07, 1.5279e-07, 1.3777e-05,\n",
       "         1.0753e-07, 1.9894e-08, 5.9891e-06, 1.0691e-07, 8.8311e-07, 9.9588e-01,\n",
       "         1.4456e-03, 1.2169e-07, 1.1007e-05],\n",
       "        [7.7901e-12, 9.9992e-01, 6.1512e-12, 2.5135e-12, 1.2922e-09, 9.6660e-09,\n",
       "         1.6270e-10, 2.6213e-05, 5.4496e-08, 1.1987e-11, 2.1822e-06, 2.0637e-08,\n",
       "         1.6035e-09, 5.7746e-11, 1.4566e-08, 4.4576e-08, 2.2200e-10, 1.5848e-08,\n",
       "         1.6301e-09, 1.3031e-10, 1.2187e-08, 2.6812e-10, 5.4765e-09, 1.4868e-05,\n",
       "         5.2247e-10, 3.7397e-05, 1.2600e-06],\n",
       "        [5.7278e-12, 2.3306e-14, 7.1772e-07, 7.7967e-08, 3.1102e-05, 1.2452e-07,\n",
       "         6.8755e-11, 4.1059e-11, 1.9512e-09, 2.4815e-14, 1.4978e-14, 4.9167e-08,\n",
       "         1.4188e-12, 5.8159e-06, 1.3131e-04, 4.6438e-07, 7.4308e-13, 1.4926e-12,\n",
       "         3.6095e-06, 2.8189e-13, 9.9983e-01, 1.9827e-13, 8.4867e-13, 1.6814e-06,\n",
       "         3.0432e-10, 2.2336e-14, 3.1839e-08],\n",
       "        [2.0404e-10, 2.5702e-08, 1.2095e-08, 5.2292e-04, 3.6576e-11, 8.6851e-06,\n",
       "         9.9664e-01, 1.1695e-10, 3.9258e-14, 1.3928e-07, 4.2754e-07, 1.4162e-04,\n",
       "         6.5149e-10, 2.6736e-03, 7.3567e-09, 2.9535e-12, 1.2528e-07, 7.2461e-06,\n",
       "         3.3744e-11, 1.2678e-11, 1.5350e-08, 6.9142e-12, 5.0439e-12, 1.1578e-06,\n",
       "         2.3050e-11, 2.3835e-08, 6.5479e-14],\n",
       "        [8.7365e-15, 5.7650e-10, 5.0590e-03, 2.5860e-06, 9.1061e-07, 2.7300e-08,\n",
       "         6.1033e-05, 1.9516e-02, 3.6828e-02, 1.0858e-08, 3.8272e-09, 3.0933e-06,\n",
       "         1.9118e-12, 2.9697e-09, 8.6193e-06, 2.6454e-07, 2.5217e-03, 1.0188e-02,\n",
       "         7.1391e-08, 1.6662e-06, 2.4008e-08, 7.3626e-10, 1.2556e-04, 1.7289e-06,\n",
       "         7.3063e-01, 1.9229e-01, 2.7674e-03],\n",
       "        [1.1174e-09, 1.1508e-09, 3.5377e-05, 6.8658e-05, 1.1534e-04, 3.6285e-09,\n",
       "         4.2690e-08, 9.5894e-01, 1.8759e-03, 6.1524e-11, 4.6356e-08, 2.2982e-11,\n",
       "         5.4032e-12, 1.4064e-05, 1.7364e-02, 7.6190e-06, 5.5600e-07, 2.6349e-10,\n",
       "         5.0398e-04, 3.2318e-08, 2.0003e-02, 7.7636e-10, 1.1831e-07, 2.9509e-04,\n",
       "         7.5410e-04, 1.9720e-06, 2.1938e-05]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "res = h @ W2 + b2\n",
    "probs = res.softmax(dim=1)\n",
    "\n",
    "# ach row is normalized\n",
    "print(probs.sum(dim=1))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding probabilities for each y label, get NLL Loss\n",
    "torch.arange(m) -> 0,1,2,3...m-1 for row indices\n",
    "Y: e.g [0,5,13,13,6..] -> actual labels. so we end up getting\n",
    "- res[0,0], res[1,5], res[2,13] etc in a vector\n",
    "- This is the predicted probability based on current weights for that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8111e-04, 9.1830e-03, 1.2665e-06, 9.9169e-01, 2.4788e-08, 6.1128e-06,\n",
       "        8.8627e-08, 1.2448e-12, 1.0154e-07, 3.0796e-10, 1.3773e-04, 1.3089e-11,\n",
       "        7.5123e-07, 3.1109e-05, 8.3208e-06, 6.5291e-14, 1.6017e-06, 7.8044e-04,\n",
       "        1.6522e-02, 9.9719e-03, 4.1843e-04, 5.6767e-10, 4.6162e-08, 6.6653e-01,\n",
       "        7.5847e-06, 4.1817e-07, 2.8504e-07, 2.2200e-10, 1.9512e-09, 1.3928e-07,\n",
       "        5.7650e-10, 1.1174e-09])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(Y)\n",
    "probs[torch.arange(m), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5532)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = -probs.log().mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--SETUP--#\n",
    "\n",
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(2147483647)\n",
    "# Number of examples\n",
    "M = len(Xtr) \n",
    "\n",
    "# Params: C, W, bias, W2, b2\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "\n",
    "# Forward pass\n",
    "W_OUT = 300\n",
    "\n",
    "# First layer\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN) # (6,100)\n",
    "bias = torch.randn(W_OUT, generator=GEN) # (1,100): 1 bias per neuron (per column)\n",
    "\n",
    "# Second layer: W_OUT inputs, 27 outputs for log-counts (to become probabilities)\n",
    "W2 = torch.randn((W_OUT, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "parameters = [C, W, bias, W2, b2]\n",
    "for param in parameters:\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.203854560852051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30fb3f940>]"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--TRAINING--#\n",
    "EPOCHS=30000\n",
    "LR=0.05/4\n",
    "BATCH_SIZE=32\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # select a random batch and use that for fwd + backward for faster iteration\n",
    "    # batch_indices = torch.randperm(len(Xtr), generator=GEN)\n",
    "    # batch_indices = batch_indices[:BATCH_SIZE]\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (32,), generator=GEN)\n",
    "\n",
    "    emb = C[Xtr[batch_indices]] # lookup embeddings corresponding to each row -> (M, BLOCK_SIZE, 2) i.e one 2D vector per char in a row of X\n",
    "\n",
    "    # view shares the same underlying elements, just rearranges logically\n",
    "        # (number of examples, number of elems per example)\n",
    "    emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "    \n",
    "    # forward pass: get linear combs + bias for each row, activate with tanh\n",
    "    h = emb_view @ W\n",
    "    h = h + bias\n",
    "    h = h.tanh()\n",
    "    \n",
    "    # forward pass 2nd layer to get 27 outputs per row\n",
    "    probs = h @ W2 + b2\n",
    "    \n",
    "    # this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "    nll_loss = F.cross_entropy(probs, Ytr[batch_indices])\n",
    "    \n",
    "    epochs.append(i)\n",
    "    losses.append(nll_loss.item())\n",
    "    # --Update--#\n",
    "    \n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "    \n",
    "    # backward pass\n",
    "    nll_loss.backward()\n",
    "    \n",
    "    # update\n",
    "    with torch.no_grad():\n",
    "        for param in parameters:\n",
    "            param -= LR * param.grad\n",
    "            \n",
    "print(\"Loss:\", nll_loss.item())\n",
    "\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loss for dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2333, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ydev)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2198, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ytr)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQDGcWn074pA7NdAWB8vTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env 28Sep)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
