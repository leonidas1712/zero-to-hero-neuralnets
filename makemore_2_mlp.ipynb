{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNe_RPhx3Cx"
   },
   "source": [
    "## Makemore Part 2: MLP\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Idea from paper: C is a lookup table (matrix) for embeddings vector of each of the words in V e.g |V|=17k\n",
    "- C: (17000x30)\n",
    "- one-hot encoding of a word: (1x17000)\n",
    "- enc @ C -> (1x30) embedding vector\n",
    "\n",
    "We can get these vectors for multiple input words (words that came before).\n",
    "\n",
    "Pass them to neurons with n_in=30, those neurons are fully connected to a hidden layer, then goes through tanh -> softmax for probabilities.\n",
    "\n",
    "Final output: 17k vector of probabilities for the 17k words in vocab.\n",
    "- Train by using actual next word's index, get the predicted prob, do -log(prob) and backprop etc.\n",
    "\n",
    "---\n",
    "\n",
    "We can use this idea for a character-level model as well.\n",
    "\n",
    "## Architecture\n",
    "- Start with indexes from the chars/words in context\n",
    "- Convert them to respective embeddings vectors in C (lookup)\n",
    "- Stack the vectors' values together, do forward pass with weights matrix and add bias\n",
    "- Activate through tanh\n",
    "- Pass through one more layer to get 27 outputs (one for each unique char in our vocab)\n",
    "- Softmax to get probabilities\n",
    "\n",
    "## Building dataset\n",
    "Hyperparameter: BLOCK_SIZE = 3\n",
    "- block_size is the number of previous chars we consider when predicting next\n",
    "\n",
    "For each word, we add to X with block_size=3 char windows, Y has the char that comes after\n",
    "- e.g '.emma': (..., e), (..e, m), (.em, m), (emm, a), (mma, .)\n",
    "- So each word contributes n+1 examples as before, n = len(word)\n",
    "\n",
    "## Lookup table\n",
    "Lookup table: C = (27,2) random init\n",
    "- In paper, they compress 17k words of vocab into Rn of n=30\n",
    "- So we do similar here for 27 unique chars -> embeddings of size 2\n",
    "\n",
    "Previously, we used one-hot encoding to lookup with enc @ W\n",
    "- But this is just the same as doing W[idx] due to all the zeroes\n",
    "\n",
    "In PyTorch, we can just do C[X] and it will work\n",
    "- produces (32,3,2) - one 2D vector for each encoded char\n",
    "- Or another way to think about it, one (3x2) vector for each row in X. 3 because BLOCK_SIZE=3, so each row in X has 3 elements. For each of those chars, we want one 2D vector - its embedding\n",
    "\n",
    "## F.cross_entropy\n",
    "F.cross_entropy(logits, targets) is the same as doing:\n",
    "\n",
    "```python\n",
    "logits = logits.softmax(dim=1)\n",
    "logits = logits[torch.arange(M), Y]\n",
    "nll_loss = -logits.prob().mean()\n",
    "```\n",
    "i.e same as softmax -> select corresponding probabilities for targets in Y -> get NLL Loss (mean)\n",
    "\n",
    "F.cross_entropy is **better** because PyTorch can optimise and not create new memory, and it can use **fused kernels** to cluster ops together and run them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 10072,
     "status": "ok",
     "timestamp": 1727498203827,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "E1zlN5r9xtTz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1727488188151,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "CNgmYr9Qx79y",
    "outputId": "2428084a-e4ac-42fe-9e84-8dc9182297ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n",
      "32033\n",
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "with open('names.txt', 'r') as names_file:\n",
    "  names = names_file.read().splitlines()\n",
    "\n",
    "print(names[:5])\n",
    "print(len(names))\n",
    "\n",
    "# build lookups\n",
    "uniq = ['.'] + sorted(list(set(''.join(names))))\n",
    "stoi = { char: idx for idx, char in enumerate(uniq)}\n",
    "itos = { idx: char for char,idx in stoi.items() }\n",
    "\n",
    "print(stoi)\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl1lLlcpJtBM"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1727488190995,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "INrlJJdu70z3",
    "outputId": "4fa9ff58-4349-46f0-fbb5-5b7f861df269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -> e\n",
      "..e -> m\n",
      ".em -> m\n",
      "emm -> a\n",
      "mma -> .\n",
      "olivia\n",
      "... -> o\n",
      "..o -> l\n",
      ".ol -> i\n",
      "oli -> v\n",
      "liv -> i\n",
      "ivi -> a\n",
      "via -> .\n",
      "ava\n",
      "... -> a\n",
      "..a -> v\n",
      ".av -> a\n",
      "ava -> .\n",
      "isabella\n",
      "... -> i\n",
      "..i -> s\n",
      ".is -> a\n",
      "isa -> b\n",
      "sab -> e\n",
      "abe -> l\n",
      "bel -> l\n",
      "ell -> a\n",
      "lla -> .\n",
      "sophia\n",
      "... -> s\n",
      "..s -> o\n",
      ".so -> p\n",
      "sop -> h\n",
      "oph -> i\n",
      "phi -> a\n",
      "hia -> .\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BLOCK_SIZE = 3\n",
    "EMBEDDING_SIZE=2\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for word in names[:5]:\n",
    "  print(word)\n",
    "  word = word + '.'\n",
    "  block = [0] * BLOCK_SIZE # ... (empty context at start)\n",
    "\n",
    "  for char in word:\n",
    "    char_idx = stoi[char]\n",
    "    X.append(block)\n",
    "    Y.append(char_idx)\n",
    "\n",
    "    block_str = ''.join(list(map(lambda i: itos[i], block)))\n",
    "    print(f'{block_str} -> {char}')\n",
    "\n",
    "    # update block: roll over sliding window\n",
    "    block = block[1:] + [char_idx]\n",
    "\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1727489615781,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "vUo1EJwqJzs0",
    "outputId": "b6926d3a-389f-4be1-9eae-bf58b6f6b454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32]) torch.int64 torch.int64\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 15],\n",
      "        [ 0, 15, 12],\n",
      "        [15, 12,  9],\n",
      "        [12,  9, 22],\n",
      "        [ 9, 22,  9],\n",
      "        [22,  9,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1, 22],\n",
      "        [ 1, 22,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  9],\n",
      "        [ 0,  9, 19],\n",
      "        [ 9, 19,  1],\n",
      "        [19,  1,  2],\n",
      "        [ 1,  2,  5],\n",
      "        [ 2,  5, 12],\n",
      "        [ 5, 12, 12],\n",
      "        [12, 12,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 19],\n",
      "        [ 0, 19, 15],\n",
      "        [19, 15, 16],\n",
      "        [15, 16,  8],\n",
      "        [16,  8,  9],\n",
      "        [ 8,  9,  1]])\n",
      "---\n",
      "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
      "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape, X.dtype, Y.dtype)\n",
    "print(X)\n",
    "print(\"---\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_c8JD9Z8Bd"
   },
   "source": [
    "## Lookup table C, embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1727490140478,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "J7DEHynvMtq7",
    "outputId": "f79ae109-2ddb-40ef-895e-6553c96d2fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5013, -0.8621],\n",
      "        [ 0.1141, -0.4797],\n",
      "        [ 1.0472,  1.5424],\n",
      "        [-1.2521, -1.6304],\n",
      "        [ 0.9521, -0.3236],\n",
      "        [-0.5833, -0.0969],\n",
      "        [-0.5720,  1.8011],\n",
      "        [-0.0412, -0.1098],\n",
      "        [-0.3862, -0.5910],\n",
      "        [ 0.3759,  0.5625],\n",
      "        [ 1.4190, -0.6763],\n",
      "        [ 1.7523,  0.7620],\n",
      "        [ 1.0095,  0.6250],\n",
      "        [ 1.4192,  0.3493],\n",
      "        [-0.8872,  0.1808],\n",
      "        [ 1.9260,  0.9209],\n",
      "        [-0.6208, -2.2271],\n",
      "        [-1.5889,  0.9987],\n",
      "        [ 0.3235,  0.5826],\n",
      "        [ 1.3593, -0.4439],\n",
      "        [-0.4701,  1.0922],\n",
      "        [-1.2478, -0.6459],\n",
      "        [ 1.1586, -1.2548],\n",
      "        [-0.0892,  0.2336],\n",
      "        [ 0.3606,  0.3916],\n",
      "        [ 1.1569,  0.9773],\n",
      "        [ 0.2489,  0.5062]]) \n",
      "------\n",
      "\n",
      "Embeddings:\n",
      "torch.Size([32, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [-0.5833, -0.0969]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [-0.5833, -0.0969],\n",
       "         [ 1.4192,  0.3493]],\n",
       "\n",
       "        [[-0.5833, -0.0969],\n",
       "         [ 1.4192,  0.3493],\n",
       "         [ 1.4192,  0.3493]],\n",
       "\n",
       "        [[ 1.4192,  0.3493],\n",
       "         [ 1.4192,  0.3493],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 1.9260,  0.9209]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 1.9260,  0.9209],\n",
       "         [ 1.0095,  0.6250]],\n",
       "\n",
       "        [[ 1.9260,  0.9209],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 0.3759,  0.5625]],\n",
       "\n",
       "        [[ 1.0095,  0.6250],\n",
       "         [ 0.3759,  0.5625],\n",
       "         [ 1.1586, -1.2548]],\n",
       "\n",
       "        [[ 0.3759,  0.5625],\n",
       "         [ 1.1586, -1.2548],\n",
       "         [ 0.3759,  0.5625]],\n",
       "\n",
       "        [[ 1.1586, -1.2548],\n",
       "         [ 0.3759,  0.5625],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.1141, -0.4797],\n",
       "         [ 1.1586, -1.2548]],\n",
       "\n",
       "        [[ 0.1141, -0.4797],\n",
       "         [ 1.1586, -1.2548],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.3759,  0.5625]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.3759,  0.5625],\n",
       "         [ 1.3593, -0.4439]],\n",
       "\n",
       "        [[ 0.3759,  0.5625],\n",
       "         [ 1.3593, -0.4439],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 1.3593, -0.4439],\n",
       "         [ 0.1141, -0.4797],\n",
       "         [ 1.0472,  1.5424]],\n",
       "\n",
       "        [[ 0.1141, -0.4797],\n",
       "         [ 1.0472,  1.5424],\n",
       "         [-0.5833, -0.0969]],\n",
       "\n",
       "        [[ 1.0472,  1.5424],\n",
       "         [-0.5833, -0.0969],\n",
       "         [ 1.0095,  0.6250]],\n",
       "\n",
       "        [[-0.5833, -0.0969],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 1.0095,  0.6250]],\n",
       "\n",
       "        [[ 1.0095,  0.6250],\n",
       "         [ 1.0095,  0.6250],\n",
       "         [ 0.1141, -0.4797]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 0.5013, -0.8621],\n",
       "         [ 1.3593, -0.4439]],\n",
       "\n",
       "        [[ 0.5013, -0.8621],\n",
       "         [ 1.3593, -0.4439],\n",
       "         [ 1.9260,  0.9209]],\n",
       "\n",
       "        [[ 1.3593, -0.4439],\n",
       "         [ 1.9260,  0.9209],\n",
       "         [-0.6208, -2.2271]],\n",
       "\n",
       "        [[ 1.9260,  0.9209],\n",
       "         [-0.6208, -2.2271],\n",
       "         [-0.3862, -0.5910]],\n",
       "\n",
       "        [[-0.6208, -2.2271],\n",
       "         [-0.3862, -0.5910],\n",
       "         [ 0.3759,  0.5625]],\n",
       "\n",
       "        [[-0.3862, -0.5910],\n",
       "         [ 0.3759,  0.5625],\n",
       "         [ 0.1141, -0.4797]]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(1097)\n",
    "\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "print(C, \"\\n------\\n\")\n",
    "\n",
    "# Embedding\n",
    "    # (32,3,2)\n",
    "emb = C[X]\n",
    "\n",
    "# For each size 3 block in X (each row), we get a (3x2) matrix which has all the 2D embeddings for the chars that make up the block\n",
    "  # e.g X[2] = [0,5,13], so emb[2] is [C[0], C[5], C[13]] stacked vertically\n",
    "# print(emb[2])\n",
    "# example = torch.cat((C[0], C[5], C[13]), dim=0).view((BLOCK_SIZE ,EMBEDDING_SIZE))\n",
    "# same as emb[2]\n",
    "# print(example)\n",
    "# print(\"\\n------\\n\")\n",
    "\n",
    "\n",
    "print(\"Embeddings:\")\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck__BTsebPl5"
   },
   "source": [
    "## Different ways to flatten\n",
    "(32,3,2) -> (32,6) so we can keep W to (6, 100)\n",
    "- 3x2=6 determined by block size and embedding size\n",
    "\n",
    "torch.cat, torch.unbind creates new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1727490193315,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "5TGsS6o-Q2HD",
    "outputId": "48929d69-be45-45d1-8069-373cc42e1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor([[ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621, -0.5833, -0.0969],\n",
      "        [ 0.5013, -0.8621, -0.5833, -0.0969,  1.4192,  0.3493],\n",
      "        [-0.5833, -0.0969,  1.4192,  0.3493,  1.4192,  0.3493],\n",
      "        [ 1.4192,  0.3493,  1.4192,  0.3493,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.9260,  0.9209],\n",
      "        [ 0.5013, -0.8621,  1.9260,  0.9209,  1.0095,  0.6250],\n",
      "        [ 1.9260,  0.9209,  1.0095,  0.6250,  0.3759,  0.5625],\n",
      "        [ 1.0095,  0.6250,  0.3759,  0.5625,  1.1586, -1.2548],\n",
      "        [ 0.3759,  0.5625,  1.1586, -1.2548,  0.3759,  0.5625],\n",
      "        [ 1.1586, -1.2548,  0.3759,  0.5625,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.1141, -0.4797,  1.1586, -1.2548],\n",
      "        [ 0.1141, -0.4797,  1.1586, -1.2548,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.3759,  0.5625],\n",
      "        [ 0.5013, -0.8621,  0.3759,  0.5625,  1.3593, -0.4439],\n",
      "        [ 0.3759,  0.5625,  1.3593, -0.4439,  0.1141, -0.4797],\n",
      "        [ 1.3593, -0.4439,  0.1141, -0.4797,  1.0472,  1.5424],\n",
      "        [ 0.1141, -0.4797,  1.0472,  1.5424, -0.5833, -0.0969],\n",
      "        [ 1.0472,  1.5424, -0.5833, -0.0969,  1.0095,  0.6250],\n",
      "        [-0.5833, -0.0969,  1.0095,  0.6250,  1.0095,  0.6250],\n",
      "        [ 1.0095,  0.6250,  1.0095,  0.6250,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.3593, -0.4439],\n",
      "        [ 0.5013, -0.8621,  1.3593, -0.4439,  1.9260,  0.9209],\n",
      "        [ 1.3593, -0.4439,  1.9260,  0.9209, -0.6208, -2.2271],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -0.3862, -0.5910],\n",
      "        [-0.6208, -2.2271, -0.3862, -0.5910,  0.3759,  0.5625],\n",
      "        [-0.3862, -0.5910,  0.3759,  0.5625,  0.1141, -0.4797]])\n"
     ]
    }
   ],
   "source": [
    "### with unbind + cat\n",
    "  # unbind: removes a dimension and returns tuple of each slice\n",
    "  # e.g dim=1, so we slice along 0,1,2 for emb\n",
    "  # get: emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], ...\n",
    "# cat: concatenate\n",
    "sliced = torch.unbind(emb, dim=1)\n",
    "cat_ver = torch.cat(sliced , dim=1)\n",
    "\n",
    "# 32x3x2\n",
    "    # flatten by dim=(1,2), meaning flatten the (3x2) -> 6 each\n",
    "flattened = torch.flatten(emb, 1, 2)\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "emb_view = emb.view((32, BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# all of the above are equivalent\n",
    "print(cat_ver.logical_and(flattened).flatten().all())\n",
    "print(flattened.logical_and(cat_ver).flatten().all())\n",
    "\n",
    "# [C[0], C[0], C[0]]\n",
    "# [C[0], C[0], C[5]]\n",
    "# [C[0], C[5], C[13]]\n",
    "# ...and so on\n",
    "    # we are just putting the embeddings per example into one row, before it was 3 rows\n",
    "print(emb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "In the matrix multiplication emb_view @ W, each row of emb_view represents a flattened embedding (e.g 6 values for a context of size 3, with each character having 2D embeddings). \n",
    "\n",
    "The matrix W has e.g 6 rows (one for each input value) and W_OUT columns (or more, depending on the number of neurons). Each column in W represents the weights for one neuron. \n",
    "\n",
    "The result of the multiplication is a matrix where each row corresponds to the linear combination (dot product) of the input embeddings with each neuron's weights. \n",
    "\n",
    "Bias\n",
    "- Each neuron - column of W - has its own bias\n",
    "- So bias also has W_OUT columns for the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: tensor([ 2.0187e-01,  1.4710e+00,  1.9650e+00,  9.3848e-02, -2.3441e-01,\n",
      "        -4.7062e-01,  1.7981e+00,  1.3198e-01,  2.7150e+00, -7.4946e-01,\n",
      "        -5.1111e-01,  3.9081e-01,  1.6414e+00, -8.1366e-01, -9.3341e-01,\n",
      "        -7.3788e-01,  8.5509e-02, -1.7808e+00,  1.1113e+00, -5.5110e-01,\n",
      "         5.0704e-01, -6.7913e-01, -1.0101e+00,  9.1796e-01,  7.6946e-01,\n",
      "        -4.4623e-01,  1.2764e+00, -1.2066e-01, -4.7289e-01, -1.0512e+00,\n",
      "        -4.3958e-03,  1.6660e-01,  2.3627e+00, -2.6778e-01,  1.3920e+00,\n",
      "        -2.6669e-02, -8.4985e-01,  1.5683e-01,  5.6315e-01,  4.2754e-01,\n",
      "         5.4978e-01,  6.3640e-01,  3.1889e-01, -6.2480e-01, -4.2308e-01,\n",
      "        -1.9012e+00,  1.8052e-01,  7.1911e-01,  1.6683e+00,  2.8072e+00,\n",
      "         2.7626e-01, -5.8916e-01,  7.7466e-02, -5.5283e-01,  1.4628e+00,\n",
      "        -1.7897e-02,  5.1046e-02,  1.7784e+00, -4.5596e-01, -5.8732e-01,\n",
      "         5.9618e-01,  1.8748e-01, -1.0953e+00, -1.1454e-02, -1.4278e+00,\n",
      "        -5.5494e-01,  2.6929e-02, -8.8307e-01,  1.4867e+00,  1.3102e+00,\n",
      "         7.8010e-01,  8.0268e-01, -7.5872e-02, -3.8843e-01,  8.5350e-01,\n",
      "         2.7264e-01,  5.7208e-01, -2.8076e+00, -6.2541e-01,  3.3910e-01,\n",
      "         1.2967e+00,  2.2458e+00, -6.3808e-01, -1.1681e+00, -3.9329e-01,\n",
      "         6.5543e-01, -1.2356e+00, -3.7094e-01,  5.9898e-01, -6.9444e-01,\n",
      "         1.1406e+00, -2.7965e-03, -1.9256e+00,  7.9234e-01, -1.7960e-01,\n",
      "         1.3692e-01, -1.3680e+00,  1.0828e+00,  1.1273e+00,  4.7051e-01]) torch.Size([100])\n",
      "Weights: tensor([[-7.1810e-01,  8.3600e-01, -1.0791e+00,  1.2057e+00,  2.6973e-01,\n",
      "          5.2423e-01, -6.1063e-02, -6.9666e-01, -3.4132e-01,  1.1586e+00,\n",
      "         -1.0619e+00, -1.1610e+00,  4.3032e-01, -1.9583e+00,  9.3387e-02,\n",
      "         -1.0691e+00,  1.5235e+00, -6.9664e-02, -5.6455e-01, -7.9108e-01,\n",
      "          1.0705e-01,  2.7462e-01, -1.1039e+00, -1.4048e+00,  7.8364e-01,\n",
      "          3.6128e-01,  6.5409e-01, -1.1603e+00,  4.6247e-02, -2.9984e-01,\n",
      "         -2.1354e-01,  1.2460e+00, -8.3801e-01,  1.0456e+00,  1.5562e+00,\n",
      "         -2.6961e-01,  7.9354e-01, -8.5949e-02, -1.5612e+00,  3.5904e-01,\n",
      "         -1.2349e+00, -6.2868e-01,  6.5242e-01,  1.4185e-01,  4.7228e-01,\n",
      "          2.1225e-01,  7.2978e-01, -1.7944e+00, -3.9467e-01, -1.2257e-01,\n",
      "         -1.3193e+00,  1.1875e-02,  1.2833e+00,  2.6286e-01,  6.6034e-02,\n",
      "         -1.1686e+00,  3.2837e-01, -1.7837e+00, -1.6474e+00,  1.9871e-01,\n",
      "          1.0863e+00, -6.5903e-01,  1.5766e-01,  1.2503e+00, -2.5422e-01,\n",
      "         -1.4714e-01,  2.1997e-01,  1.3157e+00,  2.2878e-01,  1.1145e+00,\n",
      "         -3.2283e-01, -8.0235e-02,  2.4072e+00, -1.0029e+00, -1.3323e+00,\n",
      "         -8.9409e-01, -1.6066e+00,  1.3429e+00,  9.6337e-01,  1.4746e+00,\n",
      "         -4.4829e-01,  1.4953e-01, -8.8030e-01,  1.1096e-01, -5.8594e-01,\n",
      "         -8.9357e-01,  9.2534e-01, -1.0908e+00,  8.0932e-02,  2.3321e+00,\n",
      "          1.6550e+00, -3.8977e-01, -7.7072e-01,  3.8948e-01,  2.7477e+00,\n",
      "         -1.5581e+00, -2.5622e+00,  4.6791e-01,  1.7387e-01,  1.3312e+00],\n",
      "        [-7.1088e-01,  2.5429e-02,  1.0890e+00,  1.1829e-01,  2.2395e+00,\n",
      "         -1.2319e+00, -2.8846e-01, -1.3064e+00,  4.4597e-01, -1.8175e+00,\n",
      "         -1.3924e-01,  1.3249e+00, -6.2185e-01, -9.7683e-01,  1.0342e+00,\n",
      "          1.7657e-01,  8.3350e-01,  1.5388e-02,  5.7120e-01,  8.5034e-02,\n",
      "         -2.8718e-01,  8.9056e-01,  3.7022e-01,  1.3262e+00, -1.7049e+00,\n",
      "          3.8610e-01, -2.0026e+00,  6.0783e-01, -1.6551e+00, -5.9929e-01,\n",
      "         -3.6991e-01,  2.3647e-01, -1.1753e+00,  5.3637e-01, -9.0471e-01,\n",
      "         -1.2309e+00, -5.2558e-01, -3.8553e-01, -1.0000e+00,  6.1530e-02,\n",
      "         -1.4148e-01, -3.2006e-02, -2.1585e-01,  8.3349e-01, -6.4297e-01,\n",
      "         -1.6277e+00,  7.8763e-01, -5.3737e-01,  1.3131e+00, -5.9624e-01,\n",
      "          8.3240e-01, -3.0251e-01, -4.3890e-01, -6.0381e-01,  7.1719e-01,\n",
      "         -7.8713e-01,  3.3999e-01, -8.1010e-01,  4.6389e-01,  3.9152e-01,\n",
      "         -4.9597e-02, -3.6354e-01, -2.6023e+00,  1.1007e+00, -1.7645e+00,\n",
      "          6.7286e-01, -1.7120e-02, -8.4894e-01, -1.8022e+00,  8.7541e-02,\n",
      "         -1.1445e+00,  2.0378e+00,  2.6010e+00, -2.2385e-01, -1.1921e+00,\n",
      "         -2.0477e-01,  1.0826e+00, -5.8233e-01, -1.2191e+00, -1.2410e+00,\n",
      "         -5.1301e-01,  1.8155e-01, -1.4117e+00, -2.0157e+00, -4.1464e-01,\n",
      "          1.4782e+00,  4.7040e-01, -1.2020e+00,  1.9502e-01, -1.2976e+00,\n",
      "          6.0715e-01, -8.0226e-02, -9.8605e-01,  7.5322e-02, -9.2196e-01,\n",
      "         -1.0853e-01,  1.2442e+00,  1.2436e+00, -1.1895e+00, -3.2544e-01],\n",
      "        [ 8.1460e-01,  4.8126e-01,  6.4170e-01,  7.8046e-01,  7.1301e-01,\n",
      "          2.4376e+00,  7.4449e-02,  9.6073e-01, -8.6273e-01, -8.3088e-02,\n",
      "          5.9447e-02, -7.6873e-01, -6.5726e-01, -5.6905e-01,  1.9764e-01,\n",
      "         -1.7378e+00, -6.6617e-01, -8.9936e-01, -1.2121e+00,  4.4946e-01,\n",
      "          8.8620e-01, -3.0014e-01,  2.0293e+00,  1.4800e+00,  6.6429e-01,\n",
      "          1.1615e+00, -6.1566e-01, -9.6866e-01,  1.0670e+00,  9.4479e-01,\n",
      "          1.4494e+00, -7.3998e-01,  4.7223e-01, -5.5969e-01, -1.4609e-01,\n",
      "         -4.3357e-01, -1.4038e+00,  2.1768e+00, -6.2962e-01, -4.4866e-01,\n",
      "         -2.7296e-02, -2.4778e-01, -5.1016e-01, -1.3836e+00,  7.8202e-01,\n",
      "         -1.0901e+00, -1.3537e+00,  4.2172e-01,  1.6125e-01,  6.1557e-01,\n",
      "          2.7566e-01, -4.2143e-01, -9.7029e-01,  1.9328e+00, -9.1598e-01,\n",
      "         -1.0598e+00,  1.5106e+00,  6.7990e-01, -7.2256e-01, -8.4603e-01,\n",
      "          2.5762e-01, -4.8791e-01,  1.1588e+00,  9.8472e-01,  4.2132e-01,\n",
      "          1.3541e+00,  9.6074e-01,  1.0960e+00,  8.0618e-01, -5.7572e-01,\n",
      "          2.0584e+00,  8.5097e-01, -3.5352e-01,  3.7652e-01, -3.9866e-01,\n",
      "          5.2502e-02,  7.4158e-01, -5.7739e-01, -3.3450e-01, -8.8310e-01,\n",
      "         -1.3411e+00,  1.0866e+00, -1.0402e+00,  1.2738e+00, -2.3889e-01,\n",
      "          1.6581e+00,  9.3606e-02, -1.0642e+00,  5.2892e-01, -1.3660e+00,\n",
      "          8.4573e-01, -4.6098e-02,  1.0687e+00, -1.6301e+00, -9.5060e-01,\n",
      "          3.9265e-01, -1.4449e+00, -1.8935e+00,  8.7514e-01,  1.4432e+00],\n",
      "        [ 1.4128e+00,  5.7770e-01,  3.9623e-01, -1.4493e-01, -5.7405e-01,\n",
      "         -1.2095e-01,  1.3899e+00, -1.2030e+00,  2.4030e-01, -1.6544e-01,\n",
      "          2.0322e+00, -8.2616e-01, -5.3636e-01,  1.3758e+00, -2.3940e-01,\n",
      "         -8.7633e-01, -8.3108e-01,  2.8242e-01,  1.0010e+00, -2.7377e-01,\n",
      "          1.1995e+00,  2.8674e-01,  2.1740e+00, -4.7309e-01, -5.2840e-01,\n",
      "          3.0895e-01, -5.4864e-01,  8.3270e-01, -3.5875e-01,  7.1682e-01,\n",
      "         -5.3256e-01,  7.4503e-01, -4.4002e-01,  6.9609e-01, -2.2072e-01,\n",
      "          1.3650e+00,  2.1544e-01, -9.3033e-02, -9.6285e-01,  3.8327e-01,\n",
      "          6.3439e-01, -4.1400e-01, -1.9272e-01,  1.4764e+00,  8.8626e-01,\n",
      "         -2.9724e-01,  4.7020e-01, -1.0364e+00,  6.8510e-01, -1.9297e-01,\n",
      "          6.4189e-01,  5.6032e-01,  1.2352e+00,  1.8413e+00, -2.2121e+00,\n",
      "          5.7039e-02, -2.0588e-01, -9.3632e-01, -1.7652e+00, -3.7639e-02,\n",
      "         -7.4322e-01, -7.6209e-01, -1.3490e+00,  1.2202e+00,  3.7299e-01,\n",
      "         -4.4424e-01, -3.5138e-01, -3.1173e-02,  3.4857e-01, -2.0550e+00,\n",
      "          9.3395e-01,  3.3232e-01,  3.5048e-02,  1.9604e-02,  4.7809e-01,\n",
      "         -7.2230e-01,  1.6689e+00,  1.2793e+00, -4.1101e-01, -8.7899e-01,\n",
      "          3.2840e-01, -3.4789e-01,  8.5007e-01,  7.6753e-01, -7.4803e-03,\n",
      "          3.2052e-01,  2.0832e-01,  2.2560e-01, -7.7178e-01,  1.0816e+00,\n",
      "         -2.1803e+00,  2.3792e-01, -5.1057e-01, -6.6335e-01,  8.1924e-01,\n",
      "          7.5172e-01,  3.9705e-01,  9.2073e-01, -8.5089e-01, -5.3331e-02],\n",
      "        [ 3.6215e-01, -2.6254e-01,  6.1347e-01,  8.0390e-01,  3.2292e+00,\n",
      "          2.8667e-01, -1.8176e+00, -1.6987e+00,  2.2694e-01, -5.3206e-01,\n",
      "          2.8164e-01,  6.6339e-01,  1.7068e+00,  8.2487e-02, -1.5641e+00,\n",
      "         -5.7801e-01, -4.7712e-01, -9.4715e-01, -2.8117e-01, -4.9719e-01,\n",
      "          7.6347e-01,  5.1177e-01, -4.4473e-01, -1.6531e+00, -7.2736e-01,\n",
      "          1.4863e-01, -1.6997e-01,  3.4705e-01, -2.4451e-02, -5.5490e-01,\n",
      "         -1.4115e+00,  1.4792e+00, -1.8170e+00, -6.6434e-01,  5.7631e-01,\n",
      "          1.7041e+00, -1.0011e+00,  7.5991e-01, -7.4984e-01,  5.8652e-01,\n",
      "         -7.6236e-01,  6.8469e-01, -7.1102e-01, -2.8819e-01, -7.1024e-02,\n",
      "         -1.9569e-01, -2.2118e-01,  1.4781e+00,  5.0949e-01,  7.0875e-01,\n",
      "         -2.3041e+00, -5.2504e-01,  6.0890e-01, -2.3092e+00, -7.2373e-01,\n",
      "          4.3035e-01, -1.1462e+00,  3.7116e-01,  1.3328e+00, -1.9126e-01,\n",
      "         -4.0249e-02, -1.1455e+00,  1.8672e+00, -1.3552e+00, -2.0998e+00,\n",
      "         -8.5932e-01,  9.8540e-01, -2.6434e+00,  2.6961e-01,  1.2891e-01,\n",
      "         -4.6070e-01, -9.3153e-01, -4.2360e-02,  1.4518e+00, -7.7915e-01,\n",
      "         -2.3505e+00, -9.9457e-01,  1.8400e-01, -1.8827e+00, -1.7448e+00,\n",
      "          1.4177e+00,  3.2395e-01, -6.2231e-02,  3.0489e-01,  1.2175e-01,\n",
      "          5.5836e-01, -4.4466e-01,  3.4690e-01, -2.6629e-01,  4.9420e-01,\n",
      "          3.8629e-01, -2.5782e+00,  2.2763e+00,  9.1525e-01,  3.8684e-01,\n",
      "          2.1511e+00,  3.7783e-01,  6.1362e-01, -4.9867e-01, -2.9690e-01],\n",
      "        [ 1.8192e+00,  7.5188e-01, -2.5624e-02, -2.3873e-01,  3.0330e-02,\n",
      "          1.6655e-01, -7.2749e-01, -4.5378e-01,  1.3745e-01, -7.3632e-01,\n",
      "         -2.0109e-01, -1.1377e+00,  7.4136e-01,  1.1940e+00, -1.3213e-01,\n",
      "         -7.9466e-01,  1.5852e-02,  7.4152e-01,  4.9078e-03,  1.7905e+00,\n",
      "          4.2722e-01,  2.9182e-01, -1.1522e+00,  2.9243e-01, -1.4846e-01,\n",
      "         -7.7229e-02, -1.8028e+00, -8.4563e-01, -7.5941e-01,  1.3174e+00,\n",
      "         -1.0508e+00, -2.3263e-01, -3.1366e-01,  1.3844e+00,  3.6892e-01,\n",
      "         -9.4231e-01,  8.4514e-01,  1.6384e+00,  5.4606e-01,  9.9718e-01,\n",
      "         -1.0060e+00, -3.3119e-01,  6.2402e-01, -1.3950e-01, -3.4087e-01,\n",
      "          1.8141e-01,  4.2441e-01, -1.0516e+00,  2.1074e+00, -2.9034e-01,\n",
      "         -9.8687e-01, -1.1533e+00,  1.1101e+00,  1.4474e-01,  4.5651e-01,\n",
      "          1.0404e+00, -8.0491e-01, -1.4995e+00, -2.8694e-01, -1.4160e+00,\n",
      "          1.0657e+00, -1.3080e+00,  1.6061e+00,  7.3484e-01, -5.9020e-01,\n",
      "          1.1070e+00, -1.7763e+00, -7.2777e-01,  1.4331e+00,  9.5111e-01,\n",
      "         -1.2630e+00, -4.2146e-01,  5.7142e-01, -6.2862e-01,  1.6844e+00,\n",
      "         -4.0412e-01, -1.6352e+00, -1.0059e+00,  6.9011e-04, -1.9917e+00,\n",
      "         -1.4201e+00, -1.0875e+00, -2.0102e+00, -1.1761e+00,  2.2165e-01,\n",
      "         -1.2756e+00,  1.9618e+00, -1.6436e+00,  9.9192e-01, -2.2536e+00,\n",
      "         -3.3507e-01, -1.3871e+00, -1.5678e+00, -1.6249e+00,  1.2603e+00,\n",
      "         -1.0253e+00,  3.1168e-01,  1.2904e+00, -1.4584e+00, -1.4469e-01]])\n"
     ]
    }
   ],
   "source": [
    "W_OUT = 100\n",
    "\n",
    "# (6,100)\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN)\n",
    "# (1,100)\n",
    "    # 1 bias per neuron (per column)\n",
    "bias = torch.randn(W_OUT, generator=GEN)\n",
    "print(\"Bias:\", bias, bias.shape)\n",
    "\n",
    "\n",
    "# bias = torch.randn()\n",
    "print(\"Weights:\", W)\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "# h for hidden layer\n",
    "h = emb_view @ W\n",
    "\n",
    "# why this works: bias is (100,)\n",
    "# broadcast: \n",
    "    # h: (32, 100)\n",
    "    # b: (  , 100) -> (1,100), 1 is inferred\n",
    "    # so it copies bias and adds to each row as expected - each column gets a different bias value, for each neuron\n",
    "h = h + bias\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [6.8365e-14, 4.1561e-10, 2.0476e-05, 1.9685e-01, 3.6644e-09, 2.5611e-04,\n",
       "         2.3277e-09, 4.3530e-05, 4.4425e-05, 1.3900e-09, 2.9489e-05, 2.1639e-03,\n",
       "         8.2406e-09, 9.1830e-03, 5.0304e-05, 1.9712e-07, 1.0256e-03, 6.0565e-08,\n",
       "         1.5621e-02, 1.3701e-06, 3.1891e-06, 1.0589e-05, 4.8030e-07, 2.0710e-04,\n",
       "         2.5028e-02, 6.3929e-02, 6.8553e-01],\n",
       "        [1.3785e-10, 7.1116e-05, 1.9685e-11, 3.7020e-08, 3.8112e-07, 6.1939e-05,\n",
       "         1.0589e-03, 2.4612e-02, 5.9960e-05, 8.2954e-06, 2.8608e-03, 1.1369e-06,\n",
       "         2.0626e-08, 1.2665e-06, 1.3825e-03, 6.0687e-05, 5.0792e-06, 2.9640e-02,\n",
       "         2.6060e-12, 1.0480e-08, 8.6222e-06, 5.8362e-08, 3.8414e-02, 6.9616e-01,\n",
       "         1.3612e-06, 2.0560e-01, 4.1925e-08],\n",
       "        [1.1296e-08, 9.9169e-01, 8.2763e-05, 4.7307e-08, 5.2716e-07, 1.1432e-07,\n",
       "         4.5556e-08, 1.5861e-03, 2.5982e-06, 3.0303e-08, 4.2567e-05, 4.3100e-10,\n",
       "         4.2078e-09, 4.7673e-07, 1.3123e-06, 3.5119e-03, 1.8188e-09, 9.2137e-10,\n",
       "         1.0143e-03, 5.2248e-10, 1.5106e-04, 5.9120e-09, 2.2771e-06, 1.9160e-03,\n",
       "         6.2971e-09, 5.7147e-09, 1.9878e-06],\n",
       "        [2.4788e-08, 4.1117e-05, 1.1683e-08, 8.8031e-05, 3.0312e-08, 1.8094e-03,\n",
       "         1.0015e-07, 1.4834e-08, 9.9315e-11, 1.4141e-08, 5.7513e-06, 4.2593e-04,\n",
       "         1.1685e-07, 2.8681e-01, 9.1562e-07, 3.4057e-02, 3.5671e-09, 1.1302e-13,\n",
       "         6.4183e-06, 6.1721e-08, 7.6164e-03, 7.1072e-07, 7.6732e-09, 6.6902e-01,\n",
       "         1.1124e-04, 6.7153e-12, 1.0122e-05],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [1.3228e-10, 5.2603e-01, 6.8273e-09, 7.1021e-09, 1.1829e-07, 8.2693e-05,\n",
       "         7.0980e-05, 1.4303e-01, 4.5650e-06, 1.5805e-07, 7.8844e-03, 1.0340e-06,\n",
       "         8.8627e-08, 4.5004e-07, 4.7476e-03, 6.4546e-07, 3.2316e-07, 8.9055e-03,\n",
       "         1.6497e-09, 3.4401e-08, 1.4834e-07, 2.0662e-07, 2.8947e-03, 1.7122e-01,\n",
       "         8.3633e-07, 1.3513e-01, 6.4751e-07],\n",
       "        [1.2745e-10, 1.0000e+00, 1.6020e-13, 1.8645e-10, 5.1141e-12, 4.2361e-10,\n",
       "         1.4395e-13, 2.1033e-07, 6.0098e-10, 1.2448e-12, 7.6635e-11, 1.1237e-09,\n",
       "         1.3887e-10, 3.6748e-09, 2.8929e-10, 9.9027e-09, 4.6133e-13, 1.2200e-15,\n",
       "         2.7734e-08, 2.8084e-12, 1.5415e-08, 3.5499e-10, 2.3313e-09, 4.0335e-07,\n",
       "         4.8197e-09, 3.6785e-10, 4.0072e-07],\n",
       "        [1.5062e-10, 7.9447e-01, 6.8998e-12, 3.8265e-08, 6.0892e-12, 1.4840e-01,\n",
       "         1.6850e-08, 2.2217e-09, 7.1332e-12, 1.0418e-03, 6.1412e-04, 3.0613e-04,\n",
       "         2.4696e-10, 2.5273e-03, 7.4339e-04, 7.8838e-04, 2.7189e-09, 2.0782e-17,\n",
       "         2.9269e-08, 3.4234e-11, 5.7498e-07, 8.5817e-07, 1.0154e-07, 5.1110e-02,\n",
       "         3.4165e-10, 1.5504e-12, 9.0301e-09],\n",
       "        [9.9902e-10, 1.1785e-09, 3.0751e-11, 3.6634e-06, 2.3274e-05, 2.2613e-12,\n",
       "         8.7994e-07, 6.4068e-09, 9.5297e-13, 3.0796e-10, 1.9769e-11, 3.4849e-10,\n",
       "         4.9741e-11, 5.3920e-11, 2.3951e-08, 2.5569e-09, 5.8760e-14, 5.1701e-12,\n",
       "         6.6037e-09, 5.7791e-12, 9.9997e-01, 9.0664e-11, 2.0022e-11, 1.0661e-06,\n",
       "         1.4851e-11, 1.8723e-14, 5.8557e-14],\n",
       "        [1.1216e-13, 1.3773e-04, 2.3751e-10, 4.4798e-06, 1.0092e-08, 4.3905e-05,\n",
       "         9.5929e-10, 2.1847e-09, 9.9114e-11, 1.4299e-09, 9.3408e-08, 1.6257e-09,\n",
       "         5.2884e-12, 6.7871e-09, 2.2369e-06, 3.5295e-08, 2.5777e-08, 7.6389e-13,\n",
       "         9.9981e-01, 1.1991e-09, 6.5274e-09, 2.5702e-10, 3.7675e-09, 1.0881e-07,\n",
       "         1.2617e-08, 4.5884e-13, 8.0979e-09],\n",
       "        [1.3089e-11, 7.7038e-09, 5.3127e-08, 4.4668e-05, 5.2548e-07, 7.5878e-04,\n",
       "         1.7066e-07, 1.0781e-07, 7.4577e-06, 9.0856e-09, 7.0690e-08, 1.3405e-01,\n",
       "         1.4165e-05, 6.1325e-04, 3.4102e-01, 2.6888e-07, 2.6087e-07, 2.2872e-07,\n",
       "         8.1178e-08, 7.1988e-07, 5.2155e-01, 1.1549e-09, 1.6845e-04, 6.5212e-05,\n",
       "         5.1928e-05, 6.6453e-08, 1.6523e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [3.0727e-11, 7.0382e-08, 9.0770e-05, 2.8462e-01, 4.4219e-03, 5.0848e-03,\n",
       "         2.2958e-04, 1.3853e-02, 5.1958e-02, 5.3968e-07, 1.6313e-04, 6.4986e-02,\n",
       "         6.8246e-08, 3.2213e-03, 3.9932e-02, 3.9735e-05, 2.0214e-04, 7.5298e-05,\n",
       "         2.0479e-02, 1.0849e-05, 1.3969e-01, 4.4869e-05, 3.1109e-05, 1.6134e-01,\n",
       "         1.0194e-01, 4.7200e-03, 1.0287e-01],\n",
       "        [7.1443e-10, 8.3208e-06, 1.3397e-08, 1.1205e-06, 7.6466e-01, 1.5721e-07,\n",
       "         1.3244e-01, 1.1216e-03, 1.5530e-04, 9.2832e-08, 4.5542e-10, 3.3853e-07,\n",
       "         3.4060e-08, 5.6231e-07, 1.9254e-04, 2.3368e-07, 2.6900e-08, 2.8312e-04,\n",
       "         3.6823e-07, 1.0882e-07, 8.5466e-04, 3.7873e-08, 2.7222e-08, 1.0012e-01,\n",
       "         1.5806e-04, 1.8494e-08, 1.4281e-06],\n",
       "        [6.5291e-14, 1.5102e-07, 1.1729e-04, 5.7639e-02, 1.9839e-04, 3.0956e-04,\n",
       "         7.3039e-06, 6.2358e-05, 3.8526e-06, 3.4572e-09, 7.1664e-06, 7.9779e-06,\n",
       "         8.5429e-10, 5.1024e-06, 1.1610e-04, 1.6946e-04, 2.3829e-04, 1.8623e-07,\n",
       "         8.3640e-01, 2.6639e-08, 7.1354e-06, 5.8141e-07, 2.0660e-09, 9.9714e-02,\n",
       "         2.7150e-03, 1.2070e-06, 2.2771e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [5.2470e-11, 7.6938e-03, 1.0777e-05, 5.0911e-03, 1.0572e-06, 3.5413e-03,\n",
       "         6.2156e-06, 1.4944e-01, 1.4113e-01, 1.6016e-07, 2.7195e-02, 4.4333e-02,\n",
       "         2.4544e-03, 8.7723e-03, 4.1838e-03, 1.5251e-05, 1.1605e-03, 5.5702e-05,\n",
       "         6.4435e-04, 7.8044e-04, 9.9097e-06, 7.1613e-03, 1.7232e-01, 7.4297e-03,\n",
       "         1.7301e-01, 2.0271e-01, 4.0855e-02],\n",
       "        [4.2015e-10, 1.6522e-02, 1.3572e-10, 1.3594e-07, 6.9696e-03, 1.1380e-06,\n",
       "         4.4457e-07, 6.5977e-04, 1.6107e-03, 2.7494e-08, 3.9008e-07, 1.2763e-05,\n",
       "         1.0397e-07, 7.4413e-08, 1.6280e-02, 1.5473e-02, 1.3636e-07, 1.3538e-06,\n",
       "         1.4636e-08, 2.6405e-07, 2.7999e-01, 2.0693e-08, 1.5857e-03, 6.6085e-01,\n",
       "         2.5812e-06, 3.1863e-06, 4.2912e-05],\n",
       "        [1.9791e-11, 2.0595e-06, 9.9719e-03, 9.5977e-04, 2.9245e-04, 1.4362e-04,\n",
       "         5.3716e-08, 2.1452e-06, 7.4617e-09, 7.7082e-08, 1.6292e-04, 7.7487e-07,\n",
       "         1.6527e-10, 6.1820e-04, 7.4312e-06, 5.2019e-04, 3.0618e-06, 1.2894e-13,\n",
       "         8.4620e-01, 1.9118e-07, 1.0804e-03, 2.0602e-08, 1.7234e-09, 1.3877e-01,\n",
       "         1.2669e-03, 4.8764e-13, 2.6270e-08],\n",
       "        [3.8494e-13, 7.3627e-01, 1.5682e-08, 6.9540e-09, 1.6663e-13, 4.1843e-04,\n",
       "         2.2122e-04, 3.6886e-07, 4.4483e-06, 3.9973e-03, 1.1547e-05, 1.0779e-02,\n",
       "         2.1516e-04, 2.0246e-04, 4.0275e-04, 4.2318e-07, 1.5429e-07, 3.6351e-07,\n",
       "         1.0134e-12, 2.1719e-07, 4.3730e-10, 1.8890e-05, 9.8278e-02, 4.5051e-05,\n",
       "         2.2433e-11, 1.4914e-01, 4.5380e-07],\n",
       "        [3.4293e-06, 2.5420e-09, 7.8812e-06, 1.4604e-03, 5.9348e-10, 1.0571e-05,\n",
       "         4.2870e-12, 1.2451e-07, 1.5078e-08, 5.8704e-12, 3.9242e-09, 2.5691e-07,\n",
       "         5.6767e-10, 4.0119e-05, 3.8598e-05, 7.5505e-05, 8.8371e-11, 1.2825e-14,\n",
       "         2.6853e-05, 5.5664e-10, 2.1989e-02, 6.9340e-09, 2.2668e-06, 2.2171e-07,\n",
       "         1.0763e-04, 1.4859e-05, 9.7622e-01],\n",
       "        [9.5699e-08, 2.0733e-04, 3.2152e-09, 2.9141e-05, 3.3797e-10, 2.8711e-04,\n",
       "         8.9165e-01, 6.9198e-11, 4.6177e-12, 1.0739e-01, 8.8537e-05, 8.1354e-05,\n",
       "         4.6162e-08, 1.0546e-06, 7.2800e-07, 3.1520e-09, 1.0961e-10, 2.2440e-09,\n",
       "         2.5469e-12, 1.2000e-09, 3.3605e-07, 7.7052e-08, 3.1465e-06, 2.6781e-04,\n",
       "         3.4917e-11, 4.7030e-12, 2.6047e-12],\n",
       "        [2.8478e-08, 6.6653e-01, 1.0830e-05, 1.4612e-08, 1.2338e-08, 5.5271e-07,\n",
       "         3.3243e-08, 2.8572e-01, 8.4457e-04, 3.5494e-08, 1.8938e-04, 6.2184e-10,\n",
       "         8.0545e-09, 3.9368e-08, 1.7321e-04, 3.9506e-02, 6.0118e-07, 1.4032e-10,\n",
       "         1.8904e-04, 6.0291e-09, 8.0527e-05, 2.7112e-06, 9.3329e-05, 6.6551e-03,\n",
       "         4.2669e-08, 7.3339e-08, 7.3088e-07],\n",
       "        [7.5847e-06, 1.7647e-05, 3.9970e-07, 2.8866e-05, 1.8172e-08, 4.0296e-03,\n",
       "         5.9501e-07, 3.4804e-09, 1.2925e-11, 2.8633e-07, 2.7979e-04, 5.0431e-06,\n",
       "         7.6742e-09, 5.2283e-02, 2.0955e-04, 8.9330e-02, 1.8862e-09, 5.8623e-14,\n",
       "         2.1015e-06, 8.6655e-09, 2.9872e-01, 7.3246e-08, 1.0754e-07, 5.5508e-01,\n",
       "         6.8259e-07, 3.2069e-11, 8.3104e-09],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [8.1452e-10, 1.8016e-04, 6.5544e-08, 3.9486e-07, 1.7942e-03, 1.1060e-06,\n",
       "         2.1602e-04, 3.6675e-04, 6.3153e-06, 4.7813e-09, 2.2747e-06, 1.1809e-07,\n",
       "         4.0151e-08, 6.1695e-07, 7.7422e-05, 2.8504e-07, 1.5279e-07, 1.3777e-05,\n",
       "         1.0753e-07, 1.9894e-08, 5.9891e-06, 1.0691e-07, 8.8311e-07, 9.9588e-01,\n",
       "         1.4456e-03, 1.2169e-07, 1.1007e-05],\n",
       "        [7.7901e-12, 9.9992e-01, 6.1512e-12, 2.5135e-12, 1.2922e-09, 9.6660e-09,\n",
       "         1.6270e-10, 2.6213e-05, 5.4496e-08, 1.1987e-11, 2.1822e-06, 2.0637e-08,\n",
       "         1.6035e-09, 5.7746e-11, 1.4566e-08, 4.4576e-08, 2.2200e-10, 1.5848e-08,\n",
       "         1.6301e-09, 1.3031e-10, 1.2187e-08, 2.6812e-10, 5.4765e-09, 1.4868e-05,\n",
       "         5.2247e-10, 3.7397e-05, 1.2600e-06],\n",
       "        [5.7278e-12, 2.3306e-14, 7.1772e-07, 7.7967e-08, 3.1102e-05, 1.2452e-07,\n",
       "         6.8755e-11, 4.1059e-11, 1.9512e-09, 2.4815e-14, 1.4978e-14, 4.9167e-08,\n",
       "         1.4188e-12, 5.8159e-06, 1.3131e-04, 4.6438e-07, 7.4308e-13, 1.4926e-12,\n",
       "         3.6095e-06, 2.8189e-13, 9.9983e-01, 1.9827e-13, 8.4867e-13, 1.6814e-06,\n",
       "         3.0432e-10, 2.2336e-14, 3.1839e-08],\n",
       "        [2.0404e-10, 2.5702e-08, 1.2095e-08, 5.2292e-04, 3.6576e-11, 8.6851e-06,\n",
       "         9.9664e-01, 1.1695e-10, 3.9258e-14, 1.3928e-07, 4.2754e-07, 1.4162e-04,\n",
       "         6.5149e-10, 2.6736e-03, 7.3567e-09, 2.9535e-12, 1.2528e-07, 7.2461e-06,\n",
       "         3.3744e-11, 1.2678e-11, 1.5350e-08, 6.9142e-12, 5.0439e-12, 1.1578e-06,\n",
       "         2.3050e-11, 2.3835e-08, 6.5479e-14],\n",
       "        [8.7365e-15, 5.7650e-10, 5.0590e-03, 2.5860e-06, 9.1061e-07, 2.7300e-08,\n",
       "         6.1033e-05, 1.9516e-02, 3.6828e-02, 1.0858e-08, 3.8272e-09, 3.0933e-06,\n",
       "         1.9118e-12, 2.9697e-09, 8.6193e-06, 2.6454e-07, 2.5217e-03, 1.0188e-02,\n",
       "         7.1391e-08, 1.6662e-06, 2.4008e-08, 7.3626e-10, 1.2556e-04, 1.7289e-06,\n",
       "         7.3063e-01, 1.9229e-01, 2.7674e-03],\n",
       "        [1.1174e-09, 1.1508e-09, 3.5377e-05, 6.8658e-05, 1.1534e-04, 3.6285e-09,\n",
       "         4.2690e-08, 9.5894e-01, 1.8759e-03, 6.1524e-11, 4.6356e-08, 2.2982e-11,\n",
       "         5.4032e-12, 1.4064e-05, 1.7364e-02, 7.6190e-06, 5.5600e-07, 2.6349e-10,\n",
       "         5.0398e-04, 3.2318e-08, 2.0003e-02, 7.7636e-10, 1.1831e-07, 2.9509e-04,\n",
       "         7.5410e-04, 1.9720e-06, 2.1938e-05]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "res = h @ W2 + b2\n",
    "probs = res.softmax(dim=1)\n",
    "\n",
    "# ach row is normalized\n",
    "print(probs.sum(dim=1))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding probabilities for each y label, get NLL Loss\n",
    "torch.arange(m) -> 0,1,2,3...m-1 for row indices\n",
    "Y: e.g [0,5,13,13,6..] -> actual labels. so we end up getting\n",
    "- res[0,0], res[1,5], res[2,13] etc in a vector\n",
    "- This is the predicted probability based on current weights for that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8111e-04, 9.1830e-03, 1.2665e-06, 9.9169e-01, 2.4788e-08, 6.1128e-06,\n",
       "        8.8627e-08, 1.2448e-12, 1.0154e-07, 3.0796e-10, 1.3773e-04, 1.3089e-11,\n",
       "        7.5123e-07, 3.1109e-05, 8.3208e-06, 6.5291e-14, 1.6017e-06, 7.8044e-04,\n",
       "        1.6522e-02, 9.9719e-03, 4.1843e-04, 5.6767e-10, 4.6162e-08, 6.6653e-01,\n",
       "        7.5847e-06, 4.1817e-07, 2.8504e-07, 2.2200e-10, 1.9512e-09, 1.3928e-07,\n",
       "        5.7650e-10, 1.1174e-09])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(Y)\n",
    "probs[torch.arange(m), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5532)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = -probs.log().mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(2147483647)\n",
    "# Number of examples\n",
    "M = len(X) \n",
    "\n",
    "# Params: C, W, bias, W2, b2\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "\n",
    "# Forward pass\n",
    "W_OUT = 100\n",
    "\n",
    "# First layer\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN) # (6,100)\n",
    "bias = torch.randn(W_OUT, generator=GEN) # (1,100): 1 bias per neuron (per column)\n",
    "\n",
    "# Second layer: W_OUT inputs, 27 outputs for log-counts (to become probabilities)\n",
    "W2 = torch.randn((W_OUT, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "#----#\n",
    "emb = C[X] # lookup embeddings corresponding to each row -> (M, BLOCK_SIZE, 2) i.e one 2D vector per char in a row of X\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "    # (number of examples, number of elems per example)\n",
    "emb_view = emb.view((M, BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = torch.tanh(h)\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities, then doing NLL and eman\n",
    "nll_loss = F.cross_entropy(probs, Y)\n",
    "\n",
    "# probs = probs.softmax(dim=1) # convert to probabilities\n",
    "# # get corresponding predicted probabilities for Y labels\n",
    "# probs = probs[torch.arange(M), Y]\n",
    "# nll_loss = -probs.log().mean()\n",
    "\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQDGcWn074pA7NdAWB8vTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env 28Sep)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
