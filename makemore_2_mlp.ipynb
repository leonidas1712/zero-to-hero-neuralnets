{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNe_RPhx3Cx"
   },
   "source": [
    "## Makemore Part 2: MLP\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Idea from paper: C is a lookup table (matrix) for embeddings vector of each of the words in V e.g |V|=17k\n",
    "- C: (17000x30)\n",
    "- one-hot encoding of a word: (1x17000)\n",
    "- enc @ C -> (1x30) embedding vector\n",
    "\n",
    "We can get these vectors for multiple input words (words that came before).\n",
    "\n",
    "Pass them to neurons with n_in=30, those neurons are fully connected to a hidden layer, then goes through tanh -> softmax for probabilities.\n",
    "\n",
    "Final output: 17k vector of probabilities for the 17k words in vocab.\n",
    "- Train by using actual next word's index, get the predicted prob, do -log(prob) and backprop etc.\n",
    "\n",
    "---\n",
    "\n",
    "We can use this idea for a character-level model as well.\n",
    "\n",
    "## Architecture\n",
    "- Start with indexes from the chars/words in context\n",
    "- Convert them to respective embeddings vectors in C (lookup)\n",
    "- Stack the vectors' values together, do forward pass with weights matrix and add bias\n",
    "- Activate through tanh\n",
    "- Pass through one more layer to get 27 outputs (one for each unique char in our vocab)\n",
    "- Softmax to get probabilities\n",
    "\n",
    "## Building dataset\n",
    "Hyperparameter: BLOCK_SIZE = 3\n",
    "- block_size is the number of previous chars we consider when predicting next\n",
    "\n",
    "For each word, we add to X with block_size=3 char windows, Y has the char that comes after\n",
    "- e.g '.emma': (..., e), (..e, m), (.em, m), (emm, a), (mma, .)\n",
    "- So each word contributes n+1 examples as before, n = len(word)\n",
    "\n",
    "## Lookup table\n",
    "Lookup table: C = (27,2) random init\n",
    "- In paper, they compress 17k words of vocab into Rn of n=30\n",
    "- So we do similar here for 27 unique chars -> embeddings of size 2\n",
    "\n",
    "Previously, we used one-hot encoding to lookup with enc @ W\n",
    "- But this is just the same as doing W[idx] due to all the zeroes\n",
    "\n",
    "In PyTorch, we can just do C[X] and it will work\n",
    "- produces (32,3,2) - one 2D vector for each encoded char\n",
    "- Or another way to think about it, one (3x2) vector for each row in X. 3 because BLOCK_SIZE=3, so each row in X has 3 elements. For each of those chars, we want one 2D vector - its embedding\n",
    "\n",
    "## F.cross_entropy\n",
    "F.cross_entropy(logits, targets) is the same as doing:\n",
    "\n",
    "```python\n",
    "logits = logits.softmax(dim=1)\n",
    "logits = logits[torch.arange(M), Y]\n",
    "nll_loss = -logits.prob().mean()\n",
    "```\n",
    "i.e same as softmax -> select corresponding probabilities for targets in Y -> get NLL Loss (mean)\n",
    "\n",
    "F.cross_entropy is **better** because PyTorch can optimise and not create new memory, and it can use **fused kernels** to cluster ops together and run them at the same time.\n",
    "- Also more numerically stable. logits can be subtracted or added by any number. when logits are too high, e^(high number) becomes inf, then we get nans. But PyTorch can internally subtract the data by the max number to prevent this\n",
    "\n",
    "## Batching\n",
    "Instead of fwd + backward on whole dataset which is slow, we can pick a random batch each time and fwd + backward on that.\n",
    "Just select BATCH_SIZE of random indices within [0,M), then use X[batch_indices] and Y[batch_indices]\n",
    "\n",
    "## Finding good learning rate\n",
    "1. Find min and max bounds by trial and error\n",
    "- Set very low LR, low is if it barely changes\n",
    "- High is if loss grows\n",
    "2. Create torch.linspace between low and high using exponents for the lrs e.g -3, 0 for 10^-3 -> 10^0=1\n",
    "3. During loop, ith iteration uses ith LR in list for the batch. Track the loss for that LR\n",
    "4. Graph it and find the valley in the graph - the middle should be a good LR.\n",
    "\n",
    "## Train, test, validation split (80,10,10)\n",
    "Train split: Used to optimise model parameters with gradient descent\n",
    "\n",
    "Validation split: Used to tune hyperparameters: e.g outputs in hidden layers, size of embeddings in C\n",
    "  \n",
    "Test split: Used to evaluate the performance of the model at the **end**\n",
    "- Why: achieving low loss on the same set we used to train isn't necessarily a good thing. Because when we use bigger models, they are able to just memorise the dataset, then doesn't generalise to new inputs well.\n",
    "\n",
    "Training: we tune hyperparameters by checking repeatedly on dev split, then at the end check loss against test split just one time and this is the final number we report.\n",
    "\n",
    "## Overfitting, underfitting\n",
    "Underfitting: when train loss and dev or test loss are both high and close to each other\n",
    "- Means the model isn't big enough / powerful enough to fit the data\n",
    "\n",
    "Overfitting: when train loss is low, but dev/test loss are much higher\n",
    "- Means the model is memorising the training data but doesn't generalise well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "executionInfo": {
     "elapsed": 10072,
     "status": "ok",
     "timestamp": 1727498203827,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "E1zlN5r9xtTz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl1lLlcpJtBM"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1727488190995,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "INrlJJdu70z3",
    "outputId": "4fa9ff58-4349-46f0-fbb5-5b7f861df269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 32033\n",
      "['yuheng', 'diondre', 'xavien', 'jori', 'juanluis']\n",
      "25626 28829\n"
     ]
    }
   ],
   "source": [
    "# Open up names\n",
    "names = []\n",
    "with open('names.txt', 'r') as names_file:\n",
    "  names = names_file.read().splitlines()\n",
    "\n",
    "print(\"Total length:\", len(names))\n",
    "\n",
    "# build lookups\n",
    "uniq = ['.'] + sorted(list(set(''.join(names))))\n",
    "stoi = { char: idx for idx, char in enumerate(uniq)}\n",
    "itos = { idx: char for char,idx in stoi.items() }\n",
    "\n",
    "# Hyperparameters\n",
    "BLOCK_SIZE = 4\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "def build_dataset(names):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in names:\n",
    "      # print(word)\n",
    "      word = word + '.'\n",
    "      block = [0] * BLOCK_SIZE # ... (empty context at start)\n",
    "    \n",
    "      for char in word:\n",
    "        char_idx = stoi[char]\n",
    "        X.append(block)\n",
    "        Y.append(char_idx)\n",
    "    \n",
    "        block_str = ''.join(list(map(lambda i: itos[i], block)))\n",
    "        # print(f'{block_str} -> {char}')\n",
    "    \n",
    "        # update block: roll over sliding window\n",
    "        block = block[1:] + [char_idx]\n",
    "\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# train, test, dev split: 80,10,10\n",
    "from random import Random\n",
    "Random(42).shuffle(names)\n",
    "\n",
    "print(names[0:5])\n",
    "\n",
    "n_80 = int(0.8*len(names))\n",
    "n_90 = int(0.9*len(names))\n",
    "print(n_80, n_90)\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n_80]) # 80%\n",
    "Xdev, Ydev = build_dataset(names[n_80:n_90]) # 10%\n",
    "Xtest, Ytest = build_dataset(names[n_90:]) # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228146\n",
      "0.8004742577121667\n",
      "0.09930044795876325\n",
      "0.10022529432906999\n",
      "182625 22655 22866\n"
     ]
    }
   ],
   "source": [
    "total = len(Xtr) + len(Xdev) + len(Xtest)\n",
    "print(total)\n",
    "print(len(Xtr) / total)\n",
    "print(len(Xdev) / total)\n",
    "print(len(Xtest) / total)\n",
    "print(len(Xtr) , len(Xdev), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1727489615781,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "vUo1EJwqJzs0",
    "outputId": "b6926d3a-389f-4be1-9eae-bf58b6f6b454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 4]) torch.Size([182625]) torch.int64 torch.int64\n",
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 0,  0,  0, 25],\n",
      "        [ 0,  0, 25, 21],\n",
      "        ...,\n",
      "        [ 8, 15, 12,  4],\n",
      "        [15, 12,  4,  1],\n",
      "        [12,  4,  1, 14]])\n",
      "---\n",
      "tensor([25, 21,  8,  ...,  1, 14,  0])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Ytr.shape, Xtr.dtype, Ytr.dtype)\n",
    "print(Xtr)\n",
    "print(\"---\")\n",
    "print(Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_c8JD9Z8Bd"
   },
   "source": [
    "## Lookup table C, embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1727490140478,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "J7DEHynvMtq7",
    "outputId": "f79ae109-2ddb-40ef-895e-6553c96d2fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5013, -0.8621,  0.1141, -0.4797,  1.0472,  1.5424, -1.2521, -1.6304,\n",
      "          0.9521, -0.3236],\n",
      "        [-0.5833, -0.0969, -0.5720,  1.8011, -0.0412, -0.1098, -0.3862, -0.5910,\n",
      "          0.3759,  0.5625],\n",
      "        [ 1.4190, -0.6763,  1.7523,  0.7620,  1.0095,  0.6250,  1.4192,  0.3493,\n",
      "         -0.8872,  0.1808],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -1.5889,  0.9987,  0.3235,  0.5826,\n",
      "         -1.2369, -0.5786],\n",
      "        [-0.9113,  1.4787,  0.3230, -1.6748, -0.3587,  0.8224, -0.5762, -0.1008,\n",
      "          0.6782, -0.5032],\n",
      "        [ 0.0545, -0.4910, -1.7107, -0.1619,  1.3593, -0.4439,  1.0834,  0.0320,\n",
      "         -0.5812,  0.0108],\n",
      "        [-0.0482, -0.1170, -0.0892,  0.2336, -0.4153,  0.0076,  0.7098,  0.6353,\n",
      "          0.2616, -1.0070],\n",
      "        [-0.7181,  0.8360, -0.9140, -0.3356,  0.6749, -0.6606,  0.0102, -0.3768,\n",
      "         -0.3413,  1.1586],\n",
      "        [-0.5271, -1.4327,  0.5918,  1.5556, -0.8974,  0.6933,  1.5235, -0.0697,\n",
      "          1.2950, -1.4680],\n",
      "        [ 0.0252,  0.8468,  0.1797, -1.2906,  0.7836,  0.3613,  0.0547,  1.3996,\n",
      "         -0.2099,  1.9860],\n",
      "        [ 0.1808,  0.3776, -0.8380,  1.0456, -0.9487,  0.4099,  0.3002,  0.3258,\n",
      "         -1.2162, -0.9100],\n",
      "        [-1.2349, -0.6287,  0.2815,  1.0406,  0.0178,  0.1483,  1.0566,  0.2054,\n",
      "         -0.3947, -0.1226],\n",
      "        [-0.2265,  0.1289, -0.4223,  0.8507,  0.0965, -1.7318,  0.3284, -1.7837,\n",
      "         -1.1700, -0.1490],\n",
      "        [-0.0549,  0.3628, -0.5278, -0.4945, -0.2542, -0.1471, -0.8297, -0.7266,\n",
      "         -0.4838, -1.7872],\n",
      "        [ 0.3676, -0.8236,  2.4072, -1.0029,  0.8090,  1.9090, -1.2104, -0.3956,\n",
      "          0.8433,  0.3986],\n",
      "        [-0.4483,  0.1495, -1.5291,  1.0313,  1.2232,  0.3802, -0.0795, -0.6631,\n",
      "          0.0809,  2.3321],\n",
      "        [ 0.8383,  0.8010, -0.4180,  0.3827, -0.6356, -1.2418, -2.5622,  0.4679,\n",
      "          0.3027, -1.5873],\n",
      "        [ 1.3803, -0.9970,  0.1950, -0.7574,  2.2395, -1.2319, -1.8203, -1.7171,\n",
      "          0.5226,  1.5027],\n",
      "        [ 2.1024,  0.3466, -0.6218, -0.9768,  0.5534, -1.1111, -0.8860,  0.7296,\n",
      "         -1.8385,  0.6464],\n",
      "        [-0.2872,  0.8906, -0.3067,  0.7088, -1.8585,  0.4963, -1.0555, -0.3514,\n",
      "         -1.6551, -0.5993],\n",
      "        [ 0.1360,  0.1325, -0.0439,  0.5588,  1.4310, -0.6275, -0.5256, -0.3855,\n",
      "         -1.4038, -0.1902],\n",
      "        [-0.9724, -2.4353, -0.8207,  0.7638, -0.6430, -1.6277,  0.6287,  0.2157,\n",
      "         -0.7200,  1.8399],\n",
      "        [ 0.9315,  0.6752, -0.4389, -0.6038,  0.4971,  1.3256,  0.2866,  0.2505,\n",
      "         -0.4102, -0.3827],\n",
      "        [-0.0496, -0.3635, -0.0551, -0.6078, -0.0129,  1.4252, -0.0224,  0.8542,\n",
      "         -1.8022,  0.0875],\n",
      "        [-1.1542,  0.0965,  0.5871, -2.1649, -0.5822,  0.8462,  1.0826, -0.5823,\n",
      "         -0.5806, -0.6051],\n",
      "        [ 0.7079, -1.1227, -1.5542, -0.9094, -0.9417, -0.3635, -1.0071, -0.3720,\n",
      "         -0.4000,  0.2393],\n",
      "        [ 0.8201,  0.7582,  0.3197,  0.6152, -0.1345,  0.7046,  0.1137,  0.5721,\n",
      "         -0.8211, -0.3797]]) \n",
      "------\n",
      "\n",
      "Embeddings:\n",
      "torch.Size([182625, 3, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393],\n",
       "         [-0.9724, -2.4353, -0.8207,  ...,  0.2157, -0.7200,  1.8399]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4483,  0.1495, -1.5291,  ..., -0.6631,  0.0809,  2.3321],\n",
       "         [-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032]],\n",
       "\n",
       "        [[-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625]],\n",
       "\n",
       "        [[-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625],\n",
       "         [ 0.3676, -0.8236,  2.4072,  ..., -0.3956,  0.8433,  0.3986]]])"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(1097)\n",
    "\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "print(C, \"\\n------\\n\")\n",
    "\n",
    "# Embedding\n",
    "    # (32,3,2)\n",
    "emb = C[Xtr]\n",
    "\n",
    "# For each size 3 block in X (each row), we get a (3x2) matrix which has all the 2D embeddings for the chars that make up the block\n",
    "  # e.g X[2] = [0,5,13], so emb[2] is [C[0], C[5], C[13]] stacked vertically\n",
    "# print(emb[2])\n",
    "# example = torch.cat((C[0], C[5], C[13]), dim=0).view((BLOCK_SIZE ,EMBEDDING_SIZE))\n",
    "# same as emb[2]\n",
    "# print(example)\n",
    "# print(\"\\n------\\n\")\n",
    "\n",
    "\n",
    "print(\"Embeddings:\")\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck__BTsebPl5"
   },
   "source": [
    "## Different ways to flatten\n",
    "(32,3,2) -> (32,6) so we can keep W to (6, 100)\n",
    "- 3x2=6 determined by block size and embedding size\n",
    "\n",
    "torch.cat, torch.unbind creates new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1727490193315,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "5TGsS6o-Q2HD",
    "outputId": "48929d69-be45-45d1-8069-373cc42e1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor([[ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621, -0.5833, -0.0969],\n",
      "        [ 0.5013, -0.8621, -0.5833, -0.0969,  1.4192,  0.3493],\n",
      "        [-0.5833, -0.0969,  1.4192,  0.3493,  1.4192,  0.3493],\n",
      "        [ 1.4192,  0.3493,  1.4192,  0.3493,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.9260,  0.9209],\n",
      "        [ 0.5013, -0.8621,  1.9260,  0.9209,  1.0095,  0.6250],\n",
      "        [ 1.9260,  0.9209,  1.0095,  0.6250,  0.3759,  0.5625],\n",
      "        [ 1.0095,  0.6250,  0.3759,  0.5625,  1.1586, -1.2548],\n",
      "        [ 0.3759,  0.5625,  1.1586, -1.2548,  0.3759,  0.5625],\n",
      "        [ 1.1586, -1.2548,  0.3759,  0.5625,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.1141, -0.4797,  1.1586, -1.2548],\n",
      "        [ 0.1141, -0.4797,  1.1586, -1.2548,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.3759,  0.5625],\n",
      "        [ 0.5013, -0.8621,  0.3759,  0.5625,  1.3593, -0.4439],\n",
      "        [ 0.3759,  0.5625,  1.3593, -0.4439,  0.1141, -0.4797],\n",
      "        [ 1.3593, -0.4439,  0.1141, -0.4797,  1.0472,  1.5424],\n",
      "        [ 0.1141, -0.4797,  1.0472,  1.5424, -0.5833, -0.0969],\n",
      "        [ 1.0472,  1.5424, -0.5833, -0.0969,  1.0095,  0.6250],\n",
      "        [-0.5833, -0.0969,  1.0095,  0.6250,  1.0095,  0.6250],\n",
      "        [ 1.0095,  0.6250,  1.0095,  0.6250,  0.1141, -0.4797],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  0.5013, -0.8621],\n",
      "        [ 0.5013, -0.8621,  0.5013, -0.8621,  1.3593, -0.4439],\n",
      "        [ 0.5013, -0.8621,  1.3593, -0.4439,  1.9260,  0.9209],\n",
      "        [ 1.3593, -0.4439,  1.9260,  0.9209, -0.6208, -2.2271],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -0.3862, -0.5910],\n",
      "        [-0.6208, -2.2271, -0.3862, -0.5910,  0.3759,  0.5625],\n",
      "        [-0.3862, -0.5910,  0.3759,  0.5625,  0.1141, -0.4797]])\n"
     ]
    }
   ],
   "source": [
    "### with unbind + cat\n",
    "  # unbind: removes a dimension and returns tuple of each slice\n",
    "  # e.g dim=1, so we slice along 0,1,2 for emb\n",
    "  # get: emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], ...\n",
    "# cat: concatenate\n",
    "sliced = torch.unbind(emb, dim=1)\n",
    "cat_ver = torch.cat(sliced , dim=1)\n",
    "\n",
    "# 32x3x2\n",
    "    # flatten by dim=(1,2), meaning flatten the (3x2) -> 6 each\n",
    "flattened = torch.flatten(emb, 1, 2)\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "emb_view = emb.view((32, BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# all of the above are equivalent\n",
    "print(cat_ver.logical_and(flattened).flatten().all())\n",
    "print(flattened.logical_and(cat_ver).flatten().all())\n",
    "\n",
    "# [C[0], C[0], C[0]]\n",
    "# [C[0], C[0], C[5]]\n",
    "# [C[0], C[5], C[13]]\n",
    "# ...and so on\n",
    "    # we are just putting the embeddings per example into one row, before it was 3 rows\n",
    "print(emb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "In the matrix multiplication emb_view @ W, each row of emb_view represents a flattened embedding (e.g 6 values for a context of size 3, with each character having 2D embeddings). \n",
    "\n",
    "The matrix W has e.g 6 rows (one for each input value) and W_OUT columns (or more, depending on the number of neurons). Each column in W represents the weights for one neuron. \n",
    "\n",
    "The result of the multiplication is a matrix where each row corresponds to the linear combination (dot product) of the input embeddings with each neuron's weights. \n",
    "\n",
    "Bias\n",
    "- Each neuron - column of W - has its own bias\n",
    "- So bias also has W_OUT columns for the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: tensor([ 2.0187e-01,  1.4710e+00,  1.9650e+00,  9.3848e-02, -2.3441e-01,\n",
      "        -4.7062e-01,  1.7981e+00,  1.3198e-01,  2.7150e+00, -7.4946e-01,\n",
      "        -5.1111e-01,  3.9081e-01,  1.6414e+00, -8.1366e-01, -9.3341e-01,\n",
      "        -7.3788e-01,  8.5509e-02, -1.7808e+00,  1.1113e+00, -5.5110e-01,\n",
      "         5.0704e-01, -6.7913e-01, -1.0101e+00,  9.1796e-01,  7.6946e-01,\n",
      "        -4.4623e-01,  1.2764e+00, -1.2066e-01, -4.7289e-01, -1.0512e+00,\n",
      "        -4.3958e-03,  1.6660e-01,  2.3627e+00, -2.6778e-01,  1.3920e+00,\n",
      "        -2.6669e-02, -8.4985e-01,  1.5683e-01,  5.6315e-01,  4.2754e-01,\n",
      "         5.4978e-01,  6.3640e-01,  3.1889e-01, -6.2480e-01, -4.2308e-01,\n",
      "        -1.9012e+00,  1.8052e-01,  7.1911e-01,  1.6683e+00,  2.8072e+00,\n",
      "         2.7626e-01, -5.8916e-01,  7.7466e-02, -5.5283e-01,  1.4628e+00,\n",
      "        -1.7897e-02,  5.1046e-02,  1.7784e+00, -4.5596e-01, -5.8732e-01,\n",
      "         5.9618e-01,  1.8748e-01, -1.0953e+00, -1.1454e-02, -1.4278e+00,\n",
      "        -5.5494e-01,  2.6929e-02, -8.8307e-01,  1.4867e+00,  1.3102e+00,\n",
      "         7.8010e-01,  8.0268e-01, -7.5872e-02, -3.8843e-01,  8.5350e-01,\n",
      "         2.7264e-01,  5.7208e-01, -2.8076e+00, -6.2541e-01,  3.3910e-01,\n",
      "         1.2967e+00,  2.2458e+00, -6.3808e-01, -1.1681e+00, -3.9329e-01,\n",
      "         6.5543e-01, -1.2356e+00, -3.7094e-01,  5.9898e-01, -6.9444e-01,\n",
      "         1.1406e+00, -2.7965e-03, -1.9256e+00,  7.9234e-01, -1.7960e-01,\n",
      "         1.3692e-01, -1.3680e+00,  1.0828e+00,  1.1273e+00,  4.7051e-01]) torch.Size([100])\n",
      "Weights: tensor([[-7.1810e-01,  8.3600e-01, -1.0791e+00,  1.2057e+00,  2.6973e-01,\n",
      "          5.2423e-01, -6.1063e-02, -6.9666e-01, -3.4132e-01,  1.1586e+00,\n",
      "         -1.0619e+00, -1.1610e+00,  4.3032e-01, -1.9583e+00,  9.3387e-02,\n",
      "         -1.0691e+00,  1.5235e+00, -6.9664e-02, -5.6455e-01, -7.9108e-01,\n",
      "          1.0705e-01,  2.7462e-01, -1.1039e+00, -1.4048e+00,  7.8364e-01,\n",
      "          3.6128e-01,  6.5409e-01, -1.1603e+00,  4.6247e-02, -2.9984e-01,\n",
      "         -2.1354e-01,  1.2460e+00, -8.3801e-01,  1.0456e+00,  1.5562e+00,\n",
      "         -2.6961e-01,  7.9354e-01, -8.5949e-02, -1.5612e+00,  3.5904e-01,\n",
      "         -1.2349e+00, -6.2868e-01,  6.5242e-01,  1.4185e-01,  4.7228e-01,\n",
      "          2.1225e-01,  7.2978e-01, -1.7944e+00, -3.9467e-01, -1.2257e-01,\n",
      "         -1.3193e+00,  1.1875e-02,  1.2833e+00,  2.6286e-01,  6.6034e-02,\n",
      "         -1.1686e+00,  3.2837e-01, -1.7837e+00, -1.6474e+00,  1.9871e-01,\n",
      "          1.0863e+00, -6.5903e-01,  1.5766e-01,  1.2503e+00, -2.5422e-01,\n",
      "         -1.4714e-01,  2.1997e-01,  1.3157e+00,  2.2878e-01,  1.1145e+00,\n",
      "         -3.2283e-01, -8.0235e-02,  2.4072e+00, -1.0029e+00, -1.3323e+00,\n",
      "         -8.9409e-01, -1.6066e+00,  1.3429e+00,  9.6337e-01,  1.4746e+00,\n",
      "         -4.4829e-01,  1.4953e-01, -8.8030e-01,  1.1096e-01, -5.8594e-01,\n",
      "         -8.9357e-01,  9.2534e-01, -1.0908e+00,  8.0932e-02,  2.3321e+00,\n",
      "          1.6550e+00, -3.8977e-01, -7.7072e-01,  3.8948e-01,  2.7477e+00,\n",
      "         -1.5581e+00, -2.5622e+00,  4.6791e-01,  1.7387e-01,  1.3312e+00],\n",
      "        [-7.1088e-01,  2.5429e-02,  1.0890e+00,  1.1829e-01,  2.2395e+00,\n",
      "         -1.2319e+00, -2.8846e-01, -1.3064e+00,  4.4597e-01, -1.8175e+00,\n",
      "         -1.3924e-01,  1.3249e+00, -6.2185e-01, -9.7683e-01,  1.0342e+00,\n",
      "          1.7657e-01,  8.3350e-01,  1.5388e-02,  5.7120e-01,  8.5034e-02,\n",
      "         -2.8718e-01,  8.9056e-01,  3.7022e-01,  1.3262e+00, -1.7049e+00,\n",
      "          3.8610e-01, -2.0026e+00,  6.0783e-01, -1.6551e+00, -5.9929e-01,\n",
      "         -3.6991e-01,  2.3647e-01, -1.1753e+00,  5.3637e-01, -9.0471e-01,\n",
      "         -1.2309e+00, -5.2558e-01, -3.8553e-01, -1.0000e+00,  6.1530e-02,\n",
      "         -1.4148e-01, -3.2006e-02, -2.1585e-01,  8.3349e-01, -6.4297e-01,\n",
      "         -1.6277e+00,  7.8763e-01, -5.3737e-01,  1.3131e+00, -5.9624e-01,\n",
      "          8.3240e-01, -3.0251e-01, -4.3890e-01, -6.0381e-01,  7.1719e-01,\n",
      "         -7.8713e-01,  3.3999e-01, -8.1010e-01,  4.6389e-01,  3.9152e-01,\n",
      "         -4.9597e-02, -3.6354e-01, -2.6023e+00,  1.1007e+00, -1.7645e+00,\n",
      "          6.7286e-01, -1.7120e-02, -8.4894e-01, -1.8022e+00,  8.7541e-02,\n",
      "         -1.1445e+00,  2.0378e+00,  2.6010e+00, -2.2385e-01, -1.1921e+00,\n",
      "         -2.0477e-01,  1.0826e+00, -5.8233e-01, -1.2191e+00, -1.2410e+00,\n",
      "         -5.1301e-01,  1.8155e-01, -1.4117e+00, -2.0157e+00, -4.1464e-01,\n",
      "          1.4782e+00,  4.7040e-01, -1.2020e+00,  1.9502e-01, -1.2976e+00,\n",
      "          6.0715e-01, -8.0226e-02, -9.8605e-01,  7.5322e-02, -9.2196e-01,\n",
      "         -1.0853e-01,  1.2442e+00,  1.2436e+00, -1.1895e+00, -3.2544e-01],\n",
      "        [ 8.1460e-01,  4.8126e-01,  6.4170e-01,  7.8046e-01,  7.1301e-01,\n",
      "          2.4376e+00,  7.4449e-02,  9.6073e-01, -8.6273e-01, -8.3088e-02,\n",
      "          5.9447e-02, -7.6873e-01, -6.5726e-01, -5.6905e-01,  1.9764e-01,\n",
      "         -1.7378e+00, -6.6617e-01, -8.9936e-01, -1.2121e+00,  4.4946e-01,\n",
      "          8.8620e-01, -3.0014e-01,  2.0293e+00,  1.4800e+00,  6.6429e-01,\n",
      "          1.1615e+00, -6.1566e-01, -9.6866e-01,  1.0670e+00,  9.4479e-01,\n",
      "          1.4494e+00, -7.3998e-01,  4.7223e-01, -5.5969e-01, -1.4609e-01,\n",
      "         -4.3357e-01, -1.4038e+00,  2.1768e+00, -6.2962e-01, -4.4866e-01,\n",
      "         -2.7296e-02, -2.4778e-01, -5.1016e-01, -1.3836e+00,  7.8202e-01,\n",
      "         -1.0901e+00, -1.3537e+00,  4.2172e-01,  1.6125e-01,  6.1557e-01,\n",
      "          2.7566e-01, -4.2143e-01, -9.7029e-01,  1.9328e+00, -9.1598e-01,\n",
      "         -1.0598e+00,  1.5106e+00,  6.7990e-01, -7.2256e-01, -8.4603e-01,\n",
      "          2.5762e-01, -4.8791e-01,  1.1588e+00,  9.8472e-01,  4.2132e-01,\n",
      "          1.3541e+00,  9.6074e-01,  1.0960e+00,  8.0618e-01, -5.7572e-01,\n",
      "          2.0584e+00,  8.5097e-01, -3.5352e-01,  3.7652e-01, -3.9866e-01,\n",
      "          5.2502e-02,  7.4158e-01, -5.7739e-01, -3.3450e-01, -8.8310e-01,\n",
      "         -1.3411e+00,  1.0866e+00, -1.0402e+00,  1.2738e+00, -2.3889e-01,\n",
      "          1.6581e+00,  9.3606e-02, -1.0642e+00,  5.2892e-01, -1.3660e+00,\n",
      "          8.4573e-01, -4.6098e-02,  1.0687e+00, -1.6301e+00, -9.5060e-01,\n",
      "          3.9265e-01, -1.4449e+00, -1.8935e+00,  8.7514e-01,  1.4432e+00],\n",
      "        [ 1.4128e+00,  5.7770e-01,  3.9623e-01, -1.4493e-01, -5.7405e-01,\n",
      "         -1.2095e-01,  1.3899e+00, -1.2030e+00,  2.4030e-01, -1.6544e-01,\n",
      "          2.0322e+00, -8.2616e-01, -5.3636e-01,  1.3758e+00, -2.3940e-01,\n",
      "         -8.7633e-01, -8.3108e-01,  2.8242e-01,  1.0010e+00, -2.7377e-01,\n",
      "          1.1995e+00,  2.8674e-01,  2.1740e+00, -4.7309e-01, -5.2840e-01,\n",
      "          3.0895e-01, -5.4864e-01,  8.3270e-01, -3.5875e-01,  7.1682e-01,\n",
      "         -5.3256e-01,  7.4503e-01, -4.4002e-01,  6.9609e-01, -2.2072e-01,\n",
      "          1.3650e+00,  2.1544e-01, -9.3033e-02, -9.6285e-01,  3.8327e-01,\n",
      "          6.3439e-01, -4.1400e-01, -1.9272e-01,  1.4764e+00,  8.8626e-01,\n",
      "         -2.9724e-01,  4.7020e-01, -1.0364e+00,  6.8510e-01, -1.9297e-01,\n",
      "          6.4189e-01,  5.6032e-01,  1.2352e+00,  1.8413e+00, -2.2121e+00,\n",
      "          5.7039e-02, -2.0588e-01, -9.3632e-01, -1.7652e+00, -3.7639e-02,\n",
      "         -7.4322e-01, -7.6209e-01, -1.3490e+00,  1.2202e+00,  3.7299e-01,\n",
      "         -4.4424e-01, -3.5138e-01, -3.1173e-02,  3.4857e-01, -2.0550e+00,\n",
      "          9.3395e-01,  3.3232e-01,  3.5048e-02,  1.9604e-02,  4.7809e-01,\n",
      "         -7.2230e-01,  1.6689e+00,  1.2793e+00, -4.1101e-01, -8.7899e-01,\n",
      "          3.2840e-01, -3.4789e-01,  8.5007e-01,  7.6753e-01, -7.4803e-03,\n",
      "          3.2052e-01,  2.0832e-01,  2.2560e-01, -7.7178e-01,  1.0816e+00,\n",
      "         -2.1803e+00,  2.3792e-01, -5.1057e-01, -6.6335e-01,  8.1924e-01,\n",
      "          7.5172e-01,  3.9705e-01,  9.2073e-01, -8.5089e-01, -5.3331e-02],\n",
      "        [ 3.6215e-01, -2.6254e-01,  6.1347e-01,  8.0390e-01,  3.2292e+00,\n",
      "          2.8667e-01, -1.8176e+00, -1.6987e+00,  2.2694e-01, -5.3206e-01,\n",
      "          2.8164e-01,  6.6339e-01,  1.7068e+00,  8.2487e-02, -1.5641e+00,\n",
      "         -5.7801e-01, -4.7712e-01, -9.4715e-01, -2.8117e-01, -4.9719e-01,\n",
      "          7.6347e-01,  5.1177e-01, -4.4473e-01, -1.6531e+00, -7.2736e-01,\n",
      "          1.4863e-01, -1.6997e-01,  3.4705e-01, -2.4451e-02, -5.5490e-01,\n",
      "         -1.4115e+00,  1.4792e+00, -1.8170e+00, -6.6434e-01,  5.7631e-01,\n",
      "          1.7041e+00, -1.0011e+00,  7.5991e-01, -7.4984e-01,  5.8652e-01,\n",
      "         -7.6236e-01,  6.8469e-01, -7.1102e-01, -2.8819e-01, -7.1024e-02,\n",
      "         -1.9569e-01, -2.2118e-01,  1.4781e+00,  5.0949e-01,  7.0875e-01,\n",
      "         -2.3041e+00, -5.2504e-01,  6.0890e-01, -2.3092e+00, -7.2373e-01,\n",
      "          4.3035e-01, -1.1462e+00,  3.7116e-01,  1.3328e+00, -1.9126e-01,\n",
      "         -4.0249e-02, -1.1455e+00,  1.8672e+00, -1.3552e+00, -2.0998e+00,\n",
      "         -8.5932e-01,  9.8540e-01, -2.6434e+00,  2.6961e-01,  1.2891e-01,\n",
      "         -4.6070e-01, -9.3153e-01, -4.2360e-02,  1.4518e+00, -7.7915e-01,\n",
      "         -2.3505e+00, -9.9457e-01,  1.8400e-01, -1.8827e+00, -1.7448e+00,\n",
      "          1.4177e+00,  3.2395e-01, -6.2231e-02,  3.0489e-01,  1.2175e-01,\n",
      "          5.5836e-01, -4.4466e-01,  3.4690e-01, -2.6629e-01,  4.9420e-01,\n",
      "          3.8629e-01, -2.5782e+00,  2.2763e+00,  9.1525e-01,  3.8684e-01,\n",
      "          2.1511e+00,  3.7783e-01,  6.1362e-01, -4.9867e-01, -2.9690e-01],\n",
      "        [ 1.8192e+00,  7.5188e-01, -2.5624e-02, -2.3873e-01,  3.0330e-02,\n",
      "          1.6655e-01, -7.2749e-01, -4.5378e-01,  1.3745e-01, -7.3632e-01,\n",
      "         -2.0109e-01, -1.1377e+00,  7.4136e-01,  1.1940e+00, -1.3213e-01,\n",
      "         -7.9466e-01,  1.5852e-02,  7.4152e-01,  4.9078e-03,  1.7905e+00,\n",
      "          4.2722e-01,  2.9182e-01, -1.1522e+00,  2.9243e-01, -1.4846e-01,\n",
      "         -7.7229e-02, -1.8028e+00, -8.4563e-01, -7.5941e-01,  1.3174e+00,\n",
      "         -1.0508e+00, -2.3263e-01, -3.1366e-01,  1.3844e+00,  3.6892e-01,\n",
      "         -9.4231e-01,  8.4514e-01,  1.6384e+00,  5.4606e-01,  9.9718e-01,\n",
      "         -1.0060e+00, -3.3119e-01,  6.2402e-01, -1.3950e-01, -3.4087e-01,\n",
      "          1.8141e-01,  4.2441e-01, -1.0516e+00,  2.1074e+00, -2.9034e-01,\n",
      "         -9.8687e-01, -1.1533e+00,  1.1101e+00,  1.4474e-01,  4.5651e-01,\n",
      "          1.0404e+00, -8.0491e-01, -1.4995e+00, -2.8694e-01, -1.4160e+00,\n",
      "          1.0657e+00, -1.3080e+00,  1.6061e+00,  7.3484e-01, -5.9020e-01,\n",
      "          1.1070e+00, -1.7763e+00, -7.2777e-01,  1.4331e+00,  9.5111e-01,\n",
      "         -1.2630e+00, -4.2146e-01,  5.7142e-01, -6.2862e-01,  1.6844e+00,\n",
      "         -4.0412e-01, -1.6352e+00, -1.0059e+00,  6.9011e-04, -1.9917e+00,\n",
      "         -1.4201e+00, -1.0875e+00, -2.0102e+00, -1.1761e+00,  2.2165e-01,\n",
      "         -1.2756e+00,  1.9618e+00, -1.6436e+00,  9.9192e-01, -2.2536e+00,\n",
      "         -3.3507e-01, -1.3871e+00, -1.5678e+00, -1.6249e+00,  1.2603e+00,\n",
      "         -1.0253e+00,  3.1168e-01,  1.2904e+00, -1.4584e+00, -1.4469e-01]])\n"
     ]
    }
   ],
   "source": [
    "W_OUT = 100\n",
    "\n",
    "# (6,100)\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN)\n",
    "# (1,100)\n",
    "    # 1 bias per neuron (per column)\n",
    "bias = torch.randn(W_OUT, generator=GEN)\n",
    "print(\"Bias:\", bias, bias.shape)\n",
    "\n",
    "\n",
    "# bias = torch.randn()\n",
    "print(\"Weights:\", W)\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "# h for hidden layer\n",
    "h = emb_view @ W\n",
    "\n",
    "# why this works: bias is (100,)\n",
    "# broadcast: \n",
    "    # h: (32, 100)\n",
    "    # b: (  , 100) -> (1,100), 1 is inferred\n",
    "    # so it copies bias and adds to each row as expected - each column gets a different bias value, for each neuron\n",
    "h = h + bias\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [6.8365e-14, 4.1561e-10, 2.0476e-05, 1.9685e-01, 3.6644e-09, 2.5611e-04,\n",
       "         2.3277e-09, 4.3530e-05, 4.4425e-05, 1.3900e-09, 2.9489e-05, 2.1639e-03,\n",
       "         8.2406e-09, 9.1830e-03, 5.0304e-05, 1.9712e-07, 1.0256e-03, 6.0565e-08,\n",
       "         1.5621e-02, 1.3701e-06, 3.1891e-06, 1.0589e-05, 4.8030e-07, 2.0710e-04,\n",
       "         2.5028e-02, 6.3929e-02, 6.8553e-01],\n",
       "        [1.3785e-10, 7.1116e-05, 1.9685e-11, 3.7020e-08, 3.8112e-07, 6.1939e-05,\n",
       "         1.0589e-03, 2.4612e-02, 5.9960e-05, 8.2954e-06, 2.8608e-03, 1.1369e-06,\n",
       "         2.0626e-08, 1.2665e-06, 1.3825e-03, 6.0687e-05, 5.0792e-06, 2.9640e-02,\n",
       "         2.6060e-12, 1.0480e-08, 8.6222e-06, 5.8362e-08, 3.8414e-02, 6.9616e-01,\n",
       "         1.3612e-06, 2.0560e-01, 4.1925e-08],\n",
       "        [1.1296e-08, 9.9169e-01, 8.2763e-05, 4.7307e-08, 5.2716e-07, 1.1432e-07,\n",
       "         4.5556e-08, 1.5861e-03, 2.5982e-06, 3.0303e-08, 4.2567e-05, 4.3100e-10,\n",
       "         4.2078e-09, 4.7673e-07, 1.3123e-06, 3.5119e-03, 1.8188e-09, 9.2137e-10,\n",
       "         1.0143e-03, 5.2248e-10, 1.5106e-04, 5.9120e-09, 2.2771e-06, 1.9160e-03,\n",
       "         6.2971e-09, 5.7147e-09, 1.9878e-06],\n",
       "        [2.4788e-08, 4.1117e-05, 1.1683e-08, 8.8031e-05, 3.0312e-08, 1.8094e-03,\n",
       "         1.0015e-07, 1.4834e-08, 9.9315e-11, 1.4141e-08, 5.7513e-06, 4.2593e-04,\n",
       "         1.1685e-07, 2.8681e-01, 9.1562e-07, 3.4057e-02, 3.5671e-09, 1.1302e-13,\n",
       "         6.4183e-06, 6.1721e-08, 7.6164e-03, 7.1072e-07, 7.6732e-09, 6.6902e-01,\n",
       "         1.1124e-04, 6.7153e-12, 1.0122e-05],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [1.3228e-10, 5.2603e-01, 6.8273e-09, 7.1021e-09, 1.1829e-07, 8.2693e-05,\n",
       "         7.0980e-05, 1.4303e-01, 4.5650e-06, 1.5805e-07, 7.8844e-03, 1.0340e-06,\n",
       "         8.8627e-08, 4.5004e-07, 4.7476e-03, 6.4546e-07, 3.2316e-07, 8.9055e-03,\n",
       "         1.6497e-09, 3.4401e-08, 1.4834e-07, 2.0662e-07, 2.8947e-03, 1.7122e-01,\n",
       "         8.3633e-07, 1.3513e-01, 6.4751e-07],\n",
       "        [1.2745e-10, 1.0000e+00, 1.6020e-13, 1.8645e-10, 5.1141e-12, 4.2361e-10,\n",
       "         1.4395e-13, 2.1033e-07, 6.0098e-10, 1.2448e-12, 7.6635e-11, 1.1237e-09,\n",
       "         1.3887e-10, 3.6748e-09, 2.8929e-10, 9.9027e-09, 4.6133e-13, 1.2200e-15,\n",
       "         2.7734e-08, 2.8084e-12, 1.5415e-08, 3.5499e-10, 2.3313e-09, 4.0335e-07,\n",
       "         4.8197e-09, 3.6785e-10, 4.0072e-07],\n",
       "        [1.5062e-10, 7.9447e-01, 6.8998e-12, 3.8265e-08, 6.0892e-12, 1.4840e-01,\n",
       "         1.6850e-08, 2.2217e-09, 7.1332e-12, 1.0418e-03, 6.1412e-04, 3.0613e-04,\n",
       "         2.4696e-10, 2.5273e-03, 7.4339e-04, 7.8838e-04, 2.7189e-09, 2.0782e-17,\n",
       "         2.9269e-08, 3.4234e-11, 5.7498e-07, 8.5817e-07, 1.0154e-07, 5.1110e-02,\n",
       "         3.4165e-10, 1.5504e-12, 9.0301e-09],\n",
       "        [9.9902e-10, 1.1785e-09, 3.0751e-11, 3.6634e-06, 2.3274e-05, 2.2613e-12,\n",
       "         8.7994e-07, 6.4068e-09, 9.5297e-13, 3.0796e-10, 1.9769e-11, 3.4849e-10,\n",
       "         4.9741e-11, 5.3920e-11, 2.3951e-08, 2.5569e-09, 5.8760e-14, 5.1701e-12,\n",
       "         6.6037e-09, 5.7791e-12, 9.9997e-01, 9.0664e-11, 2.0022e-11, 1.0661e-06,\n",
       "         1.4851e-11, 1.8723e-14, 5.8557e-14],\n",
       "        [1.1216e-13, 1.3773e-04, 2.3751e-10, 4.4798e-06, 1.0092e-08, 4.3905e-05,\n",
       "         9.5929e-10, 2.1847e-09, 9.9114e-11, 1.4299e-09, 9.3408e-08, 1.6257e-09,\n",
       "         5.2884e-12, 6.7871e-09, 2.2369e-06, 3.5295e-08, 2.5777e-08, 7.6389e-13,\n",
       "         9.9981e-01, 1.1991e-09, 6.5274e-09, 2.5702e-10, 3.7675e-09, 1.0881e-07,\n",
       "         1.2617e-08, 4.5884e-13, 8.0979e-09],\n",
       "        [1.3089e-11, 7.7038e-09, 5.3127e-08, 4.4668e-05, 5.2548e-07, 7.5878e-04,\n",
       "         1.7066e-07, 1.0781e-07, 7.4577e-06, 9.0856e-09, 7.0690e-08, 1.3405e-01,\n",
       "         1.4165e-05, 6.1325e-04, 3.4102e-01, 2.6888e-07, 2.6087e-07, 2.2872e-07,\n",
       "         8.1178e-08, 7.1988e-07, 5.2155e-01, 1.1549e-09, 1.6845e-04, 6.5212e-05,\n",
       "         5.1928e-05, 6.6453e-08, 1.6523e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [3.0727e-11, 7.0382e-08, 9.0770e-05, 2.8462e-01, 4.4219e-03, 5.0848e-03,\n",
       "         2.2958e-04, 1.3853e-02, 5.1958e-02, 5.3968e-07, 1.6313e-04, 6.4986e-02,\n",
       "         6.8246e-08, 3.2213e-03, 3.9932e-02, 3.9735e-05, 2.0214e-04, 7.5298e-05,\n",
       "         2.0479e-02, 1.0849e-05, 1.3969e-01, 4.4869e-05, 3.1109e-05, 1.6134e-01,\n",
       "         1.0194e-01, 4.7200e-03, 1.0287e-01],\n",
       "        [7.1443e-10, 8.3208e-06, 1.3397e-08, 1.1205e-06, 7.6466e-01, 1.5721e-07,\n",
       "         1.3244e-01, 1.1216e-03, 1.5530e-04, 9.2832e-08, 4.5542e-10, 3.3853e-07,\n",
       "         3.4060e-08, 5.6231e-07, 1.9254e-04, 2.3368e-07, 2.6900e-08, 2.8312e-04,\n",
       "         3.6823e-07, 1.0882e-07, 8.5466e-04, 3.7873e-08, 2.7222e-08, 1.0012e-01,\n",
       "         1.5806e-04, 1.8494e-08, 1.4281e-06],\n",
       "        [6.5291e-14, 1.5102e-07, 1.1729e-04, 5.7639e-02, 1.9839e-04, 3.0956e-04,\n",
       "         7.3039e-06, 6.2358e-05, 3.8526e-06, 3.4572e-09, 7.1664e-06, 7.9779e-06,\n",
       "         8.5429e-10, 5.1024e-06, 1.1610e-04, 1.6946e-04, 2.3829e-04, 1.8623e-07,\n",
       "         8.3640e-01, 2.6639e-08, 7.1354e-06, 5.8141e-07, 2.0660e-09, 9.9714e-02,\n",
       "         2.7150e-03, 1.2070e-06, 2.2771e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [5.2470e-11, 7.6938e-03, 1.0777e-05, 5.0911e-03, 1.0572e-06, 3.5413e-03,\n",
       "         6.2156e-06, 1.4944e-01, 1.4113e-01, 1.6016e-07, 2.7195e-02, 4.4333e-02,\n",
       "         2.4544e-03, 8.7723e-03, 4.1838e-03, 1.5251e-05, 1.1605e-03, 5.5702e-05,\n",
       "         6.4435e-04, 7.8044e-04, 9.9097e-06, 7.1613e-03, 1.7232e-01, 7.4297e-03,\n",
       "         1.7301e-01, 2.0271e-01, 4.0855e-02],\n",
       "        [4.2015e-10, 1.6522e-02, 1.3572e-10, 1.3594e-07, 6.9696e-03, 1.1380e-06,\n",
       "         4.4457e-07, 6.5977e-04, 1.6107e-03, 2.7494e-08, 3.9008e-07, 1.2763e-05,\n",
       "         1.0397e-07, 7.4413e-08, 1.6280e-02, 1.5473e-02, 1.3636e-07, 1.3538e-06,\n",
       "         1.4636e-08, 2.6405e-07, 2.7999e-01, 2.0693e-08, 1.5857e-03, 6.6085e-01,\n",
       "         2.5812e-06, 3.1863e-06, 4.2912e-05],\n",
       "        [1.9791e-11, 2.0595e-06, 9.9719e-03, 9.5977e-04, 2.9245e-04, 1.4362e-04,\n",
       "         5.3716e-08, 2.1452e-06, 7.4617e-09, 7.7082e-08, 1.6292e-04, 7.7487e-07,\n",
       "         1.6527e-10, 6.1820e-04, 7.4312e-06, 5.2019e-04, 3.0618e-06, 1.2894e-13,\n",
       "         8.4620e-01, 1.9118e-07, 1.0804e-03, 2.0602e-08, 1.7234e-09, 1.3877e-01,\n",
       "         1.2669e-03, 4.8764e-13, 2.6270e-08],\n",
       "        [3.8494e-13, 7.3627e-01, 1.5682e-08, 6.9540e-09, 1.6663e-13, 4.1843e-04,\n",
       "         2.2122e-04, 3.6886e-07, 4.4483e-06, 3.9973e-03, 1.1547e-05, 1.0779e-02,\n",
       "         2.1516e-04, 2.0246e-04, 4.0275e-04, 4.2318e-07, 1.5429e-07, 3.6351e-07,\n",
       "         1.0134e-12, 2.1719e-07, 4.3730e-10, 1.8890e-05, 9.8278e-02, 4.5051e-05,\n",
       "         2.2433e-11, 1.4914e-01, 4.5380e-07],\n",
       "        [3.4293e-06, 2.5420e-09, 7.8812e-06, 1.4604e-03, 5.9348e-10, 1.0571e-05,\n",
       "         4.2870e-12, 1.2451e-07, 1.5078e-08, 5.8704e-12, 3.9242e-09, 2.5691e-07,\n",
       "         5.6767e-10, 4.0119e-05, 3.8598e-05, 7.5505e-05, 8.8371e-11, 1.2825e-14,\n",
       "         2.6853e-05, 5.5664e-10, 2.1989e-02, 6.9340e-09, 2.2668e-06, 2.2171e-07,\n",
       "         1.0763e-04, 1.4859e-05, 9.7622e-01],\n",
       "        [9.5699e-08, 2.0733e-04, 3.2152e-09, 2.9141e-05, 3.3797e-10, 2.8711e-04,\n",
       "         8.9165e-01, 6.9198e-11, 4.6177e-12, 1.0739e-01, 8.8537e-05, 8.1354e-05,\n",
       "         4.6162e-08, 1.0546e-06, 7.2800e-07, 3.1520e-09, 1.0961e-10, 2.2440e-09,\n",
       "         2.5469e-12, 1.2000e-09, 3.3605e-07, 7.7052e-08, 3.1465e-06, 2.6781e-04,\n",
       "         3.4917e-11, 4.7030e-12, 2.6047e-12],\n",
       "        [2.8478e-08, 6.6653e-01, 1.0830e-05, 1.4612e-08, 1.2338e-08, 5.5271e-07,\n",
       "         3.3243e-08, 2.8572e-01, 8.4457e-04, 3.5494e-08, 1.8938e-04, 6.2184e-10,\n",
       "         8.0545e-09, 3.9368e-08, 1.7321e-04, 3.9506e-02, 6.0118e-07, 1.4032e-10,\n",
       "         1.8904e-04, 6.0291e-09, 8.0527e-05, 2.7112e-06, 9.3329e-05, 6.6551e-03,\n",
       "         4.2669e-08, 7.3339e-08, 7.3088e-07],\n",
       "        [7.5847e-06, 1.7647e-05, 3.9970e-07, 2.8866e-05, 1.8172e-08, 4.0296e-03,\n",
       "         5.9501e-07, 3.4804e-09, 1.2925e-11, 2.8633e-07, 2.7979e-04, 5.0431e-06,\n",
       "         7.6742e-09, 5.2283e-02, 2.0955e-04, 8.9330e-02, 1.8862e-09, 5.8623e-14,\n",
       "         2.1015e-06, 8.6655e-09, 2.9872e-01, 7.3246e-08, 1.0754e-07, 5.5508e-01,\n",
       "         6.8259e-07, 3.2069e-11, 8.3104e-09],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [8.1452e-10, 1.8016e-04, 6.5544e-08, 3.9486e-07, 1.7942e-03, 1.1060e-06,\n",
       "         2.1602e-04, 3.6675e-04, 6.3153e-06, 4.7813e-09, 2.2747e-06, 1.1809e-07,\n",
       "         4.0151e-08, 6.1695e-07, 7.7422e-05, 2.8504e-07, 1.5279e-07, 1.3777e-05,\n",
       "         1.0753e-07, 1.9894e-08, 5.9891e-06, 1.0691e-07, 8.8311e-07, 9.9588e-01,\n",
       "         1.4456e-03, 1.2169e-07, 1.1007e-05],\n",
       "        [7.7901e-12, 9.9992e-01, 6.1512e-12, 2.5135e-12, 1.2922e-09, 9.6660e-09,\n",
       "         1.6270e-10, 2.6213e-05, 5.4496e-08, 1.1987e-11, 2.1822e-06, 2.0637e-08,\n",
       "         1.6035e-09, 5.7746e-11, 1.4566e-08, 4.4576e-08, 2.2200e-10, 1.5848e-08,\n",
       "         1.6301e-09, 1.3031e-10, 1.2187e-08, 2.6812e-10, 5.4765e-09, 1.4868e-05,\n",
       "         5.2247e-10, 3.7397e-05, 1.2600e-06],\n",
       "        [5.7278e-12, 2.3306e-14, 7.1772e-07, 7.7967e-08, 3.1102e-05, 1.2452e-07,\n",
       "         6.8755e-11, 4.1059e-11, 1.9512e-09, 2.4815e-14, 1.4978e-14, 4.9167e-08,\n",
       "         1.4188e-12, 5.8159e-06, 1.3131e-04, 4.6438e-07, 7.4308e-13, 1.4926e-12,\n",
       "         3.6095e-06, 2.8189e-13, 9.9983e-01, 1.9827e-13, 8.4867e-13, 1.6814e-06,\n",
       "         3.0432e-10, 2.2336e-14, 3.1839e-08],\n",
       "        [2.0404e-10, 2.5702e-08, 1.2095e-08, 5.2292e-04, 3.6576e-11, 8.6851e-06,\n",
       "         9.9664e-01, 1.1695e-10, 3.9258e-14, 1.3928e-07, 4.2754e-07, 1.4162e-04,\n",
       "         6.5149e-10, 2.6736e-03, 7.3567e-09, 2.9535e-12, 1.2528e-07, 7.2461e-06,\n",
       "         3.3744e-11, 1.2678e-11, 1.5350e-08, 6.9142e-12, 5.0439e-12, 1.1578e-06,\n",
       "         2.3050e-11, 2.3835e-08, 6.5479e-14],\n",
       "        [8.7365e-15, 5.7650e-10, 5.0590e-03, 2.5860e-06, 9.1061e-07, 2.7300e-08,\n",
       "         6.1033e-05, 1.9516e-02, 3.6828e-02, 1.0858e-08, 3.8272e-09, 3.0933e-06,\n",
       "         1.9118e-12, 2.9697e-09, 8.6193e-06, 2.6454e-07, 2.5217e-03, 1.0188e-02,\n",
       "         7.1391e-08, 1.6662e-06, 2.4008e-08, 7.3626e-10, 1.2556e-04, 1.7289e-06,\n",
       "         7.3063e-01, 1.9229e-01, 2.7674e-03],\n",
       "        [1.1174e-09, 1.1508e-09, 3.5377e-05, 6.8658e-05, 1.1534e-04, 3.6285e-09,\n",
       "         4.2690e-08, 9.5894e-01, 1.8759e-03, 6.1524e-11, 4.6356e-08, 2.2982e-11,\n",
       "         5.4032e-12, 1.4064e-05, 1.7364e-02, 7.6190e-06, 5.5600e-07, 2.6349e-10,\n",
       "         5.0398e-04, 3.2318e-08, 2.0003e-02, 7.7636e-10, 1.1831e-07, 2.9509e-04,\n",
       "         7.5410e-04, 1.9720e-06, 2.1938e-05]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "res = h @ W2 + b2\n",
    "probs = res.softmax(dim=1)\n",
    "\n",
    "# ach row is normalized\n",
    "print(probs.sum(dim=1))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding probabilities for each y label, get NLL Loss\n",
    "torch.arange(m) -> 0,1,2,3...m-1 for row indices\n",
    "Y: e.g [0,5,13,13,6..] -> actual labels. so we end up getting\n",
    "- res[0,0], res[1,5], res[2,13] etc in a vector\n",
    "- This is the predicted probability based on current weights for that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8111e-04, 9.1830e-03, 1.2665e-06, 9.9169e-01, 2.4788e-08, 6.1128e-06,\n",
       "        8.8627e-08, 1.2448e-12, 1.0154e-07, 3.0796e-10, 1.3773e-04, 1.3089e-11,\n",
       "        7.5123e-07, 3.1109e-05, 8.3208e-06, 6.5291e-14, 1.6017e-06, 7.8044e-04,\n",
       "        1.6522e-02, 9.9719e-03, 4.1843e-04, 5.6767e-10, 4.6162e-08, 6.6653e-01,\n",
       "        7.5847e-06, 4.1817e-07, 2.8504e-07, 2.2200e-10, 1.9512e-09, 1.3928e-07,\n",
       "        5.7650e-10, 1.1174e-09])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(Y)\n",
    "probs[torch.arange(m), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5532)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = -probs.log().mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--SETUP--#\n",
    "\n",
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(2147483647)\n",
    "# Number of examples\n",
    "M = len(Xtr) \n",
    "\n",
    "# Params: C, W, bias, W2, b2\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "\n",
    "# Forward pass\n",
    "W_OUT = 300\n",
    "\n",
    "# First layer\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN) # (6,100)\n",
    "bias = torch.randn(W_OUT, generator=GEN) # (1,100): 1 bias per neuron (per column)\n",
    "\n",
    "# Second layer: W_OUT inputs, 27 outputs for log-counts (to become probabilities)\n",
    "W2 = torch.randn((W_OUT, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "parameters = [C, W, bias, W2, b2]\n",
    "\n",
    "for param in parameters:\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3425703048706055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x407ebe7f0>]"
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--TRAINING--#\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "EPOCHS=50000\n",
    "LR=0.1\n",
    "BATCH_SIZE=128\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # select a random batch and use that for fwd + backward for faster iteration\n",
    "    # batch_indices = torch.randperm(len(Xtr), generator=GEN)\n",
    "    # batch_indices = batch_indices[:BATCH_SIZE]\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (32,), generator=GEN)\n",
    "\n",
    "    emb = C[Xtr[batch_indices]] # lookup embeddings corresponding to each row -> (M, BLOCK_SIZE, 2) i.e one 2D vector per char in a row of X\n",
    "\n",
    "    # view shares the same underlying elements, just rearranges logically\n",
    "        # (number of examples, number of elems per example)\n",
    "    emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "    \n",
    "    # forward pass: get linear combs + bias for each row, activate with tanh\n",
    "    h = emb_view @ W\n",
    "    h = h + bias\n",
    "    h = h.tanh()\n",
    "    \n",
    "    # forward pass 2nd layer to get 27 outputs per row\n",
    "    probs = h @ W2 + b2\n",
    "    \n",
    "    # this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "    nll_loss = F.cross_entropy(probs, Ytr[batch_indices])\n",
    "    \n",
    "    epochs.append(i)\n",
    "    losses.append(nll_loss.log10().item())\n",
    "    # --Update--#\n",
    "\n",
    "    nll_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    # for param in parameters:\n",
    "    #     param.grad = None\n",
    "    \n",
    "    # # backward pass\n",
    "    # nll_loss.backward()\n",
    "    \n",
    "    # # update\n",
    "    # with torch.no_grad():\n",
    "    #     for param in parameters:\n",
    "    #         param -= LR * param.grad\n",
    "            \n",
    "print(\"Loss:\", nll_loss.item())\n",
    "\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loss for dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1825, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ydev)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1842, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 1073,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtest]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ytest)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sysda\n",
      "daiyana\n",
      "iabina\n",
      "arman\n",
      "cobreyia\n",
      "gidriey\n",
      "hareon\n",
      "gaygon\n",
      "gaviland\n",
      "meka\n",
      "khia\n",
      "tery\n",
      "varo\n",
      "markio\n",
      "otya\n",
      "synoro\n",
      "bzelana\n",
      "nikolaiden\n",
      "sheva\n",
      "xam\n"
     ]
    }
   ],
   "source": [
    "def get_word(max_count=10):\n",
    "    current_block = [0] * BLOCK_SIZE\n",
    "    string = []\n",
    "\n",
    "    for i in range(max_count):\n",
    "        emb = C[current_block].view(1, BLOCK_SIZE * EMBEDDING_SIZE)\n",
    "        \n",
    "        # fwd pass\n",
    "        h = emb @ W + bias\n",
    "        h = h.tanh()\n",
    "    \n",
    "        probs = h @ W2 + b2\n",
    "        # turn into probabilities\n",
    "        probs = probs.softmax(dim=1)\n",
    "    \n",
    "        sampled = torch.multinomial(probs, num_samples=1, replacement=True, generator=GEN)\n",
    "        sampled_idx = sampled.item()\n",
    "        char = itos[sampled_idx]\n",
    "\n",
    "        if char == '.':\n",
    "            break\n",
    "\n",
    "        string.append(char)\n",
    "\n",
    "        current_block = current_block[1:] + [sampled_idx]\n",
    "\n",
    "\n",
    "    return ''.join(string)\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    word = get_word()\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'gaygon' in names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQDGcWn074pA7NdAWB8vTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env 28Sep)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
