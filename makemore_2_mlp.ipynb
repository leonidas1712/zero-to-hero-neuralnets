{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNe_RPhx3Cx"
   },
   "source": [
    "## Makemore Part 2: MLP\n",
    "## Best result: 2.1447 dev loss, 2.1379 train loss, beating Andrej's 2.20 dev loss challenge\n",
    "- Increased context size to 4\n",
    "- Implemented an extra part from the paper of connecting word features directly to final layer\n",
    "- Used PyTorch SGD optimizer with a step scheduler, gamma=0.1 every 10k steps\n",
    "\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Idea from paper: C is a lookup table (matrix) for embeddings vector of each of the words in V e.g |V|=17k\n",
    "- C: (17000x30)\n",
    "- one-hot encoding of a word: (1x17000)\n",
    "- enc @ C -> (1x30) embedding vector\n",
    "\n",
    "We can get these vectors for multiple input words (words that came before).\n",
    "\n",
    "Pass them to neurons with n_in=30, those neurons are fully connected to a hidden layer, then goes through tanh -> softmax for probabilities.\n",
    "\n",
    "Final output: 17k vector of probabilities for the 17k words in vocab.\n",
    "- Train by using actual next word's index, get the predicted prob, do -log(prob) and backprop etc.\n",
    "\n",
    "---\n",
    "\n",
    "We can use this idea for a character-level model as well.\n",
    "\n",
    "## Architecture\n",
    "- Start with indexes from the chars/words in context\n",
    "- Convert them to respective embeddings vectors in C (lookup)\n",
    "- Stack the vectors' values together, do forward pass with weights matrix and add bias\n",
    "- Activate through tanh\n",
    "- Pass through one more layer to get 27 outputs (one for each unique char in our vocab)\n",
    "- Softmax to get probabilities\n",
    "\n",
    "## Building dataset\n",
    "Hyperparameter: BLOCK_SIZE = 3\n",
    "- block_size is the number of previous chars we consider when predicting next\n",
    "\n",
    "For each word, we add to X with block_size=3 char windows, Y has the char that comes after\n",
    "- e.g '.emma': (..., e), (..e, m), (.em, m), (emm, a), (mma, .)\n",
    "- So each word contributes n+1 examples as before, n = len(word)\n",
    "\n",
    "## Lookup table\n",
    "Lookup table: C = (27,2) random init\n",
    "- In paper, they compress 17k words of vocab into Rn of n=30\n",
    "- So we do similar here for 27 unique chars -> embeddings of size 2\n",
    "\n",
    "Previously, we used one-hot encoding to lookup with enc @ W\n",
    "- But this is just the same as doing W[idx] due to all the zeroes\n",
    "\n",
    "In PyTorch, we can just do C[X] and it will work\n",
    "- produces (32,3,2) - one 2D vector for each encoded char\n",
    "- Or another way to think about it, one (3x2) vector for each row in X. 3 because BLOCK_SIZE=3, so each row in X has 3 elements. For each of those chars, we want one 2D vector - its embedding\n",
    "\n",
    "## F.cross_entropy\n",
    "F.cross_entropy(logits, targets) is the same as doing:\n",
    "\n",
    "```python\n",
    "logits = logits.softmax(dim=1)\n",
    "logits = logits[torch.arange(M), Y]\n",
    "nll_loss = -logits.log().mean()\n",
    "```\n",
    "i.e same as softmax -> select corresponding probabilities for targets in Y -> get NLL Loss (mean)\n",
    "\n",
    "F.cross_entropy is **better** because PyTorch can optimise and not create new memory, and it can use **fused kernels** to cluster ops together and run them at the same time.\n",
    "- Also more numerically stable. logits can be subtracted or added by any number. when logits are too high, e^(high number) becomes inf, then we get nans. But PyTorch can internally subtract the data by the max number to prevent this\n",
    "\n",
    "## Batching\n",
    "Instead of fwd + backward on whole dataset which is slow, we can pick a random batch each time and fwd + backward on that.\n",
    "Just select BATCH_SIZE of random indices within [0,M), then use X[batch_indices] and Y[batch_indices]\n",
    "\n",
    "## Finding good learning rate\n",
    "1. Find min and max bounds by trial and error\n",
    "- Set very low LR, low is if it barely changes\n",
    "- High is if loss grows\n",
    "2. Create torch.linspace between low and high using exponents for the lrs e.g -3, 0 for 10^-3 -> 10^0=1\n",
    "3. During loop, ith iteration uses ith LR in list for the batch. Track the loss for that LR\n",
    "4. Graph it and find the valley in the graph - the middle should be a good LR.\n",
    "\n",
    "## Train, test, validation split (80,10,10)\n",
    "Train split: Used to optimise model parameters with gradient descent\n",
    "\n",
    "Validation split: Used to tune hyperparameters: e.g outputs in hidden layers, size of embeddings in C\n",
    "  \n",
    "Test split: Used to evaluate the performance of the model at the **end**\n",
    "- Why: achieving low loss on the same set we used to train isn't necessarily a good thing. Because when we use bigger models, they are able to just memorise the dataset, then doesn't generalise to new inputs well.\n",
    "\n",
    "Training: we tune hyperparameters by checking repeatedly on dev split, then at the end check loss against test split just one time and this is the final number we report.\n",
    "\n",
    "## Overfitting, underfitting\n",
    "Underfitting: when train loss and dev or test loss are both high and close to each other\n",
    "- Means the model isn't big enough / powerful enough to fit the data\n",
    "\n",
    "Overfitting: when train loss is low, but dev/test loss are much higher\n",
    "- Means the model is memorising the training data but doesn't generalise well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 10072,
     "status": "ok",
     "timestamp": 1727498203827,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "E1zlN5r9xtTz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl1lLlcpJtBM"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1727488190995,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "INrlJJdu70z3",
    "outputId": "4fa9ff58-4349-46f0-fbb5-5b7f861df269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 32033\n",
      "['yuheng', 'diondre', 'xavien', 'jori', 'juanluis']\n",
      "25626 28829\n"
     ]
    }
   ],
   "source": [
    "# Open up names\n",
    "names = []\n",
    "with open('names.txt', 'r') as names_file:\n",
    "  names = names_file.read().splitlines()\n",
    "\n",
    "print(\"Total length:\", len(names))\n",
    "\n",
    "# build lookups\n",
    "uniq = ['.'] + sorted(list(set(''.join(names))))\n",
    "stoi = { char: idx for idx, char in enumerate(uniq)}\n",
    "itos = { idx: char for char,idx in stoi.items() }\n",
    "\n",
    "# Hyperparameters\n",
    "BLOCK_SIZE = 4\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "def build_dataset(names):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for word in names:\n",
    "      # print(word)\n",
    "      word = word + '.'\n",
    "      block = [0] * BLOCK_SIZE # ... (empty context at start)\n",
    "    \n",
    "      for char in word:\n",
    "        char_idx = stoi[char]\n",
    "        X.append(block)\n",
    "        Y.append(char_idx)\n",
    "    \n",
    "        block_str = ''.join(list(map(lambda i: itos[i], block)))\n",
    "        # print(f'{block_str} -> {char}')\n",
    "    \n",
    "        # update block: roll over sliding window\n",
    "        block = block[1:] + [char_idx]\n",
    "\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# train, test, dev split: 80,10,10\n",
    "from random import Random\n",
    "Random(42).shuffle(names)\n",
    "\n",
    "print(names[0:5])\n",
    "\n",
    "n_80 = int(0.8*len(names))\n",
    "n_90 = int(0.9*len(names))\n",
    "print(n_80, n_90)\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n_80]) # 80%\n",
    "Xdev, Ydev = build_dataset(names[n_80:n_90]) # 10%\n",
    "Xtest, Ytest = build_dataset(names[n_90:]) # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228146\n",
      "0.8004742577121667\n",
      "0.09930044795876325\n",
      "0.10022529432906999\n",
      "182625 22655 22866\n"
     ]
    }
   ],
   "source": [
    "total = len(Xtr) + len(Xdev) + len(Xtest)\n",
    "print(total)\n",
    "print(len(Xtr) / total)\n",
    "print(len(Xdev) / total)\n",
    "print(len(Xtest) / total)\n",
    "print(len(Xtr) , len(Xdev), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1727489615781,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "vUo1EJwqJzs0",
    "outputId": "b6926d3a-389f-4be1-9eae-bf58b6f6b454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 4]) torch.Size([182625]) torch.int64 torch.int64\n",
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 0,  0,  0, 25],\n",
      "        [ 0,  0, 25, 21],\n",
      "        ...,\n",
      "        [ 8, 15, 12,  4],\n",
      "        [15, 12,  4,  1],\n",
      "        [12,  4,  1, 14]])\n",
      "---\n",
      "tensor([25, 21,  8,  ...,  1, 14,  0])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape, Ytr.shape, Xtr.dtype, Ytr.dtype)\n",
    "print(Xtr)\n",
    "print(\"---\")\n",
    "print(Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_c8JD9Z8Bd"
   },
   "source": [
    "## Lookup table C, embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1727490140478,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "J7DEHynvMtq7",
    "outputId": "f79ae109-2ddb-40ef-895e-6553c96d2fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5013, -0.8621,  0.1141, -0.4797,  1.0472,  1.5424, -1.2521, -1.6304,\n",
      "          0.9521, -0.3236],\n",
      "        [-0.5833, -0.0969, -0.5720,  1.8011, -0.0412, -0.1098, -0.3862, -0.5910,\n",
      "          0.3759,  0.5625],\n",
      "        [ 1.4190, -0.6763,  1.7523,  0.7620,  1.0095,  0.6250,  1.4192,  0.3493,\n",
      "         -0.8872,  0.1808],\n",
      "        [ 1.9260,  0.9209, -0.6208, -2.2271, -1.5889,  0.9987,  0.3235,  0.5826,\n",
      "         -1.2369, -0.5786],\n",
      "        [-0.9113,  1.4787,  0.3230, -1.6748, -0.3587,  0.8224, -0.5762, -0.1008,\n",
      "          0.6782, -0.5032],\n",
      "        [ 0.0545, -0.4910, -1.7107, -0.1619,  1.3593, -0.4439,  1.0834,  0.0320,\n",
      "         -0.5812,  0.0108],\n",
      "        [-0.0482, -0.1170, -0.0892,  0.2336, -0.4153,  0.0076,  0.7098,  0.6353,\n",
      "          0.2616, -1.0070],\n",
      "        [-0.7181,  0.8360, -0.9140, -0.3356,  0.6749, -0.6606,  0.0102, -0.3768,\n",
      "         -0.3413,  1.1586],\n",
      "        [-0.5271, -1.4327,  0.5918,  1.5556, -0.8974,  0.6933,  1.5235, -0.0697,\n",
      "          1.2950, -1.4680],\n",
      "        [ 0.0252,  0.8468,  0.1797, -1.2906,  0.7836,  0.3613,  0.0547,  1.3996,\n",
      "         -0.2099,  1.9860],\n",
      "        [ 0.1808,  0.3776, -0.8380,  1.0456, -0.9487,  0.4099,  0.3002,  0.3258,\n",
      "         -1.2162, -0.9100],\n",
      "        [-1.2349, -0.6287,  0.2815,  1.0406,  0.0178,  0.1483,  1.0566,  0.2054,\n",
      "         -0.3947, -0.1226],\n",
      "        [-0.2265,  0.1289, -0.4223,  0.8507,  0.0965, -1.7318,  0.3284, -1.7837,\n",
      "         -1.1700, -0.1490],\n",
      "        [-0.0549,  0.3628, -0.5278, -0.4945, -0.2542, -0.1471, -0.8297, -0.7266,\n",
      "         -0.4838, -1.7872],\n",
      "        [ 0.3676, -0.8236,  2.4072, -1.0029,  0.8090,  1.9090, -1.2104, -0.3956,\n",
      "          0.8433,  0.3986],\n",
      "        [-0.4483,  0.1495, -1.5291,  1.0313,  1.2232,  0.3802, -0.0795, -0.6631,\n",
      "          0.0809,  2.3321],\n",
      "        [ 0.8383,  0.8010, -0.4180,  0.3827, -0.6356, -1.2418, -2.5622,  0.4679,\n",
      "          0.3027, -1.5873],\n",
      "        [ 1.3803, -0.9970,  0.1950, -0.7574,  2.2395, -1.2319, -1.8203, -1.7171,\n",
      "          0.5226,  1.5027],\n",
      "        [ 2.1024,  0.3466, -0.6218, -0.9768,  0.5534, -1.1111, -0.8860,  0.7296,\n",
      "         -1.8385,  0.6464],\n",
      "        [-0.2872,  0.8906, -0.3067,  0.7088, -1.8585,  0.4963, -1.0555, -0.3514,\n",
      "         -1.6551, -0.5993],\n",
      "        [ 0.1360,  0.1325, -0.0439,  0.5588,  1.4310, -0.6275, -0.5256, -0.3855,\n",
      "         -1.4038, -0.1902],\n",
      "        [-0.9724, -2.4353, -0.8207,  0.7638, -0.6430, -1.6277,  0.6287,  0.2157,\n",
      "         -0.7200,  1.8399],\n",
      "        [ 0.9315,  0.6752, -0.4389, -0.6038,  0.4971,  1.3256,  0.2866,  0.2505,\n",
      "         -0.4102, -0.3827],\n",
      "        [-0.0496, -0.3635, -0.0551, -0.6078, -0.0129,  1.4252, -0.0224,  0.8542,\n",
      "         -1.8022,  0.0875],\n",
      "        [-1.1542,  0.0965,  0.5871, -2.1649, -0.5822,  0.8462,  1.0826, -0.5823,\n",
      "         -0.5806, -0.6051],\n",
      "        [ 0.7079, -1.1227, -1.5542, -0.9094, -0.9417, -0.3635, -1.0071, -0.3720,\n",
      "         -0.4000,  0.2393],\n",
      "        [ 0.8201,  0.7582,  0.3197,  0.6152, -0.1345,  0.7046,  0.1137,  0.5721,\n",
      "         -0.8211, -0.3797]]) \n",
      "------\n",
      "\n",
      "Embeddings:\n",
      "torch.Size([182625, 4, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393]],\n",
       "\n",
       "        [[ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.5013, -0.8621,  0.1141,  ..., -1.6304,  0.9521, -0.3236],\n",
       "         [ 0.7079, -1.1227, -1.5542,  ..., -0.3720, -0.4000,  0.2393],\n",
       "         [-0.9724, -2.4353, -0.8207,  ...,  0.2157, -0.7200,  1.8399]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5271, -1.4327,  0.5918,  ..., -0.0697,  1.2950, -1.4680],\n",
       "         [-0.4483,  0.1495, -1.5291,  ..., -0.6631,  0.0809,  2.3321],\n",
       "         [-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032]],\n",
       "\n",
       "        [[-0.4483,  0.1495, -1.5291,  ..., -0.6631,  0.0809,  2.3321],\n",
       "         [-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625]],\n",
       "\n",
       "        [[-0.2265,  0.1289, -0.4223,  ..., -1.7837, -1.1700, -0.1490],\n",
       "         [-0.9113,  1.4787,  0.3230,  ..., -0.1008,  0.6782, -0.5032],\n",
       "         [-0.5833, -0.0969, -0.5720,  ..., -0.5910,  0.3759,  0.5625],\n",
       "         [ 0.3676, -0.8236,  2.4072,  ..., -0.3956,  0.8433,  0.3986]]])"
      ]
     },
     "execution_count": 1122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(1097)\n",
    "\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "print(C, \"\\n------\\n\")\n",
    "\n",
    "# Embedding\n",
    "    # (32,3,2)\n",
    "emb = C[Xtr]\n",
    "\n",
    "# For each size 3 block in X (each row), we get a (3x2) matrix which has all the 2D embeddings for the chars that make up the block\n",
    "  # e.g X[2] = [0,5,13], so emb[2] is [C[0], C[5], C[13]] stacked vertically\n",
    "# print(emb[2])\n",
    "# example = torch.cat((C[0], C[5], C[13]), dim=0).view((BLOCK_SIZE ,EMBEDDING_SIZE))\n",
    "# same as emb[2]\n",
    "# print(example)\n",
    "# print(\"\\n------\\n\")\n",
    "\n",
    "\n",
    "print(\"Embeddings:\")\n",
    "print(emb.shape)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck__BTsebPl5"
   },
   "source": [
    "## Different ways to flatten\n",
    "(32,3,2) -> (32,6) so we can keep W to (6, 100)\n",
    "- 3x2=6 determined by block size and embedding size\n",
    "\n",
    "torch.cat, torch.unbind creates new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1727490193315,
     "user": {
      "displayName": "Leonidas Raghav",
      "userId": "15052167714208649093"
     },
     "user_tz": -480
    },
    "id": "5TGsS6o-Q2HD",
    "outputId": "48929d69-be45-45d1-8069-373cc42e1b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "torch.Size([182625, 40])\n"
     ]
    }
   ],
   "source": [
    "### with unbind + cat\n",
    "  # unbind: removes a dimension and returns tuple of each slice\n",
    "  # e.g dim=1, so we slice along 0,1,2 for emb\n",
    "  # get: emb[:, 0, :], emb[:, 1, :], emb[:, 2, :], ...\n",
    "# cat: concatenate\n",
    "sliced = torch.unbind(emb, dim=1)\n",
    "cat_ver = torch.cat(sliced , dim=1)\n",
    "\n",
    "# 32x3x2\n",
    "    # flatten by dim=(1,2), meaning flatten the (3x2) -> 6 each\n",
    "flattened = torch.flatten(emb, 1, 2)\n",
    "\n",
    "# view shares the same underlying elements, just rearranges logically\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "\n",
    "# all of the above are equivalent\n",
    "print(cat_ver.logical_and(flattened).flatten().all())\n",
    "print(flattened.logical_and(cat_ver).flatten().all())\n",
    "\n",
    "# [C[0], C[0], C[0]]\n",
    "# [C[0], C[0], C[5]]\n",
    "# [C[0], C[5], C[13]]\n",
    "# ...and so on\n",
    "    # we are just putting the embeddings per example into one row, before it was 3 rows\n",
    "print(emb_view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "In the matrix multiplication emb_view @ W, each row of emb_view represents a flattened embedding (e.g 6 values for a context of size 3, with each character having 2D embeddings). \n",
    "\n",
    "The matrix W has e.g 6 rows (one for each input value) and W_OUT columns (or more, depending on the number of neurons). Each column in W represents the weights for one neuron. \n",
    "\n",
    "The result of the multiplication is a matrix where each row corresponds to the linear combination (dot product) of the input embeddings with each neuron's weights. \n",
    "\n",
    "Bias\n",
    "- Each neuron - column of W - has its own bias\n",
    "- So bias also has W_OUT columns for the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: tensor([ 2.0187e-01,  1.4710e+00,  1.9650e+00,  9.3848e-02, -2.3441e-01,\n",
      "        -4.7062e-01,  1.7981e+00,  1.3198e-01,  2.7150e+00, -7.4946e-01,\n",
      "        -5.1111e-01,  3.9081e-01,  1.6414e+00, -8.1366e-01, -9.3341e-01,\n",
      "        -7.3788e-01,  8.5509e-02, -1.7808e+00,  1.1113e+00, -5.5110e-01,\n",
      "         5.0704e-01, -6.7913e-01, -1.0101e+00,  9.1796e-01,  7.6946e-01,\n",
      "        -4.4623e-01,  1.2764e+00, -1.2066e-01, -4.7289e-01, -1.0512e+00,\n",
      "        -4.3958e-03,  1.6660e-01,  2.3627e+00, -2.6778e-01,  1.3920e+00,\n",
      "        -2.6669e-02, -8.4985e-01,  1.5683e-01,  5.6315e-01,  4.2754e-01,\n",
      "         5.4978e-01,  6.3640e-01,  3.1889e-01, -6.2480e-01, -4.2308e-01,\n",
      "        -1.9012e+00,  1.8052e-01,  7.1911e-01,  1.6683e+00,  2.8072e+00,\n",
      "         2.7626e-01, -5.8916e-01,  7.7466e-02, -5.5283e-01,  1.4628e+00,\n",
      "        -1.7897e-02,  5.1046e-02,  1.7784e+00, -4.5596e-01, -5.8732e-01,\n",
      "         5.9618e-01,  1.8748e-01, -1.0953e+00, -1.1454e-02, -1.4278e+00,\n",
      "        -5.5494e-01,  2.6929e-02, -8.8307e-01,  1.4867e+00,  1.3102e+00,\n",
      "         7.8010e-01,  8.0268e-01, -7.5872e-02, -3.8843e-01,  8.5350e-01,\n",
      "         2.7264e-01,  5.7208e-01, -2.8076e+00, -6.2541e-01,  3.3910e-01,\n",
      "         1.2967e+00,  2.2458e+00, -6.3808e-01, -1.1681e+00, -3.9329e-01,\n",
      "         6.5543e-01, -1.2356e+00, -3.7094e-01,  5.9898e-01, -6.9444e-01,\n",
      "         1.1406e+00, -2.7965e-03, -1.9256e+00,  7.9234e-01, -1.7960e-01,\n",
      "         1.3692e-01, -1.3680e+00,  1.0828e+00,  1.1273e+00,  4.7051e-01]) torch.Size([100])\n",
      "Weights: tensor([[-7.1810e-01,  8.3600e-01, -1.0791e+00,  1.2057e+00,  2.6973e-01,\n",
      "          5.2423e-01, -6.1063e-02, -6.9666e-01, -3.4132e-01,  1.1586e+00,\n",
      "         -1.0619e+00, -1.1610e+00,  4.3032e-01, -1.9583e+00,  9.3387e-02,\n",
      "         -1.0691e+00,  1.5235e+00, -6.9664e-02, -5.6455e-01, -7.9108e-01,\n",
      "          1.0705e-01,  2.7462e-01, -1.1039e+00, -1.4048e+00,  7.8364e-01,\n",
      "          3.6128e-01,  6.5409e-01, -1.1603e+00,  4.6247e-02, -2.9984e-01,\n",
      "         -2.1354e-01,  1.2460e+00, -8.3801e-01,  1.0456e+00,  1.5562e+00,\n",
      "         -2.6961e-01,  7.9354e-01, -8.5949e-02, -1.5612e+00,  3.5904e-01,\n",
      "         -1.2349e+00, -6.2868e-01,  6.5242e-01,  1.4185e-01,  4.7228e-01,\n",
      "          2.1225e-01,  7.2978e-01, -1.7944e+00, -3.9467e-01, -1.2257e-01,\n",
      "         -1.3193e+00,  1.1875e-02,  1.2833e+00,  2.6286e-01,  6.6034e-02,\n",
      "         -1.1686e+00,  3.2837e-01, -1.7837e+00, -1.6474e+00,  1.9871e-01,\n",
      "          1.0863e+00, -6.5903e-01,  1.5766e-01,  1.2503e+00, -2.5422e-01,\n",
      "         -1.4714e-01,  2.1997e-01,  1.3157e+00,  2.2878e-01,  1.1145e+00,\n",
      "         -3.2283e-01, -8.0235e-02,  2.4072e+00, -1.0029e+00, -1.3323e+00,\n",
      "         -8.9409e-01, -1.6066e+00,  1.3429e+00,  9.6337e-01,  1.4746e+00,\n",
      "         -4.4829e-01,  1.4953e-01, -8.8030e-01,  1.1096e-01, -5.8594e-01,\n",
      "         -8.9357e-01,  9.2534e-01, -1.0908e+00,  8.0932e-02,  2.3321e+00,\n",
      "          1.6550e+00, -3.8977e-01, -7.7072e-01,  3.8948e-01,  2.7477e+00,\n",
      "         -1.5581e+00, -2.5622e+00,  4.6791e-01,  1.7387e-01,  1.3312e+00],\n",
      "        [-7.1088e-01,  2.5429e-02,  1.0890e+00,  1.1829e-01,  2.2395e+00,\n",
      "         -1.2319e+00, -2.8846e-01, -1.3064e+00,  4.4597e-01, -1.8175e+00,\n",
      "         -1.3924e-01,  1.3249e+00, -6.2185e-01, -9.7683e-01,  1.0342e+00,\n",
      "          1.7657e-01,  8.3350e-01,  1.5388e-02,  5.7120e-01,  8.5034e-02,\n",
      "         -2.8718e-01,  8.9056e-01,  3.7022e-01,  1.3262e+00, -1.7049e+00,\n",
      "          3.8610e-01, -2.0026e+00,  6.0783e-01, -1.6551e+00, -5.9929e-01,\n",
      "         -3.6991e-01,  2.3647e-01, -1.1753e+00,  5.3637e-01, -9.0471e-01,\n",
      "         -1.2309e+00, -5.2558e-01, -3.8553e-01, -1.0000e+00,  6.1530e-02,\n",
      "         -1.4148e-01, -3.2006e-02, -2.1585e-01,  8.3349e-01, -6.4297e-01,\n",
      "         -1.6277e+00,  7.8763e-01, -5.3737e-01,  1.3131e+00, -5.9624e-01,\n",
      "          8.3240e-01, -3.0251e-01, -4.3890e-01, -6.0381e-01,  7.1719e-01,\n",
      "         -7.8713e-01,  3.3999e-01, -8.1010e-01,  4.6389e-01,  3.9152e-01,\n",
      "         -4.9597e-02, -3.6354e-01, -2.6023e+00,  1.1007e+00, -1.7645e+00,\n",
      "          6.7286e-01, -1.7120e-02, -8.4894e-01, -1.8022e+00,  8.7541e-02,\n",
      "         -1.1445e+00,  2.0378e+00,  2.6010e+00, -2.2385e-01, -1.1921e+00,\n",
      "         -2.0477e-01,  1.0826e+00, -5.8233e-01, -1.2191e+00, -1.2410e+00,\n",
      "         -5.1301e-01,  1.8155e-01, -1.4117e+00, -2.0157e+00, -4.1464e-01,\n",
      "          1.4782e+00,  4.7040e-01, -1.2020e+00,  1.9502e-01, -1.2976e+00,\n",
      "          6.0715e-01, -8.0226e-02, -9.8605e-01,  7.5322e-02, -9.2196e-01,\n",
      "         -1.0853e-01,  1.2442e+00,  1.2436e+00, -1.1895e+00, -3.2544e-01],\n",
      "        [ 8.1460e-01,  4.8126e-01,  6.4170e-01,  7.8046e-01,  7.1301e-01,\n",
      "          2.4376e+00,  7.4449e-02,  9.6073e-01, -8.6273e-01, -8.3088e-02,\n",
      "          5.9447e-02, -7.6873e-01, -6.5726e-01, -5.6905e-01,  1.9764e-01,\n",
      "         -1.7378e+00, -6.6617e-01, -8.9936e-01, -1.2121e+00,  4.4946e-01,\n",
      "          8.8620e-01, -3.0014e-01,  2.0293e+00,  1.4800e+00,  6.6429e-01,\n",
      "          1.1615e+00, -6.1566e-01, -9.6866e-01,  1.0670e+00,  9.4479e-01,\n",
      "          1.4494e+00, -7.3998e-01,  4.7223e-01, -5.5969e-01, -1.4609e-01,\n",
      "         -4.3357e-01, -1.4038e+00,  2.1768e+00, -6.2962e-01, -4.4866e-01,\n",
      "         -2.7296e-02, -2.4778e-01, -5.1016e-01, -1.3836e+00,  7.8202e-01,\n",
      "         -1.0901e+00, -1.3537e+00,  4.2172e-01,  1.6125e-01,  6.1557e-01,\n",
      "          2.7566e-01, -4.2143e-01, -9.7029e-01,  1.9328e+00, -9.1598e-01,\n",
      "         -1.0598e+00,  1.5106e+00,  6.7990e-01, -7.2256e-01, -8.4603e-01,\n",
      "          2.5762e-01, -4.8791e-01,  1.1588e+00,  9.8472e-01,  4.2132e-01,\n",
      "          1.3541e+00,  9.6074e-01,  1.0960e+00,  8.0618e-01, -5.7572e-01,\n",
      "          2.0584e+00,  8.5097e-01, -3.5352e-01,  3.7652e-01, -3.9866e-01,\n",
      "          5.2502e-02,  7.4158e-01, -5.7739e-01, -3.3450e-01, -8.8310e-01,\n",
      "         -1.3411e+00,  1.0866e+00, -1.0402e+00,  1.2738e+00, -2.3889e-01,\n",
      "          1.6581e+00,  9.3606e-02, -1.0642e+00,  5.2892e-01, -1.3660e+00,\n",
      "          8.4573e-01, -4.6098e-02,  1.0687e+00, -1.6301e+00, -9.5060e-01,\n",
      "          3.9265e-01, -1.4449e+00, -1.8935e+00,  8.7514e-01,  1.4432e+00],\n",
      "        [ 1.4128e+00,  5.7770e-01,  3.9623e-01, -1.4493e-01, -5.7405e-01,\n",
      "         -1.2095e-01,  1.3899e+00, -1.2030e+00,  2.4030e-01, -1.6544e-01,\n",
      "          2.0322e+00, -8.2616e-01, -5.3636e-01,  1.3758e+00, -2.3940e-01,\n",
      "         -8.7633e-01, -8.3108e-01,  2.8242e-01,  1.0010e+00, -2.7377e-01,\n",
      "          1.1995e+00,  2.8674e-01,  2.1740e+00, -4.7309e-01, -5.2840e-01,\n",
      "          3.0895e-01, -5.4864e-01,  8.3270e-01, -3.5875e-01,  7.1682e-01,\n",
      "         -5.3256e-01,  7.4503e-01, -4.4002e-01,  6.9609e-01, -2.2072e-01,\n",
      "          1.3650e+00,  2.1544e-01, -9.3033e-02, -9.6285e-01,  3.8327e-01,\n",
      "          6.3439e-01, -4.1400e-01, -1.9272e-01,  1.4764e+00,  8.8626e-01,\n",
      "         -2.9724e-01,  4.7020e-01, -1.0364e+00,  6.8510e-01, -1.9297e-01,\n",
      "          6.4189e-01,  5.6032e-01,  1.2352e+00,  1.8413e+00, -2.2121e+00,\n",
      "          5.7039e-02, -2.0588e-01, -9.3632e-01, -1.7652e+00, -3.7639e-02,\n",
      "         -7.4322e-01, -7.6209e-01, -1.3490e+00,  1.2202e+00,  3.7299e-01,\n",
      "         -4.4424e-01, -3.5138e-01, -3.1173e-02,  3.4857e-01, -2.0550e+00,\n",
      "          9.3395e-01,  3.3232e-01,  3.5048e-02,  1.9604e-02,  4.7809e-01,\n",
      "         -7.2230e-01,  1.6689e+00,  1.2793e+00, -4.1101e-01, -8.7899e-01,\n",
      "          3.2840e-01, -3.4789e-01,  8.5007e-01,  7.6753e-01, -7.4803e-03,\n",
      "          3.2052e-01,  2.0832e-01,  2.2560e-01, -7.7178e-01,  1.0816e+00,\n",
      "         -2.1803e+00,  2.3792e-01, -5.1057e-01, -6.6335e-01,  8.1924e-01,\n",
      "          7.5172e-01,  3.9705e-01,  9.2073e-01, -8.5089e-01, -5.3331e-02],\n",
      "        [ 3.6215e-01, -2.6254e-01,  6.1347e-01,  8.0390e-01,  3.2292e+00,\n",
      "          2.8667e-01, -1.8176e+00, -1.6987e+00,  2.2694e-01, -5.3206e-01,\n",
      "          2.8164e-01,  6.6339e-01,  1.7068e+00,  8.2487e-02, -1.5641e+00,\n",
      "         -5.7801e-01, -4.7712e-01, -9.4715e-01, -2.8117e-01, -4.9719e-01,\n",
      "          7.6347e-01,  5.1177e-01, -4.4473e-01, -1.6531e+00, -7.2736e-01,\n",
      "          1.4863e-01, -1.6997e-01,  3.4705e-01, -2.4451e-02, -5.5490e-01,\n",
      "         -1.4115e+00,  1.4792e+00, -1.8170e+00, -6.6434e-01,  5.7631e-01,\n",
      "          1.7041e+00, -1.0011e+00,  7.5991e-01, -7.4984e-01,  5.8652e-01,\n",
      "         -7.6236e-01,  6.8469e-01, -7.1102e-01, -2.8819e-01, -7.1024e-02,\n",
      "         -1.9569e-01, -2.2118e-01,  1.4781e+00,  5.0949e-01,  7.0875e-01,\n",
      "         -2.3041e+00, -5.2504e-01,  6.0890e-01, -2.3092e+00, -7.2373e-01,\n",
      "          4.3035e-01, -1.1462e+00,  3.7116e-01,  1.3328e+00, -1.9126e-01,\n",
      "         -4.0249e-02, -1.1455e+00,  1.8672e+00, -1.3552e+00, -2.0998e+00,\n",
      "         -8.5932e-01,  9.8540e-01, -2.6434e+00,  2.6961e-01,  1.2891e-01,\n",
      "         -4.6070e-01, -9.3153e-01, -4.2360e-02,  1.4518e+00, -7.7915e-01,\n",
      "         -2.3505e+00, -9.9457e-01,  1.8400e-01, -1.8827e+00, -1.7448e+00,\n",
      "          1.4177e+00,  3.2395e-01, -6.2231e-02,  3.0489e-01,  1.2175e-01,\n",
      "          5.5836e-01, -4.4466e-01,  3.4690e-01, -2.6629e-01,  4.9420e-01,\n",
      "          3.8629e-01, -2.5782e+00,  2.2763e+00,  9.1525e-01,  3.8684e-01,\n",
      "          2.1511e+00,  3.7783e-01,  6.1362e-01, -4.9867e-01, -2.9690e-01],\n",
      "        [ 1.8192e+00,  7.5188e-01, -2.5624e-02, -2.3873e-01,  3.0330e-02,\n",
      "          1.6655e-01, -7.2749e-01, -4.5378e-01,  1.3745e-01, -7.3632e-01,\n",
      "         -2.0109e-01, -1.1377e+00,  7.4136e-01,  1.1940e+00, -1.3213e-01,\n",
      "         -7.9466e-01,  1.5852e-02,  7.4152e-01,  4.9078e-03,  1.7905e+00,\n",
      "          4.2722e-01,  2.9182e-01, -1.1522e+00,  2.9243e-01, -1.4846e-01,\n",
      "         -7.7229e-02, -1.8028e+00, -8.4563e-01, -7.5941e-01,  1.3174e+00,\n",
      "         -1.0508e+00, -2.3263e-01, -3.1366e-01,  1.3844e+00,  3.6892e-01,\n",
      "         -9.4231e-01,  8.4514e-01,  1.6384e+00,  5.4606e-01,  9.9718e-01,\n",
      "         -1.0060e+00, -3.3119e-01,  6.2402e-01, -1.3950e-01, -3.4087e-01,\n",
      "          1.8141e-01,  4.2441e-01, -1.0516e+00,  2.1074e+00, -2.9034e-01,\n",
      "         -9.8687e-01, -1.1533e+00,  1.1101e+00,  1.4474e-01,  4.5651e-01,\n",
      "          1.0404e+00, -8.0491e-01, -1.4995e+00, -2.8694e-01, -1.4160e+00,\n",
      "          1.0657e+00, -1.3080e+00,  1.6061e+00,  7.3484e-01, -5.9020e-01,\n",
      "          1.1070e+00, -1.7763e+00, -7.2777e-01,  1.4331e+00,  9.5111e-01,\n",
      "         -1.2630e+00, -4.2146e-01,  5.7142e-01, -6.2862e-01,  1.6844e+00,\n",
      "         -4.0412e-01, -1.6352e+00, -1.0059e+00,  6.9011e-04, -1.9917e+00,\n",
      "         -1.4201e+00, -1.0875e+00, -2.0102e+00, -1.1761e+00,  2.2165e-01,\n",
      "         -1.2756e+00,  1.9618e+00, -1.6436e+00,  9.9192e-01, -2.2536e+00,\n",
      "         -3.3507e-01, -1.3871e+00, -1.5678e+00, -1.6249e+00,  1.2603e+00,\n",
      "         -1.0253e+00,  3.1168e-01,  1.2904e+00, -1.4584e+00, -1.4469e-01]])\n"
     ]
    }
   ],
   "source": [
    "W_OUT = 100\n",
    "\n",
    "# (6,100)\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN)\n",
    "# (1,100)\n",
    "    # 1 bias per neuron (per column)\n",
    "bias = torch.randn(W_OUT, generator=GEN)\n",
    "print(\"Bias:\", bias, bias.shape)\n",
    "\n",
    "\n",
    "# bias = torch.randn()\n",
    "print(\"Weights:\", W)\n",
    "\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "# h for hidden layer\n",
    "h = emb_view @ W\n",
    "\n",
    "# why this works: bias is (100,)\n",
    "# broadcast: \n",
    "    # h: (32, 100)\n",
    "    # b: (  , 100) -> (1,100), 1 is inferred\n",
    "    # so it copies bias and adds to each row as expected - each column gets a different bias value, for each neuron\n",
    "h = h + bias\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [6.8365e-14, 4.1561e-10, 2.0476e-05, 1.9685e-01, 3.6644e-09, 2.5611e-04,\n",
       "         2.3277e-09, 4.3530e-05, 4.4425e-05, 1.3900e-09, 2.9489e-05, 2.1639e-03,\n",
       "         8.2406e-09, 9.1830e-03, 5.0304e-05, 1.9712e-07, 1.0256e-03, 6.0565e-08,\n",
       "         1.5621e-02, 1.3701e-06, 3.1891e-06, 1.0589e-05, 4.8030e-07, 2.0710e-04,\n",
       "         2.5028e-02, 6.3929e-02, 6.8553e-01],\n",
       "        [1.3785e-10, 7.1116e-05, 1.9685e-11, 3.7020e-08, 3.8112e-07, 6.1939e-05,\n",
       "         1.0589e-03, 2.4612e-02, 5.9960e-05, 8.2954e-06, 2.8608e-03, 1.1369e-06,\n",
       "         2.0626e-08, 1.2665e-06, 1.3825e-03, 6.0687e-05, 5.0792e-06, 2.9640e-02,\n",
       "         2.6060e-12, 1.0480e-08, 8.6222e-06, 5.8362e-08, 3.8414e-02, 6.9616e-01,\n",
       "         1.3612e-06, 2.0560e-01, 4.1925e-08],\n",
       "        [1.1296e-08, 9.9169e-01, 8.2763e-05, 4.7307e-08, 5.2716e-07, 1.1432e-07,\n",
       "         4.5556e-08, 1.5861e-03, 2.5982e-06, 3.0303e-08, 4.2567e-05, 4.3100e-10,\n",
       "         4.2078e-09, 4.7673e-07, 1.3123e-06, 3.5119e-03, 1.8188e-09, 9.2137e-10,\n",
       "         1.0143e-03, 5.2248e-10, 1.5106e-04, 5.9120e-09, 2.2771e-06, 1.9160e-03,\n",
       "         6.2971e-09, 5.7147e-09, 1.9878e-06],\n",
       "        [2.4788e-08, 4.1117e-05, 1.1683e-08, 8.8031e-05, 3.0312e-08, 1.8094e-03,\n",
       "         1.0015e-07, 1.4834e-08, 9.9315e-11, 1.4141e-08, 5.7513e-06, 4.2593e-04,\n",
       "         1.1685e-07, 2.8681e-01, 9.1562e-07, 3.4057e-02, 3.5671e-09, 1.1302e-13,\n",
       "         6.4183e-06, 6.1721e-08, 7.6164e-03, 7.1072e-07, 7.6732e-09, 6.6902e-01,\n",
       "         1.1124e-04, 6.7153e-12, 1.0122e-05],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [1.3228e-10, 5.2603e-01, 6.8273e-09, 7.1021e-09, 1.1829e-07, 8.2693e-05,\n",
       "         7.0980e-05, 1.4303e-01, 4.5650e-06, 1.5805e-07, 7.8844e-03, 1.0340e-06,\n",
       "         8.8627e-08, 4.5004e-07, 4.7476e-03, 6.4546e-07, 3.2316e-07, 8.9055e-03,\n",
       "         1.6497e-09, 3.4401e-08, 1.4834e-07, 2.0662e-07, 2.8947e-03, 1.7122e-01,\n",
       "         8.3633e-07, 1.3513e-01, 6.4751e-07],\n",
       "        [1.2745e-10, 1.0000e+00, 1.6020e-13, 1.8645e-10, 5.1141e-12, 4.2361e-10,\n",
       "         1.4395e-13, 2.1033e-07, 6.0098e-10, 1.2448e-12, 7.6635e-11, 1.1237e-09,\n",
       "         1.3887e-10, 3.6748e-09, 2.8929e-10, 9.9027e-09, 4.6133e-13, 1.2200e-15,\n",
       "         2.7734e-08, 2.8084e-12, 1.5415e-08, 3.5499e-10, 2.3313e-09, 4.0335e-07,\n",
       "         4.8197e-09, 3.6785e-10, 4.0072e-07],\n",
       "        [1.5062e-10, 7.9447e-01, 6.8998e-12, 3.8265e-08, 6.0892e-12, 1.4840e-01,\n",
       "         1.6850e-08, 2.2217e-09, 7.1332e-12, 1.0418e-03, 6.1412e-04, 3.0613e-04,\n",
       "         2.4696e-10, 2.5273e-03, 7.4339e-04, 7.8838e-04, 2.7189e-09, 2.0782e-17,\n",
       "         2.9269e-08, 3.4234e-11, 5.7498e-07, 8.5817e-07, 1.0154e-07, 5.1110e-02,\n",
       "         3.4165e-10, 1.5504e-12, 9.0301e-09],\n",
       "        [9.9902e-10, 1.1785e-09, 3.0751e-11, 3.6634e-06, 2.3274e-05, 2.2613e-12,\n",
       "         8.7994e-07, 6.4068e-09, 9.5297e-13, 3.0796e-10, 1.9769e-11, 3.4849e-10,\n",
       "         4.9741e-11, 5.3920e-11, 2.3951e-08, 2.5569e-09, 5.8760e-14, 5.1701e-12,\n",
       "         6.6037e-09, 5.7791e-12, 9.9997e-01, 9.0664e-11, 2.0022e-11, 1.0661e-06,\n",
       "         1.4851e-11, 1.8723e-14, 5.8557e-14],\n",
       "        [1.1216e-13, 1.3773e-04, 2.3751e-10, 4.4798e-06, 1.0092e-08, 4.3905e-05,\n",
       "         9.5929e-10, 2.1847e-09, 9.9114e-11, 1.4299e-09, 9.3408e-08, 1.6257e-09,\n",
       "         5.2884e-12, 6.7871e-09, 2.2369e-06, 3.5295e-08, 2.5777e-08, 7.6389e-13,\n",
       "         9.9981e-01, 1.1991e-09, 6.5274e-09, 2.5702e-10, 3.7675e-09, 1.0881e-07,\n",
       "         1.2617e-08, 4.5884e-13, 8.0979e-09],\n",
       "        [1.3089e-11, 7.7038e-09, 5.3127e-08, 4.4668e-05, 5.2548e-07, 7.5878e-04,\n",
       "         1.7066e-07, 1.0781e-07, 7.4577e-06, 9.0856e-09, 7.0690e-08, 1.3405e-01,\n",
       "         1.4165e-05, 6.1325e-04, 3.4102e-01, 2.6888e-07, 2.6087e-07, 2.2872e-07,\n",
       "         8.1178e-08, 7.1988e-07, 5.2155e-01, 1.1549e-09, 1.6845e-04, 6.5212e-05,\n",
       "         5.1928e-05, 6.6453e-08, 1.6523e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [3.0727e-11, 7.0382e-08, 9.0770e-05, 2.8462e-01, 4.4219e-03, 5.0848e-03,\n",
       "         2.2958e-04, 1.3853e-02, 5.1958e-02, 5.3968e-07, 1.6313e-04, 6.4986e-02,\n",
       "         6.8246e-08, 3.2213e-03, 3.9932e-02, 3.9735e-05, 2.0214e-04, 7.5298e-05,\n",
       "         2.0479e-02, 1.0849e-05, 1.3969e-01, 4.4869e-05, 3.1109e-05, 1.6134e-01,\n",
       "         1.0194e-01, 4.7200e-03, 1.0287e-01],\n",
       "        [7.1443e-10, 8.3208e-06, 1.3397e-08, 1.1205e-06, 7.6466e-01, 1.5721e-07,\n",
       "         1.3244e-01, 1.1216e-03, 1.5530e-04, 9.2832e-08, 4.5542e-10, 3.3853e-07,\n",
       "         3.4060e-08, 5.6231e-07, 1.9254e-04, 2.3368e-07, 2.6900e-08, 2.8312e-04,\n",
       "         3.6823e-07, 1.0882e-07, 8.5466e-04, 3.7873e-08, 2.7222e-08, 1.0012e-01,\n",
       "         1.5806e-04, 1.8494e-08, 1.4281e-06],\n",
       "        [6.5291e-14, 1.5102e-07, 1.1729e-04, 5.7639e-02, 1.9839e-04, 3.0956e-04,\n",
       "         7.3039e-06, 6.2358e-05, 3.8526e-06, 3.4572e-09, 7.1664e-06, 7.9779e-06,\n",
       "         8.5429e-10, 5.1024e-06, 1.1610e-04, 1.6946e-04, 2.3829e-04, 1.8623e-07,\n",
       "         8.3640e-01, 2.6639e-08, 7.1354e-06, 5.8141e-07, 2.0660e-09, 9.9714e-02,\n",
       "         2.7150e-03, 1.2070e-06, 2.2771e-03],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [5.2470e-11, 7.6938e-03, 1.0777e-05, 5.0911e-03, 1.0572e-06, 3.5413e-03,\n",
       "         6.2156e-06, 1.4944e-01, 1.4113e-01, 1.6016e-07, 2.7195e-02, 4.4333e-02,\n",
       "         2.4544e-03, 8.7723e-03, 4.1838e-03, 1.5251e-05, 1.1605e-03, 5.5702e-05,\n",
       "         6.4435e-04, 7.8044e-04, 9.9097e-06, 7.1613e-03, 1.7232e-01, 7.4297e-03,\n",
       "         1.7301e-01, 2.0271e-01, 4.0855e-02],\n",
       "        [4.2015e-10, 1.6522e-02, 1.3572e-10, 1.3594e-07, 6.9696e-03, 1.1380e-06,\n",
       "         4.4457e-07, 6.5977e-04, 1.6107e-03, 2.7494e-08, 3.9008e-07, 1.2763e-05,\n",
       "         1.0397e-07, 7.4413e-08, 1.6280e-02, 1.5473e-02, 1.3636e-07, 1.3538e-06,\n",
       "         1.4636e-08, 2.6405e-07, 2.7999e-01, 2.0693e-08, 1.5857e-03, 6.6085e-01,\n",
       "         2.5812e-06, 3.1863e-06, 4.2912e-05],\n",
       "        [1.9791e-11, 2.0595e-06, 9.9719e-03, 9.5977e-04, 2.9245e-04, 1.4362e-04,\n",
       "         5.3716e-08, 2.1452e-06, 7.4617e-09, 7.7082e-08, 1.6292e-04, 7.7487e-07,\n",
       "         1.6527e-10, 6.1820e-04, 7.4312e-06, 5.2019e-04, 3.0618e-06, 1.2894e-13,\n",
       "         8.4620e-01, 1.9118e-07, 1.0804e-03, 2.0602e-08, 1.7234e-09, 1.3877e-01,\n",
       "         1.2669e-03, 4.8764e-13, 2.6270e-08],\n",
       "        [3.8494e-13, 7.3627e-01, 1.5682e-08, 6.9540e-09, 1.6663e-13, 4.1843e-04,\n",
       "         2.2122e-04, 3.6886e-07, 4.4483e-06, 3.9973e-03, 1.1547e-05, 1.0779e-02,\n",
       "         2.1516e-04, 2.0246e-04, 4.0275e-04, 4.2318e-07, 1.5429e-07, 3.6351e-07,\n",
       "         1.0134e-12, 2.1719e-07, 4.3730e-10, 1.8890e-05, 9.8278e-02, 4.5051e-05,\n",
       "         2.2433e-11, 1.4914e-01, 4.5380e-07],\n",
       "        [3.4293e-06, 2.5420e-09, 7.8812e-06, 1.4604e-03, 5.9348e-10, 1.0571e-05,\n",
       "         4.2870e-12, 1.2451e-07, 1.5078e-08, 5.8704e-12, 3.9242e-09, 2.5691e-07,\n",
       "         5.6767e-10, 4.0119e-05, 3.8598e-05, 7.5505e-05, 8.8371e-11, 1.2825e-14,\n",
       "         2.6853e-05, 5.5664e-10, 2.1989e-02, 6.9340e-09, 2.2668e-06, 2.2171e-07,\n",
       "         1.0763e-04, 1.4859e-05, 9.7622e-01],\n",
       "        [9.5699e-08, 2.0733e-04, 3.2152e-09, 2.9141e-05, 3.3797e-10, 2.8711e-04,\n",
       "         8.9165e-01, 6.9198e-11, 4.6177e-12, 1.0739e-01, 8.8537e-05, 8.1354e-05,\n",
       "         4.6162e-08, 1.0546e-06, 7.2800e-07, 3.1520e-09, 1.0961e-10, 2.2440e-09,\n",
       "         2.5469e-12, 1.2000e-09, 3.3605e-07, 7.7052e-08, 3.1465e-06, 2.6781e-04,\n",
       "         3.4917e-11, 4.7030e-12, 2.6047e-12],\n",
       "        [2.8478e-08, 6.6653e-01, 1.0830e-05, 1.4612e-08, 1.2338e-08, 5.5271e-07,\n",
       "         3.3243e-08, 2.8572e-01, 8.4457e-04, 3.5494e-08, 1.8938e-04, 6.2184e-10,\n",
       "         8.0545e-09, 3.9368e-08, 1.7321e-04, 3.9506e-02, 6.0118e-07, 1.4032e-10,\n",
       "         1.8904e-04, 6.0291e-09, 8.0527e-05, 2.7112e-06, 9.3329e-05, 6.6551e-03,\n",
       "         4.2669e-08, 7.3339e-08, 7.3088e-07],\n",
       "        [7.5847e-06, 1.7647e-05, 3.9970e-07, 2.8866e-05, 1.8172e-08, 4.0296e-03,\n",
       "         5.9501e-07, 3.4804e-09, 1.2925e-11, 2.8633e-07, 2.7979e-04, 5.0431e-06,\n",
       "         7.6742e-09, 5.2283e-02, 2.0955e-04, 8.9330e-02, 1.8862e-09, 5.8623e-14,\n",
       "         2.1015e-06, 8.6655e-09, 2.9872e-01, 7.3246e-08, 1.0754e-07, 5.5508e-01,\n",
       "         6.8259e-07, 3.2069e-11, 8.3104e-09],\n",
       "        [1.8066e-10, 7.5123e-07, 1.4455e-05, 6.0647e-03, 4.7260e-01, 1.8111e-04,\n",
       "         3.2319e-02, 6.2554e-03, 5.6768e-03, 1.6017e-06, 2.3496e-07, 4.4994e-04,\n",
       "         6.0862e-08, 3.1595e-05, 3.9317e-02, 6.1128e-06, 9.8764e-06, 1.4791e-04,\n",
       "         1.1991e-03, 4.1817e-07, 1.5482e-02, 3.8828e-06, 6.5541e-06, 3.8989e-01,\n",
       "         3.0132e-02, 1.2833e-06, 2.0421e-04],\n",
       "        [8.1452e-10, 1.8016e-04, 6.5544e-08, 3.9486e-07, 1.7942e-03, 1.1060e-06,\n",
       "         2.1602e-04, 3.6675e-04, 6.3153e-06, 4.7813e-09, 2.2747e-06, 1.1809e-07,\n",
       "         4.0151e-08, 6.1695e-07, 7.7422e-05, 2.8504e-07, 1.5279e-07, 1.3777e-05,\n",
       "         1.0753e-07, 1.9894e-08, 5.9891e-06, 1.0691e-07, 8.8311e-07, 9.9588e-01,\n",
       "         1.4456e-03, 1.2169e-07, 1.1007e-05],\n",
       "        [7.7901e-12, 9.9992e-01, 6.1512e-12, 2.5135e-12, 1.2922e-09, 9.6660e-09,\n",
       "         1.6270e-10, 2.6213e-05, 5.4496e-08, 1.1987e-11, 2.1822e-06, 2.0637e-08,\n",
       "         1.6035e-09, 5.7746e-11, 1.4566e-08, 4.4576e-08, 2.2200e-10, 1.5848e-08,\n",
       "         1.6301e-09, 1.3031e-10, 1.2187e-08, 2.6812e-10, 5.4765e-09, 1.4868e-05,\n",
       "         5.2247e-10, 3.7397e-05, 1.2600e-06],\n",
       "        [5.7278e-12, 2.3306e-14, 7.1772e-07, 7.7967e-08, 3.1102e-05, 1.2452e-07,\n",
       "         6.8755e-11, 4.1059e-11, 1.9512e-09, 2.4815e-14, 1.4978e-14, 4.9167e-08,\n",
       "         1.4188e-12, 5.8159e-06, 1.3131e-04, 4.6438e-07, 7.4308e-13, 1.4926e-12,\n",
       "         3.6095e-06, 2.8189e-13, 9.9983e-01, 1.9827e-13, 8.4867e-13, 1.6814e-06,\n",
       "         3.0432e-10, 2.2336e-14, 3.1839e-08],\n",
       "        [2.0404e-10, 2.5702e-08, 1.2095e-08, 5.2292e-04, 3.6576e-11, 8.6851e-06,\n",
       "         9.9664e-01, 1.1695e-10, 3.9258e-14, 1.3928e-07, 4.2754e-07, 1.4162e-04,\n",
       "         6.5149e-10, 2.6736e-03, 7.3567e-09, 2.9535e-12, 1.2528e-07, 7.2461e-06,\n",
       "         3.3744e-11, 1.2678e-11, 1.5350e-08, 6.9142e-12, 5.0439e-12, 1.1578e-06,\n",
       "         2.3050e-11, 2.3835e-08, 6.5479e-14],\n",
       "        [8.7365e-15, 5.7650e-10, 5.0590e-03, 2.5860e-06, 9.1061e-07, 2.7300e-08,\n",
       "         6.1033e-05, 1.9516e-02, 3.6828e-02, 1.0858e-08, 3.8272e-09, 3.0933e-06,\n",
       "         1.9118e-12, 2.9697e-09, 8.6193e-06, 2.6454e-07, 2.5217e-03, 1.0188e-02,\n",
       "         7.1391e-08, 1.6662e-06, 2.4008e-08, 7.3626e-10, 1.2556e-04, 1.7289e-06,\n",
       "         7.3063e-01, 1.9229e-01, 2.7674e-03],\n",
       "        [1.1174e-09, 1.1508e-09, 3.5377e-05, 6.8658e-05, 1.1534e-04, 3.6285e-09,\n",
       "         4.2690e-08, 9.5894e-01, 1.8759e-03, 6.1524e-11, 4.6356e-08, 2.2982e-11,\n",
       "         5.4032e-12, 1.4064e-05, 1.7364e-02, 7.6190e-06, 5.5600e-07, 2.6349e-10,\n",
       "         5.0398e-04, 3.2318e-08, 2.0003e-02, 7.7636e-10, 1.1831e-07, 2.9509e-04,\n",
       "         7.5410e-04, 1.9720e-06, 2.1938e-05]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "res = h @ W2 + b2\n",
    "probs = res.softmax(dim=1)\n",
    "\n",
    "# ach row is normalized\n",
    "print(probs.sum(dim=1))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corresponding probabilities for each y label, get NLL Loss\n",
    "torch.arange(m) -> 0,1,2,3...m-1 for row indices\n",
    "Y: e.g [0,5,13,13,6..] -> actual labels. so we end up getting\n",
    "- res[0,0], res[1,5], res[2,13] etc in a vector\n",
    "- This is the predicted probability based on current weights for that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8111e-04, 9.1830e-03, 1.2665e-06, 9.9169e-01, 2.4788e-08, 6.1128e-06,\n",
       "        8.8627e-08, 1.2448e-12, 1.0154e-07, 3.0796e-10, 1.3773e-04, 1.3089e-11,\n",
       "        7.5123e-07, 3.1109e-05, 8.3208e-06, 6.5291e-14, 1.6017e-06, 7.8044e-04,\n",
       "        1.6522e-02, 9.9719e-03, 4.1843e-04, 5.6767e-10, 4.6162e-08, 6.6653e-01,\n",
       "        7.5847e-06, 4.1817e-07, 2.8504e-07, 2.2200e-10, 1.9512e-09, 1.3928e-07,\n",
       "        5.7650e-10, 1.1174e-09])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(Y)\n",
    "probs[torch.arange(m), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5532)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = -probs.log().mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--SETUP--#\n",
    "\n",
    "# Generator\n",
    "GEN = torch.Generator().manual_seed(2147483647)\n",
    "# Number of examples\n",
    "M = len(Xtr) \n",
    "\n",
    "# Params: C, W, bias, W2, b2\n",
    "# Lookup table\n",
    "    # 27 rows: one for each unique char\n",
    "    # 2 cols: m=2 embeddings vectors.\n",
    "C = torch.randn(27, EMBEDDING_SIZE, generator=GEN)\n",
    "\n",
    "# Forward pass\n",
    "W_OUT = 300\n",
    "\n",
    "# First layer\n",
    "W = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE), W_OUT, generator=GEN) # (6,100)\n",
    "bias = torch.randn(W_OUT, generator=GEN) # (1,100): 1 bias per neuron (per column)\n",
    "\n",
    "# Second layer: W_OUT inputs, 27 outputs for log-counts (to become probabilities)\n",
    "W2 = torch.randn((W_OUT, 27), generator=GEN)\n",
    "b2 = torch.randn(27, generator=GEN)\n",
    "\n",
    "W3 = torch.randn((BLOCK_SIZE * EMBEDDING_SIZE, 27), generator=GEN)\n",
    "\n",
    "parameters = [C, W, bias, W2, W3, b2]\n",
    "\n",
    "for param in parameters:\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3049089908599854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c6511c0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSWElEQVR4nO3deXgT1RoG8Ddd0gXoAqUtlEJZBASEQoFSkEUpgiu4VkXBXkVBULTqFVxAQSlXEXFBUBB3BUVxA0EsO5atUHYKBUrL0g3oTtfM/aM0JG2SZpJJZpK8v+fp80AymTmdJplvzvnOd1SCIAggIiIikomb3A0gIiIi18ZghIiIiGTFYISIiIhkxWCEiIiIZMVghIiIiGTFYISIiIhkxWCEiIiIZMVghIiIiGTlIXcDzKHRaHD+/Hk0a9YMKpVK7uYQERGRGQRBQHFxMVq3bg03N+P9Hw4RjJw/fx7h4eFyN4OIiIgskJWVhTZt2hh93iGCkWbNmgGo/WX8/Pxkbg0RERGZo6ioCOHh4drruDEOEYzUDc34+fkxGCEiInIwjaVYMIGViIiIZGVRMLJw4UJERETA29sb0dHR2LVrl8ntCwoKMHnyZLRq1QpeXl7o3Lkz1qxZY1GDiYiIyLmIHqZZsWIFEhISsHjxYkRHR2PBggUYOXIk0tLSEBwc3GD7yspKjBgxAsHBwVi5ciXCwsJw5swZBAQESNF+IiIicnAqQRAEMS+Ijo5Gv3798PHHHwOonXYbHh6OZ555BtOmTWuw/eLFi/Huu+/i2LFj8PT0tKiRRUVF8Pf3R2FhIXNGiIiIHIS5129RwzSVlZVISUlBbGzstR24uSE2NhbJyckGX/P7778jJiYGkydPRkhICHr06IE5c+agpqbG6HEqKipQVFSk90NERETOSVQwkp+fj5qaGoSEhOg9HhISguzsbIOvOXXqFFauXImamhqsWbMGr7/+Ot577z289dZbRo+TmJgIf39/7Q9rjBARETkvm8+m0Wg0CA4OxmeffYaoqCjExcXh1VdfxeLFi42+Zvr06SgsLNT+ZGVl2bqZREREJBNRCaxBQUFwd3dHTk6O3uM5OTkIDQ01+JpWrVrB09MT7u7u2seuv/56ZGdno7KyEmq1usFrvLy84OXlJaZpRERE5KBE9Yyo1WpERUUhKSlJ+5hGo0FSUhJiYmIMvmbQoEFIT0+HRqPRPnb8+HG0atXKYCBCRERErkX0ME1CQgKWLFmCr776CkePHsWkSZNQWlqK+Ph4AMC4ceMwffp07faTJk3CpUuXMHXqVBw/fhyrV6/GnDlzMHnyZOl+CyIiInJYouuMxMXFIS8vDzNmzEB2djYiIyOxdu1abVJrZmam3sp84eHhWLduHZ5//nn07NkTYWFhmDp1Kl5++WXpfgsiIiJyWKLrjMiBdUaIiIgcj03qjBA15nR+KT7bchJlldVyN4WIiBwEgxGFWbr1FG6etwk5ReVyN8UiN83bhDlrjmHeuuNyN4WIiBwEgxGFeWv1UZzKL8W8dWlyN8Uqe85ckrsJRETkIBiMKFS1RvGpPERERJJgMGKAIAg4mVcCDQMCIiIim2MwYsAHSScw/L3NeHvNUbmbQkRE5PQYjBiw4J8TAIDPt52WuSWW++rfDCzflSl3M4iIiBoluugZKV9uUTlm/n4YAHBvVBt4ujPmJCIi5eJVygmVVFyr8aH8knZEROTqGIwQERGRrBiMEBERkawYjMgor7gCT3y1BxuP5crdFCIiItkwGJHRW6uP4J+jOYj/crek+2WaCBERORIGIzKqv/7M+YIrkh9DpbL8tQVllSivqpGuMURERAYwGFGQF37cL3cTtArLqhA5az2iZq+XuylEROTkGIwoyDkb9IxY6sC5AgBAaSV7RoiIyLYYjEikpKKaa9noYH0TIiIyF4MRCVwovIIeM9fhgU+T5W4KAP1AQGzKSHlVDT7dfBLpuSX19ilg2bbT+Dc93/oGEhER6WA5eAn8uf8CAGDPmcsyt8R6H204gYUbTzZ4fFt6Pmb9eQQAkDH3dns3i4iInBh7Ruwov6QCiWuO4mReSeMby2RfZoHBxzMvldm3IURE5DIYjNjRCz/ux6dbTuGOD7fJ3RQiIiLFYDBiR/sya4dxrti4dse/J5nXQUREjoPBiIIIEtROFQQBM347rP2/ypqqZ0RERHbABFYrLdyYjnfXpcndDC3WBSEiIkfDnhErKSkQEUNgIRAiIlIIBiN2VFav10IlugqIND7ZlI7+c5KQeZEzZIiISH4MRuzk3/R8VCukQus7a9OQV1yBuWuPNnhOTIrJybwSfPVvBiqrNRK2joiIXA1zRuxk7tpjev9/4qvd2JVxSe8x3ZGTGoUELqYMf28zAKC0shpPD+skc2uIiMhRMRiRyT9Hcxs8phuAnLlYas/mWMVYoTQiIiJzcJjGDl5eeQAHzhY2uh1zSomIyBUxGLExQRCwYk+W3M0gIiJSLAYjLqBGI2DriTwUXqky+PyOUxeRsCIVl0orJTumFAXciIjINTAYkdisP45Y9Lo9GZeQXVSu/f/+s4V460/L9lXfV/9m4NHPd+GeT7YbfP7Bz3bgl33nMPvPI7JNNyYiItfFYARAaUU1cnQCAWss234aS7acQnZh7f5SswrMet19i5MbPLZ022lJ2vRb6jkAwMk800mxWRauzHsipxgaB5j9Q0REysRgBEDv2esRPSdJG0CY48zFUsz47ZDB595ecxQPflYbXHy6+ZQkbazvu51n8NW/Gdh1+pLs1VQzLpZhzpqGNUuIiIjMwWAE0BbtSjlz2ezXPLxkJ75OPmP0+QwbVjfNulSGV1cdwszfD+OBT5Ox6Xie0W3Tc0ts1g5dUvXiEBGR62EwYqFzBVfserxLpZUQBAE1GgF3fLRN77nNacaDkZELtuB4jvQBiZTJrkRE5NoYjDSiuLwKGo2AU3klsg2H7Dh1EX1mr8eU7/dh5+mLRmfFGHOlSvqVfKPeWi/5PomIyDUxGGnEQ0t2YPbqI7j5vc1YuDFd9OulmOK6ePNJAMDqgxckLRNfP7bKuFiGben5BretP8uGBdqIiEgqDEYacehcEb7YngEAmPf3cVGvjZi2GluOG76420pjE3PfqbdGjq78kgoJ21E/eGH0QkREhjEY0WFOL0ZRufxDJNb4ZNNJi15nSQ9PblE5covKsXBjOgbN3SBqthIREbkOBiMimepZkFONRsDKlLPIUMgCe1U1GvSfk4T+c5Lw7ro0nC8sx/z1aXI3i+qpqtHI3QQiIq7aK5a9psqKIQgCVuzOwiurDsrdFK2SiuoGj7EumrJM/+UAftpzFhtfHIbw5r5yN4eIXBh7RiwgdjaLPezOuCR3E8jB/LArC9UaAZ+zRgwRyYzBiAXe+P2w3E1oQAkJonXF44iIiMTgME09J3KKG91m9cELdmjJNY3FGV+ZqARrikriNfGMlccnIiIyhT0jOgQBGPH+FpPb7Dh1ya49AAVl8lc6Hbdsl15AZKwXZvnuLDu1iIiInAmDEYWLnCV/pdMtx/NwWaf8e4UZwZgCRo2IiMhBMBghs+gO6fS2MEBigEJERIa4fDBSLLKIGSmvkBsRETk2lw5GajQCbnjjb7mbQURE5NJcOhgprdQvzPX9zkyZWmK+kvKGxcQsZcthk3MFV2y3cyIiciouHYzUl3zqotxNMEh3pd4/D9h3WnEd5nsQEZGtMBhxANvSdVb+lbA2yF+HsqXbGRERkYVcOhiRuOaXU9t8PE/uJhARkZNy6WDEIUk8XGJuGfk9Zy5Le2AiIqKrXDoYUUldD90Oko7lSLq/X/aek3R/pghSR1JEROQUXDoYcUTlVdKWot9ygsMvREQkL5cORrbrJoa6qN9Sz8vdBCIicnEWBSMLFy5EREQEvL29ER0djV27dhnd9ssvv4RKpdL78fb2trjBUnrqmxS5m0BEDqSsshopZy5Bo+GQI5GURAcjK1asQEJCAmbOnIm9e/eiV69eGDlyJHJzc42+xs/PDxcuXND+nDlj2ZL35Niqauz7BZ5fUmF2gq5YqVkFmPHbIRSWcTkBV/LQkp24d1EyvtvJ7zAiKYkORubPn48JEyYgPj4e3bp1w+LFi+Hr64tly5YZfY1KpUJoaKj2JyQkxKpGk2P6Y7/9hoQ2peWi71v/4LkVqTbZ/5iF2/F18hnM+vOITfZPyrQ/qwAA8OOes/I2hMjJiApGKisrkZKSgtjY2Gs7cHNDbGwskpOTjb6upKQE7dq1Q3h4OEaPHo3Dhw+bPE5FRQWKior0fojEWLgxHYDtc2LS80psun97sFXvERGRuUQFI/n5+aipqWnQsxESEoLsbMPVPLt06YJly5bht99+w7fffguNRoOBAwfi7FnjdxaJiYnw9/fX/oSHh4tpJhERETkQm8+miYmJwbhx4xAZGYmhQ4fil19+QcuWLfHpp58afc306dNRWFio/cnKyrJ1M4mInMqZi6U45QQ9d+QaPMRsHBQUBHd3d+Tk6BfeysnJQWhoqFn78PT0RO/evZGenm50Gy8vL3h5eYlpGhERXVVVo8HQdzcBAI7OGgUftbu8DWpEUXkV9mUWYFDHFvBwd+mKEy5L1F9drVYjKioKSUlJ2sc0Gg2SkpIQExNj1j5qampw8OBBtGrVSlxLiYjILFeqarT/Lryi/BlfcZ/uwPhlu/DpllNyN4VkIjoETUhIwJIlS/DVV1/h6NGjmDRpEkpLSxEfHw8AGDduHKZPn67dftasWfj7779x6tQp7N27F4888gjOnDmDJ554QrrfgoiIHNbRC7WTFH7dZ7/lKSxRWa1BfkmF3M1wSqKGaQAgLi4OeXl5mDFjBrKzsxEZGYm1a9dqk1ozMzPh5nYtxrl8+TImTJiA7OxsBAYGIioqCv/++y+6desm3W9B5CDSc4tx8FwhxkSGOeTaSESubNSCLTiVX4pNLw5DRFATyfcvCIJF3wv/HMmBn48n+rdvLnmb7EV0MAIAU6ZMwZQpUww+t2nTJr3/v//++3j//fctOQyRJKpqNPBUyDh07PwtAABvD3fcegOHKokcyan8UgDA+iM5mDCkg6T7Liqvwm0fbMXwrsF4c3QPs193vuAKnvh6DwAgY+7tkrbJnpTxDU1kQ31mr0dJRbXczdBz4Fyhxa+9VFqJvGJ2FRM5kxW7snD28hV8lSyuum+uhd8FS7eewqC5G5B1qcyi10uNwQg5veLyamw8Zny5Akei0QjoM3s9+r39D8p1khSJjBHAonaOwN5/p7dWH8W5giuY+9cxux7XGAYj5JSctahoZY1G++/cIvaOkGHMRiJzVWs0jW9kBwxGiIjIJuatS8Obf5he/oOs4yyBJ4MRUoSyymrJhh00GgHnC65Isi8iskxltQYfb0zHF9szcPayMvISSLkYjJDsKqpr0G3GOkTO+luSRdueW5GK84XlErSMyLSyymqUVSorOVopdHMgqmpq/11SUY3KamUMCxjDhSPlwWCEZJd1qbYXo7xKgxqN9V8Ev++37Uq9SuEqiYnF5VVYtOkkMi8q6+66ukaDbjPWoduMdaiuUfYFVgmKy6vQY+Y6DPrfBrmbYtSBswWInpOEVfuML+SqNNaWK1JK7MVghOzq3/R8uZsAoHYoZ/2RHOQUOVYPiivWSZv1xxH8b+0x3PrBFov3kV1YLukdrwABReXXekQKFFxyXSkXm4NXp7MreVr65O/3Ire4As+v2C93U1wOgxGyq4eX7pS7CQCAlXvPYsLXezD4nY2yHN8FYwqLJZ+6CAAorbQsp+jbHWcwIDEJb68+KmWzGpVbVI7LpZV2PWYdVve1THWNQiI3F8RghFzS5rQ8ALDL+PVvqefw5h+HoZFgCIrEqwtClm47bbdjllRUo/+cJPSevd5uxyTXpHKSWxuLysET2YozXq6nLk8FAES3byHpfpXS/W5rjvh7Km32yOXSSgQ2UcvdDCKj2DNCsnOVHuVLEnTZS3EX9HPKWcz87ZDV+yHHsGrfOfSevR7vrFVGpU1THDDulJ2zfH8yGCGHYG7y4TM/7LNxSxzfCz/t11v/gheAxmk0gt2Wjs8tKsf6IzmSDeu9uy4NAPDJppOS7I9sQ6oewOeW78MDi5P13j+n80vxxu+HDdZfUsrnn8EIKd7v+8+jz+z12HE1kdGUP5xwWu+JnGL8lnqO9Q9kNPn7vej71j9Gn5fyTzPk3Y2Y8PUe/JSSJd1OzbT1RB4OnC2wyb6lmLZva+Z2MuhOq88tKlfUOlG/pp7HroxLOHT+2mKc9y9Oxpf/ZuCpb1JkbJlpDEZIUQx9qT/7wz5cLqvCY1/ssn+DFGDE+1swdXkqko46x2J/juivQ9kNHrNV73h5VW1S9ebjeRbvw5K2nS+4gkc/34W7Pt5u8XFN+eCfEzbZryEajYC3/jyCNQcv2GT/pRW1wceZi6XoPycJw97dZJPjWEP3u7SuV++gFauF2xqDEZKd2XcjEt5Y6d7ZfL8zEw9+loyjF4rw+/7zir2D073TAZTTvWprtugRyi0uR5UTFCoTBEGygmu2XkJhz5nLNt2/rjWHLmDpttN4+ru9Ntn/B0m1gVXdDUK2jPWKmDNC5CReWXUQO05dwq0fbMWzP+zDit3iu8d/338ev6We0/5//ZEc3PHRVqTnFhvc3tIvEGu/eJzhAmyttOxi9H87CXd+tM1mxxCbaLwpLRePLN0pehbOg5/tQPScJFyxsAaLs+KK1o6HwQgpihJKnCebkZuiq6yyGs/+sA9Tl6eiuLy2EueEr/fg0LkiPPNDqnY7Jfxuqw/YptvaWvuzCvDc8n12SbD7fX9t0Hgs23CgKIfHvtiNben5ePnnA6Jet/P0JVwsrcTujEsQBEGyoGSLFUNE5FiUkorGYIRkkVtcjlN5JQAaVov8ZscZPLxkB0oqlLUAmUYjID23WH/YQBC0Y/xA7Viy7qyLuuBECmK/NP4+nI1/juToPWarcyoItefG0hkgoxdux6+p5/HcilRRr0vNKsBFO81ysYf8Ysunf4//Yjeun7EW2Y0sErk/qwCTvk0xudbPe+uPW9wOa2UXluORpTuxvt571x4csXKt9dP9lRGNMBghWfR/Owk3v7cZuUXleh8lQQBe//UQ/j15ET1mrjM6zCGWFB+3WX8eQez8LXjfxBf17R9uNTnrwlr658r4b1VUXoUnv0nBE1/vEZXp/82OM1iy5ZTodn2y6SRi52/BayLrlwiCgD0Zl7T/P51favZrd2dcwpiF2xE9J0nUMW1BCb1edb0Zv+oMFxoyeuF2/HUoGxO/tc3MisPnC5FmRa/TjN8OYVt6PiZ8vUfCVinDwbOFGP7epgY3CcRghGR2PKfE5POx8y1fHE1qX/6bAQD4cEO60W0u2ngtEt07N1OXv1KdHhBz80SqajR4/ddDeHvNUeSKTMh77+/aWhbf78w0+zWvrjqIG/+3EfctTja5nbGYq+7iWy1TwrG5N9FyTMk2p21nLpof+JmruLwKt3+4DSMXbIHGwvQkW3+G5PT4V7txMq8UT9go0NJ9r2kEAalZBQ6TJ8ZghBzethP5Np8JYIo1F5tPN5/E0Hc32m31YFMt1ej8HlfsUDfhu52ZOCfj381WrlTWYLdOb4+jkCJk0q0yXCNTMsLfh7Mx688jNj/Oo5/vFH3OSm0wTGos8HxnbRrGLNyOaT8flPyYtsBghBza9vR8PPL5Tgycu0HU6+z1PVkvvaSBxL+O4czFMpNDP7amlAQ2e5FqYbGFGw33kD35zR689qvh4aoLhVfw6eaTKCyTLpfIFiokDkbt2Tv0pJ0Ke209kY+TeaZ7du1t5IJrPcl1ifg/7z0rV3NEYTBCstON7L/dccb4hgbsOu04d6DGLlCAeUMNmZfK8Nbqa3d8rhZENKaiugbz/07D3kz71LNYslVnFeCrf4vCsipsPZFv9DX3L05G4l/H8N+f90vaFt3PkBTvi2PZxbjsxMMlUrHmXJdX1WB/VoHVgZru376xYW8lYzBCsqrfxfjW1eXelaK8qsYmQ0CW3J2v2ncOX2zPMGtbo99vZn7xCQKQdakMc9YcxYVCxxhKWbYtAx9uSMc9n/xr82MZO43vrDO9GN3Zy7Xncstx4wGLFKQISKypAEuNG7t0J0Yv3I7vd9XmWVnyJ6vRCFb39CnlpobBCJEJN83bhIFzN+BEjrQ1KaSefTHrjyN48af9Bu+ydJNevzHR81T/S23s0p34bMsp/OfLxpPtbDkl0txzdUJn5pXuMIgt8l+MfYHLmbvkSDQaAf+m56PwinKHq/48IP06V6U6dWBSrlaktaTIIlCbNN7zjXU4ne+4vSG6GIyQS/nzwHm8vPKA2RnmF67WbPjHxLowUoQVupfyP/afx6urDppd5rtGI2DZ9tNYmXIWGSZqRwDiunEzL9Xu6+iFogbP5RVXSLaqrC30mvW3TS4mxpg6E3LceZoTG5ZW1mDhxnRZZvt8tysTDy/didEf61fBVdJikDN+O2yX45SUV+P7nZmih8U+2pCO0soazPtbvnwzKXnI3QBybSpIl1Bojinf77PbsSz1zA+1bezdNhD3RbUxuW1OUblePYYaS+dTNuKvgxfQv31ztGjqhX9P5uPhJTsxvGswPn+sn6THkfKdMPvPI7ijZ2u9x37ff95p1vIwRkyv27vr0tApuClGdg+1YYsaqltdu7Hg2RWcyi/FK6ssn/HiLG9n9oyQw6io1uDFn/Zjf1aB3E3RU1Jum6qmjVcWFTBnzVEcOGvdSpypWQX4z5e7cUqnuzev3rEnfbcXYz6pXc112bYMAEDSMelXEc4tbvg7S3mz/OwP+sGooV4fR6Qb0Is9X3V5LPUdkmmFV1tfXGurBZcodkFMV8VghBzKypSzGL1Q/BLnL/20X1RVyOM5xRhj5nEWbz5p9Dlr6mjUX3zPEGOBkJhKpgfPFWLDsVw8rpMbMttAnYasS8rNh5jx2yH8slf/XOUUVTSa66PknAUxPYZS9PbUD2KWbjtteEOz96eMi31MYhIO66x4/fm204idvxn/XSluHSClcpaePgYjJC8rP0jmfhArqjV6c/AbM/GbFKSa2QNTIEHNiL8OZTd47PD5Ikxdnmr0NYJgvHfimR/ED0fpBk4XS8SNX0v5fXiqXu0GQ5e0nKJyfKRTCffrZMOJuSPeb/g3P3Ledr0htrj8FpdX4dPNJ5F1SXlDGj/tycI9n2xH3tUeLXsOuZrrQmG5Xo/YB0knANTW3yitqLYy98k2AddfBy/gvkX/Ys3BC/jPl7uRnmu7JFVlhIwMRkhmldUaxUX2giDgcpl9ayxYsoCdqWBJtxKmlKf3zMVS/HPUvHU1si6VGe0Krx9w6Lr5vc3IK65AXnEF/th/3mCy8TSRq9vqssXwkinWftm/+ccRJP51DLd9uFWS9tRnqgdj47FcDHlno9GKsi+tPIC9mQV4Z63pKc3W2HnqIjZa+Tcz9j7sPnMdxn+xy+Bz9c/L7/sbJkTbqvNn0nd7sefMZTz93V5sOJaLx4y00ZkwGCFZPfbFboted6HwSoO1NTal2fciYw0p7iAXbTI+PGQtYxeo+xtZR6bOH/vPY/A7GzHJyGJsCT+aLvr10YYTGP3xNjzzwz6DPU+ZEvUSXKmqaVCK/+iFInyyKV3UAoN1p0vsX7W4vApfJ2cYfG71wQu4UlmD5JMXr25rfsAqVQ9F/Je7kXmpDPcvTja5yu8hiXqbTuaVNshXivtsB+K/3I3cYtNLJlg6LGSsSN3lsiqsPZSt3W/9fCNT1h66gO3p0tWSMZbXAyizN8oSDEbI7jbWCxoGv7NR9D5iEjdg6Lub9L6gLQ1szKG03hugNgvf3gwlmBry6ZbaQOlvI6uTFpebHtr6OvkMzhcav/hIVdck/ovdiJ6ThOM5xVh94AJKKqpx6wdb8c7aNMR9mix5fZn6pv18UG8Kaf1fa4bIVZAbk2HFe2bIuxuNJrU2lgj8xh/G14qpP4X9jJGg55KJqa8vrzyAYfM2oazSumTy+ud/4rcpSDIxrd+Q8wVXMPHbvRi7dCcOnC2wqj2uhMEI2V28hEGDraqD1r/YmbrpUsLy8XKzd7Am9eFueX8LJn+/F1N17n73ny00mHMihbr3zPp6wdqxeknWKw2sK1KjEXDgrGWrsQ6bt0n0a3T9fbhhbpM5tpio5ro3s8Doc+YWkVuxJwtnLpbhzwMXxDatUaaWF9B939etU5OnE7Df9bH4ZHuxrP3sKSXRmMEIOTQbldVo4H82HBPX9amJmTnm+DApHceynWO6qim2Cn5slU+SebEM9y/WL1Ov0QiotCCgmL8+DXd9vB0v68wG0T0dP+6xrKKnFBeln1IsO7YxJ3QSN7/fmWlx8rHuzYW0b51rexv98XZUVjf8e64/koOCskpOJW4Ei56RQ1tr4Z2aVPafLcR+C+p8lFXWLuqmq/BKFRL/si7o+X3/+QaJdipV7YVPzAXc1BCJOewVJCqNocvNsm2n8ceB8w3qwSw3owy4IKBBTsvCjbUB6y/7DE/7PnOxTNTUbkEATuQUI+6zHWa/xhjdGU5S+zr5DL5OPoPVz96I7q39Ld6PoSG+pVtP4eauwSZzMxpTUlGNzq/9hfEx7fQerytK2KuN5W2u8/qvh1BaWY35D0Rava86SgmRGIwQGWDoDkdKy7Y3rOFgqxWIi8ur0W3OOvSLCJR837/uO4cxvcMaPH7ERA5BRn4pTuZZnruQsCLVoVYnfXtNw8Ufy6s0Zlfd1F3RWbdehikXS83L7anz6ippc1Ns6cWfDuCvqYNFvaaxXp+3Vh+VbJHOr4xMM7fkpqWOSlW7aGfd2lLPx3a2eF9KxWEaonr+vJrIaG+6Zd2lFD0nCQCwO8P42LelnluRKvo1t1iZh2GsR8CeqmXq+jF3hsbbIi6sb685irIq+77fzQ2831+v9HVX5OlX2HHqomT7UsrwEYMRIrLK2KU7UFVz7Qstr5EZN5bkSShN/Z6ZuoRUW+fxzllj3jBe/UTYxhw6Z32ekaVTWU31WphbeLAxtlxV2l50qwW/JGH12K0n8nGlUvqVrcXiMA0RWWV7uv5d2hNf2W6KtVIy/xVHAdfaPfV63p63oNfMHIIgYO2hC0jLLkFVjQZ/6KzO/F69PCxdmRfL8MYfhy1aAqDASBFEe70dBeFaD6ct7Dh1ETd1DbbZ/s3BYISIJGXN2Hhj3pNhuXRzip/9uPssHq2XuGhPdit8ZaKH4f1/9P82/4iszyHGxG/3Gnw8p8h4r9yUH/ZatKjkp1tO4RMbFhi0ltheMKViMEJEDuPjjbabrWHME181nsvz/j/HkXQsBy2aqO3QIvkUlFUi61IZpv9yEBOGdJBkn+dtVCtIV+GVKlEzjHSZyqmQahhJbpbUrJEac0aIiEzYZmYuxIGzhYqZJmkrXyefweB3NmJbej7GL5NmvZTnV5heGkAKpqq3WsNZeiWe/CZF9iFQBiNERBK5LMEKzmSaMyRAK5ElQ1hSYjBCRCSR/U7Sba9kp6yoUUPGyT1Uw2CEiGzqy+2nsWJ3ptzNcGq2GoYgshcmsBKRTdWt2HpvnzbwcOf9jy2cznecirREhjAYISK72HHqkkU1Hqhx82SY8kyOacXuTPRuK/3SENZiMEJEdvHI5zvlbgKRy3v5Z/PWRLI39pkSERG5uLpF+OTCYISIiMjF/ZZ6vvGNbIjBCBEREcmKwQgRERHJisEIERERyYrBCBEREcmKwQgRERHJisEIERERyYrBCBEREcnKomBk4cKFiIiIgLe3N6Kjo7Fr1y6zXrd8+XKoVCqMGTPGksMSERGRExIdjKxYsQIJCQmYOXMm9u7di169emHkyJHIzc01+bqMjAy8+OKLGDx4sMWNJSIiIucjOhiZP38+JkyYgPj4eHTr1g2LFy+Gr68vli1bZvQ1NTU1GDt2LN5880106NDBqgYTERGRcxEVjFRWViIlJQWxsbHXduDmhtjYWCQnJxt93axZsxAcHIzHH3/crONUVFSgqKhI74eIiIick6hgJD8/HzU1NQgJCdF7PCQkBNnZ2QZfs23bNnz++edYsmSJ2cdJTEyEv7+/9ic8PFxMM4mIiMiB2HQ2TXFxMR599FEsWbIEQUFBZr9u+vTpKCws1P5kZWXZsJVEREQkJw8xGwcFBcHd3R05OTl6j+fk5CA0NLTB9idPnkRGRgbuvPNO7WMajab2wB4eSEtLQ8eOHRu8zsvLC15eXmKaRkRERA5KVM+IWq1GVFQUkpKStI9pNBokJSUhJiamwfZdu3bFwYMHkZqaqv256667cNNNNyE1NZXDL0RERAqh0QiyHVtUzwgAJCQkYPz48ejbty/69++PBQsWoLS0FPHx8QCAcePGISwsDImJifD29kaPHj30Xh8QEAAADR4nIiIi+WxLz8eQzi1lObboYCQuLg55eXmYMWMGsrOzERkZibVr12qTWjMzM+HmxsKuREREjqSiWiPbsVWCIMjXL2OmoqIi+Pv7o7CwEH5+fpLtN2Laasn2RURE5MiWjOuLEd1CGt9QBHOv3+zCICIiIlkxGCEiIiJZMRghIiIiWTEYISIiItTIOLWXwQgRERHhRE6xbMdmMEJERESyYjBCREREsmIwQkRERLJiMEJERESyYjBCREREUKnkOzaDESIiIoJKxmiEwQgRERHJisEIERERyYrBCBEREcmKwQgRERExgZWIiIhcF4MRIiIiggqcTUNEREQuisEIERER4VJphWzHZjBCREREyLp0RbZjMxghIiIiCBBkOzaDESIiImICKxEREcmLdUaIiIjIZTEYISIiIlkxGCEiIiIO0xAREZHrYjBCREREnE1DRERE8uIwDREREclKJWM0wmCEiIiIZBykYTBCREREMmMwQkRERLJiMEJERERMYCUiIiJ5uTGBlYiIiOTEBFYiIiJyWQxGiIiISFYMRoiIiEhWDEaIiIhI1qQRBiNERETEhfKIiIhIXqwzQkRERC6LwQgRERGxzggRERHJi8M0REREJCsmsBIREZGs2DNCREREsmIwQkRERDLjMA0RERHJiD0jRERE5LIYjBAREZGsGIwQERER3DhMQ0RERHJinREiIiJyWQxGiIiISFYMRoiIiEhWDEaIiIiIdUaIiIhIXjLGIpYFIwsXLkRERAS8vb0RHR2NXbt2Gd32l19+Qd++fREQEIAmTZogMjIS33zzjcUNJiIiIumpZOwaER2MrFixAgkJCZg5cyb27t2LXr16YeTIkcjNzTW4ffPmzfHqq68iOTkZBw4cQHx8POLj47Fu3TqrG09ERESOT3QwMn/+fEyYMAHx8fHo1q0bFi9eDF9fXyxbtszg9sOGDcPdd9+N66+/Hh07dsTUqVPRs2dPbNu2zerGExERkTQcJmeksrISKSkpiI2NvbYDNzfExsYiOTm50dcLgoCkpCSkpaVhyJAh4ltLRERENiFn0TMPMRvn5+ejpqYGISEheo+HhITg2LFjRl9XWFiIsLAwVFRUwN3dHZ988glGjBhhdPuKigpUVFRo/19UVCSmmURERCSSnD0jooIRSzVr1gypqakoKSlBUlISEhIS0KFDBwwbNszg9omJiXjzzTft0TQiIiKCvLNpRAUjQUFBcHd3R05Ojt7jOTk5CA0NNfo6Nzc3dOrUCQAQGRmJo0ePIjEx0WgwMn36dCQkJGj/X1RUhPDwcDFNJSIiIgchKmdErVYjKioKSUlJ2sc0Gg2SkpIQExNj9n40Go3eMEx9Xl5e8PPz0/shIiIi5yR6mCYhIQHjx49H37590b9/fyxYsAClpaWIj48HAIwbNw5hYWFITEwEUDvk0rdvX3Ts2BEVFRVYs2YNvvnmGyxatEja34SIiIgs5lA5I3FxccjLy8OMGTOQnZ2NyMhIrF27VpvUmpmZCTe3ax0upaWlePrpp3H27Fn4+Piga9eu+PbbbxEXFyfdb0FERERWkbPomUoQBEG2o5upqKgI/v7+KCwslHTIJmLaasn2RURE5MieHNIBr9x2vaT7NPf6zbVpiIiISFYMRoiIiAjXt2om27EZjBARERG8PdxlOzaDESIiInKctWmIiIjIWckXjTAYISIiInh7yhcSMBghIiIi9ItoLtuxGYwQERERc0aIiIhIXirmjBAREZGrYjBCREREsmIwIrNHB7STuwlERESyYjAiMzkThoiIiJSAwQgRERFxNg05l18nD5K7CURE5EAYjMjMGUdpIsMD5G4CERE5EAYjZmJuhzj/GdRe7iYQEZGDYDBCNsHgjYiIzMVghBrw8/aweh+CIEFDiIjIJTAYkZlKgV0Is0b3kLsJRETkQhiMmKldc19Zj//yqK6yHt9cS8b1lbsJRETkYBiMmOntu2/A3b3DsHJijCzH79iyiSzHFWtEtxC5m0BERA6GwYiZWjbzwvtxkegb0VyW40sxXTZhRGfrG2KmFk3VdjsWERE5NgYjVvBVu9vtWMF+3vh32s0WvfbGTkHImHs73N3My08RYH32Kaf2EhE5FlZgdWDGru9hAT5m7+OxgRFmbdfawD6Vmkvio3bHm3d1l7sZRETkABiMmEntLu5UbXv5JrO3feOu7jg2e5So/Qf4euLE27di0rCOol5HRESkNAxGzDAuph0igsQlkJo7ZbduM29P40M+E4caDjg8RQZIUjN32IeIiMgUBiNmMFV3w8vDtnkjix/pg2m3Km8opktIM9zTO8zkNgIrnxEROQyVjKulMRixku2TWO335oi9Phi/Th5k1hty3fND7JrAS0REzsv6ut8uztp7f7G5KLa0dHw/AEBGfqnMLSEiIleinCuhE2jl7y36Nff0adPgsSk3dZKiORZrbGpvh6sF2JRYyp6IiBwPgxEDhncNNmu7+pfi6bddL/pY3p4N/wR92gWI3o85rMnh8NBJVv118iApmkNERATAxYORZ2823AOxcGwftGsh71o0Subn7Sl3E4iISGIseiaThFu6GHzc29MdN5vZO6LL2N/xvqiGQzFERERUiwmsRjw3vDMy8ksxppHpq0RERGQdl+4ZMcXf1xNfxPfH6EjrgxG50zwnS5AQO3X4dQCAB/qyl4eIiKTl8j0jfdoGYG9mgdzNkNyYyNb4NfU8ACCmYwsAgLn5q4a2m3JzJ9zSPRSdgptK1UQiIiIA7BnByokD5W6CTTzYv62k+1OpVOgS2kxUCXjWXyUichxyLjHi8sGImwXrq7RoogYA9G/fXOrmSKJ/++bo1toPgOGpw0REREri8sM0llj19CCs3HsWjw2MwOqDF/D6r4cQ1zccN4T5W7XfoKZq5JdUok/bQES08EXGxTIM6GB+wDP5po4I9fPGHT1bw8/bE/tn3AIvBiNERKRwDEYs0LaFLxJGdAYAPDqgHYZe1xJtAn3g5qbC6mdvRIsmXhbtd/u0m1FepYG/jyf+SRiKimoNmngZ/hMFNW14jAAfNR6NidD+39/XvvVAPN3lTtUlalybQB+cvXxF7mYQkQ7eNkugbQtf7XBP99b+CK1XFt7DzHE4Lw93+Pt4al9jKBD5/oloRLdvjsWPRFnZauvpFsi57YZQVmZVuKZGAltXo4TPDhHpYzBiQ9Nv7Yr2QU3w/IjrJNvnwE5BWPFUjMFZLXVrxsjhk7FR6N7aumEqsi0GI0SkVAxGbOipoR2x8cVhCG4mfgE9MX55eiBmj+lhUdVYQ0Z0C4Gft/NeuJ6P7Sx3E4jIzgLsPGxN4jAYAdC7bYBdj/eQxNNu+7QNxKMD2jW6iu79fcPN2l8zb0/sfX0EgptZlvuidD3bsAeHyNW8PKqr3E0gExiMAPj+iQE2P4ZufY437upm8+MZEurvjWOzR+HDh3o3uq2Hu5usiyaR9ARWfgEAdA1tho4yDmmSPPh1ds2rFqwwb2sMRgD4qN0RFuAjdzPswtvTXbIPZa82ARLtich+PNzdsP75oXI3g4h0OG9iABnVroWvJPu5q1drVFTXIDI8UJL92UvXVs3kboIsAnzUyCmqkLsZimBJsUNzPHFjeyzddtom+ybrsKf3GiWeC/aMuKCeEvVouLmpENevLbqEGr64m7sWjlhDO7dE/wjLq98aqtHiCppfrRxM5mlhwfl67Y5u8PLg1yqRWPzUkEP64KFI3BfVBq/dLn7sU871F+QkNmdkfEw7G7XEsJgOLex6vMbseGW4qO2bqN0BAOueG4LnYqWbzu+MhnVpKXcTMKp7qNxNIB2u+a0sg2lXM7kfGxghb0OcRCt/H8y7v5d2DR57i70+RJbjihHid60H6L6oNqJfH+Br356UVy0ILG1JbNDa8urss4igJngutjO6GukxVAqVCrhJpqDgySEd8NfUwXY9pofbtb/nzDu7ISKoYRKzh42G76hxDEbs5InB7bHlpZsw8055ZtIoVYifF2Kvl6Y+ihhR7WrzXPpYOK17zt09JGyNNLb+9ya9/7fWScp+dEA7BPg0DC6WPdbX5u1yVa38bVtfyFrfPR6NL+L7y3b861vZ90bijl6t9P7P2WXKwmDETlQqFdq28G20Foi91CWxdglR1t3bwI627aqvKwX+w4QB2PDCUEwc2tHifSntYhPe3HRics/whvVVbu5qvx6e1go7X7aWeE9PuZtAOrw83LX/tlU+G1mOwYiL+idhKA6+cQsCmyirKuH/7hX3Ba4SOVF5VI/acWK1hxs6tGxYUl+Mbx6X765SrMYCFUOk/L7eMX04Nrw4TMI9Ko+3p7ve/+uvUaVUrloEUOx3B9kWgxEX5enuhmbeygpEAMe6Y+kU3AyfPqrsRde2/vcmrHtuiOwzaUL9vRtcrJ3Fxw/3RseWTfDBg40XEzTXpGEdOSuHXArf7S5OGXcHlrdByiTBDgYS2hpzS7cQjI5sLVkbpC7BH97c1+jUazlESFTjxhz2ChTv6NkaSS8Mk/Q8u6tUDhuMPDrAfrOwIsMD7HYspXJ3kqRbx3y3OzjdAMBZ3kiWqu0Jsbw7xNPKL+wbrwtCiyZqDL4uyKJhBJVKJekdsZ+PMnqr5FiXSOp0qpEuMHXz6WGW5zzZygAzpmh3syB5lQvdGeaoQWt9zvFbOBi1hxv+M6g94vqGo02g/e4UlWL+A71kOa6hYme+ag/sfGU4vv6PuPwPfzt+MUq1joSYIbBbe9jmQq6UBG4A6N/e8sJ5tmbuTI9AO0+/ttahN0diz2uxFk0bT51xC54dLn/9FqVN/7W0NaueHihpO6zFYEQmM+7shv/d59zZ9oam7nVo2QT39Klf8+Lax8mcL2HdKaveRu4KDM108VEb3rZ2UUDzP9Jqdze9zHxbmzCkg92OZUpdUS9nsWRcX3z0UG8cfOMWuZsimZ8nDcQYE8OG9qiP0yrAeOJuUy8PRVRAtiYm7hLaDA/1v7YC+ohuyq85ZEjvtvrLeNSVO5ALgxGymZiOLbDw4T74aWKM9rE37+reYDtvz2tvw7aNzPoYE9kaL4/qov2/h7ubzacD12fLZFB73nPNHq3/t7i7d5jJ7Zs3Nf/39rEgWdXe+Uv+Pp64s1drNPP2xIw7bFv/x5KFOIP9Gp+NU/+iGtUuEAtMDBsueDBSdDvE6tM2EN8+Hq1XdE8p/jOoPdoHNcH9fcMNDm+MaeQzMKJbCD4Z2wduOic+3oxClq/b+P0lha9E9g5LzaJgZOHChYiIiIC3tzeio6Oxa9cuo9suWbIEgwcPRmBgIAIDAxEbG2tye7IvWxf+ub1nK73Evo6NTKdtrIdiwYO9G3TxtpDwTiu8uf1Xb5bymPebqLSqe2qPzR6FR2MiJDtufT9Psr4LuENL8QnFSjVrdMMg3JS7e7fB4keiMKBDc9tVSbUw9runj/ELtvrqBf7G64Kw85VYyw5gQly/az0SlgxfzrizGza8MLS2h8ZAXtTs0T3wwYORWDLOcDHAJeP6ol0L8e9LcysgD+0s/m8t1dBnUy95180VHYysWLECCQkJmDlzJvbu3YtevXph5MiRyM3NNbj9pk2b8NBDD2Hjxo1ITk5GeHg4brnlFpw7d87qxhMZ8uvkQRa/duMLw3Bs9ih88Vg/o9tMvrmTxfs35OeJ0o3dPti/rdHndHNGdKfZ1i0IN6JbiHZG0Z29rJshZKxMvyAiceWnp2Ia30giTbyUNQTVKbgpOgU3xfInYxBj556/xtzTu412OrO9hQX44NjsUTideBsmDOmAHmHiE2HrLt6GLuE+aneMjgyDfyOJ5KKv/2a+7aWu0P3y1WVI6musB1oOooOR+fPnY8KECYiPj0e3bt2wePFi+Pr6YtmyZQa3/+677/D0008jMjISXbt2xdKlS6HRaJCUlGR148nx1H0m68au4we1b7DNn8/c2OBu8N4+bTD3nhsa3f9LI7tYNd3Pw90N3p7uuKmr4RL1G14YikeijV/wLSH3jKp/EoZixZMDcGuPUKyZOhibXhyGvgZWRb6xk/7fpP6UZmvzEeoPZdgz2XVM7zDZ1mmpf1Gvn+8UFqC8C0fddOb6Qk0MLbWUaIaWt6e79r3x8qiumHJTJ/w1dTASRnSWZP+AuOrKSi6NNKnebKsfJgzAq7ddr8g8F1HBSGVlJVJSUhAbe637zc3NDbGxsUhOTjZrH2VlZaiqqkLz5sYz2SsqKlBUVKT3Q47L0CXlk7F98PuUQXjKQHJmjzD/BmtmvPdAL5N3/eYcU4qCah1aNjV5kRzeNRh+3pZ3d3p52j+NK7CJGtEdWkClUsHb093gAmJv3tW9wQrJUk5pBhrOUDJ2lmePkX5dIC8Pd733XKDEs6WMFXyLbt9cWxW4Tv2hTFvNbLKUqaFdY0NrY6PbYvWzN0relmbennhxZBeL1rkx9XVgScXiOh/YIS/HUjEdW2DCkA6KmtVWR9Q3X35+PmpqahASoh9VhYSEIDs726x9vPzyy2jdurVeQFNfYmIi/P39tT/h4eFGtyXbCbfhtGO1hxt6tgmAm5tK0XcWuswpm92zTYBVxzCU4GspPwkr7I4fGIEmjYwp2+v7Te2uvC9SY14Y0Rl39GyFGDNqbxjj1kjPmZgLsdRn7rfJg8wqLnf7Da0Q3MwxyuNba3Sk6SRYqTnOp8E0u96GzZ07F8uXL8eqVavg7W38jTl9+nQUFhZqf7KysuzYSqqz4MFIjOwegpUT7Td2bwl7fBifHtbRaFKblNo2l24c/u16Kwv72nhq7hg7fwnXZ02ukDErnhyALiHN8KOF+SvPDL8OHz/cB25uKtwQ1jCYlSIQv6Nnq8Y3MpPYhTN7hQe4RHG5+nRnfpnT4+puIoBWW1C07DGdGTzW3AT0tnDVclsQdRaCgoLg7u6OnJwcvcdzcnIQGmr6DTlv3jzMnTsXf//9N3r2NF1fw8vLC35+fno/ZH9tAn3x6aN9DeYPuJr/juqKEDOmWor1xxRpu67b6wyx1C+o91D/thjUqUWD4Rap+NZLAq27sKndr33NWJsf81ys8aJXtigNHt2hBdY9P0TRBdKk9P2EaLsez1al46VcdqBNoA8eHdDOrGGmexvUUKrV1MsDz5qR+K47fGJqavQbOj2o/r6euMvChHOpCipKQVQwolarERUVpZd8WpeMGhNj/M7hnXfewezZs7F27Vr07Wv7u0tyXaYS6MT63721CbPv3W+7irE3GBj60b1Tih8UAaA2kAD074gMad5EjU0vDsOuV4Y3eM7b0x3fPTEATwy2TxG1peP74p7eYfj9mWs9Fh882Nvi+hPBzbwMJjzbyy0KTPqTmqXT5MfF1AYVz8caSSI1EoM+P6Izgpp6YXyMtEHJN49H47pg61blrtOiqRdmj+mB7q2vfVaNvYffM1FdOuGWLkafq6M720xM3Z0PHzKev3X7DdL1nNmS6P6hhIQELFmyBF999RWOHj2KSZMmobS0FPHx8QCAcePGYfr06drt//e//+H111/HsmXLEBERgezsbGRnZ6OkpES634LoqsaKFokR168tjs0ehXvNrBEASJM3see1a/lUdV9Is0Z3xy9PDzSrVyMiqIlZBbMkV6+7Ory5L+bHRaJr6LWezR5h/tgxfTiGmTFzRfdcTru1K3ZMHw5/H0/ZKkUO6hRk1euNJX72Dpe38qVYhoYlZo3ugaOzRqGXyN6p5k3U2PXKcLw5umFScncj08PNEd7c1+KhNXM8MbgD7otqY9PFGB8SkbBviqn1rpSUxyo6GImLi8O8efMwY8YMREZGIjU1FWvXrtUmtWZmZuLChQva7RctWoTKykrcd999aNWqlfZn3rx50v0WEpgwuPaOyxXufpxZ3TCAVNnilix7b+2xDSWeerq7oU/bQHi4G/7I2rIqrDnEzCCw9PzUJXJ2CpZnFeKHo9vi+djOklbTVAEYfn0wPhnbR7J9WmtBXKRFwxw+FuYk1f1ddWehRYYHYPWzgy3aXx2x+TiGehON8fZ0x7z7e9k0X2bisA5Y/Iit3xfKiUYsmoM4ZcoUTJkyxeBzmzZt0vt/RkaGJYewu/EDIxDdoQU6SdS1R2RPUiYxiqH2cENltQbDrw/B7tOXZGmDvXi6u2Fq7HVIzSqQdL8qlQq3ydiVPuL6EBw4W6gd4hzTOwxjeochYtpqu7ajdYAPirKLLX59m0DrKhlLWclZCu4qFaLbK6vgnS3JW/9VQVQqlUVz1cl6QXa6q29voH6GLSip69PWDsy8BRXVGslKSW94YShufm+zJPuyRqi/8QubmCqyjbF2T8Z6mQZfF4StJ/LN2sfEYR3RoWVTh0/SHR0ZhlN5pdL/HhL+vW3BWG5JXXJwkIg1peTEhfJcXF3C2cMSVxU1xtAX+fMjOiPWDl3V795nu0RUV+Xt6d5o6WxDjMVrvmoPnW3sH9X9+FQMhnVpiUVmvhffj5P/PWXoM/XZo41PFKg7v57ubri9ZyvJKqTKxd1NhRdHdsEQC9Z3kct1wU31VsO2JuzRnRmT9tYobXHAiUM74tYeofj4YWmLFEqNPSMuLrpDCxx6c6RNl4dvLEcgwFeNpeONrwUjhWZeHggVUeLZFmaN7o4Zvx0GAPRtF4gJBqrP1meq2mWLJmpcLK2UrH3meKBvG/y456xdjwmYV3BOCv3bN0f/9uavXnp3b/OTm+3J0vwNc9hz+YL7o9rgp5SzGGunmyWpfP2f/vhj/3kE+HpiuIllEtY9NwR93/7H4HMqlUpUL6tuMOnlce3v38TLA4seMZxoq6ReXAYjJPtqja7g0Jsj0dTLQxuMvDSyC6KvVuX00kmSlaMcvBimFtiyxQrQG18chpO5JRjY0bqZLEpgq95+MQnB1lx8wpv7oF3zJhhgRTVZc9W18+27b8D9fcMVVZyrvqh2gWjq5aFXCn9I55Zm9dC4uakkHfZzZLwKEdlYh5ZNTAZ8Tb088H5cLwiCtCXc7c2a79S6fB7dIR9vtRva+zaxW66Ps+kf0Ry7MqRJKt75ynCriv5ZOuSm9nCzWy6LpR0+Pmp37H19BDxkXvDS0TEYIZJY/a8kQ19S9e9mldrdbw9/Pz9Euw6Sj9od/yQMAaDS62qu78enYpCWXYTXr/Y0uYrmTdR44ZbOeHXVIQCmJ2Y+2D9csmDEFtWHbU33BmDarV0bPN8v4lp9l9QZI6yakm9JSfc61pYCELPCsJIxGCFyUEoa77VG53rroZhTR6Q2t6O5LMFI6wDrppDqsaA3aWx0O20wYkq0HYZTlEzt4YadV2uHGAqmuob64c9nbkSInzcCfJU346Sx4ZvlTw7A0q2n8cZdtXVvrFlpWAkYjBCR0xvSuSWCmqgxWoIKvSF+3vj+iWg0U/iQWliAD7a9fBOKy6tx6wdbZW2LLfKJzNFYj04PA4sX1mfPlrdv0QS92wbAz9vTaIHDOgM6tNDL34lqF4i599yACBHDmkq6n2EwQi6huYPMta9Pztw2sUPgSs7D8/P2wPy4SMn2N9DK0vBSMnUH3SbQF2cvl9mxNdJ4oG+43E2QhZubCr9MGmjwuedjO+P9f46bfP2DEpWQl4OyU/eJJBDVLhBLxilrgUZHyKB/bGAE2gT64Kmh9llYz9k5wJ9cVkN11it6sJ9rBiNA3ZTeq8ta6PRdeLgrqR9DeuwZIaekm0/xs5E7DVvxVXvgclmVwedu7RGK0/mlki/21qKJ9AWrAnzV2PrfmyRb54esV/8vEdk2AP+mm1dpVaxR3UOx9nA27u1jn+Tq52M7o0NQEwy+riXfcy6IwQjZnO5sEh8LFp7ztWEBJ1v49NEoPPvDPpzKLwWgf3ez6JEoCIIgyZetIACLxvbB2sPZmDCkvdX7M4QXBWXa+t+bkHWpDH3aBtosGHk/LhL3n8y3erViwLypvd6e7ojr57jDDGQdBiNkc96e7nj3vp6orNGIWozq50kDMfevo5hxR3cbtk56PcL8seHFYUYXGpPyAn/rDa1wqx0XWesSqsz1m2KvD5a7CXYV3tzX7NkTuis6qxtJitTlo3Y3WT2USEoMRsgu7rcgIS2qXSB+mmjfIRYyLfb6YPzv3hvQvbV15dk7hzbDxrQ8iVoFdAltfDqwq/JVe2DDC0Ph4eamN0ODnV6OpW7mjJQ9xUrq+WQwQuSg5PgeUalURrvSxeRnTh1+HdxVKozqESpNwxyAnPmrHVo2lfHojs1Wicf9I8RVlg0L8EHy9JsdukqzKZxNQ05pXEzt8tmDr7PPFMzRka3h6a6SfBaAFOP19iJmhpCv2gP/HdUVPdsE2K5BDmbC4Nq8nxdHdhH1ujaBjl3syhWF+nnj88fMmOGnc8MhQEArfx80cdK1xJzztyKX98SNHdAvojmub2WfHIcFcZGYd38veIoYkzdlx/ThSMspxhA7BVMkv1dv74YXbukCbxNJ3oZ6w+7q1Rqn8krQV+SdNsnnpq7Bii+aZ28MRsgpubmp0LuttNNnTVGpVPCUsA5AqL83Qp1kzQl7Unu4obJaI3czzFa/MqmpQMQYNzcVEm4R15tCclNG0RklLULJYISIiGxKQXmSpMPfxxM7XxkOLysW+pMKgxFyePyiUwZl3OsRkRhKWZFZ/nCIiMgFDOoo/dRMImfBYISIyA5eHNkFs0Z3x9/PDxH92lHda6dAP34j1wlyNnUrBxtamFK319fZ1zbiMA05PGf/kBrH8anGRLULRMqZy4hTwMJr3p7uGBcTYdFrP3yoN47nFKN7a2VWwHVGN3YKwrb0fDx6tUyAlDzcrvUDvHd/L3yyKR0PO/CKu1JgMEJkY5HhAXI3wWl5e7qhvEqDoZ0Nl4Nf/uQAZBeWm1063d7atTBvNoPaw017B032seyxfjidX4rOIdIVjHvt9uuxYncWnh1+nfaxls28MPNOx1rywhYYjBDZyD8JQ/HngfN4/EbbLGJHQPK04Th7+QpuaGP4Qu3p7qbIQOSniTFYuecspt/WVbY2dLRjVVZH7MNTe7hJvszAE4M74InBHGozhMEIkY10Cm6K52I7y90Mu2kT6GP3YwY2USNQZyG4r+L7Y+K3KZg9pofd2yJGv4jm6CdTkbKdrwxHSUU1WjYzf9FKS93bpw1O57MgGzWOwQgRSaJ7a3+8H9cLYQHy9UTEdGyB1BkjFLUAmNKE+HnDXmvxvvdALzsdiRwdgxEikszdvdvI3QQGIuQ0fHUq8jbX6QF0RgxGiByWy04jInIJHu5u2PXqcGg0li0V4EgYjBARESlUcDNlVEi1NRY9I3JYHI4gIufAYISIiIhkxWCEHB7zFYmIHBuDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIgfFWURE5CwYjJDDE1gVnYjIoTEYISIiIlkxGCEiIiJZMRghIiIiWTEYISIiIlkxGCGHx1klRESOjcEIkYN5/Mb2AIBXbusqc0uIiKThIXcDiEic126/HpOGdURQUy+5m0JEJAn2jBA5GJVKxUCEiJwKgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDESIiIpIVgxEiIiKSFYMRIiIikhWDEXJ4Ue0C5W4CERFZgeXgyeE93L8tPNzcEN2hudxNISIiCzAYIYfn4e6Gh6Pbyt0MIiKykEXDNAsXLkRERAS8vb0RHR2NXbt2Gd328OHDuPfeexEREQGVSoUFCxZY2lYiIiJyQqKDkRUrViAhIQEzZ87E3r170atXL4wcORK5ubkGty8rK0OHDh0wd+5chIaGWt1gIiIici6ig5H58+djwoQJiI+PR7du3bB48WL4+vpi2bJlBrfv168f3n33XTz44IPw8uJKo0RERKRPVDBSWVmJlJQUxMbGXtuBmxtiY2ORnJwsWaMqKipQVFSk90NERETOSVQwkp+fj5qaGoSEhOg9HhISguzsbMkalZiYCH9/f+1PeHi4ZPsmIiIiZVFknZHp06ejsLBQ+5OVlSV3k4iIiMhGRE3tDQoKgru7O3JycvQez8nJkTQ51cvLi/klRERELkJUz4harUZUVBSSkpK0j2k0GiQlJSEmJkbyxhEREZHzE130LCEhAePHj0ffvn3Rv39/LFiwAKWlpYiPjwcAjBs3DmFhYUhMTARQm/R65MgR7b/PnTuH1NRUNG3aFJ06dZLwVyEiIiJHJDoYiYuLQ15eHmbMmIHs7GxERkZi7dq12qTWzMxMuLld63A5f/48evfurf3/vHnzMG/ePAwdOhSbNm2y/jcgIiIih6YSBEGQuxGNKSoqgr+/PwoLC+Hn5yd3c4iIiMgM5l6/FTmbhoiIiFwHgxEiIiKSlUOs2ls3ksRKrERERI6j7rrdWEaIQwQjxcXFAMBKrERERA6ouLgY/v7+Rp93iARWjUaD8+fPo1mzZlCpVJLtt6ioCOHh4cjKymJirA3xPNsPz7V98DzbB8+zfdjyPAuCgOLiYrRu3Vpvpm19DtEz4ubmhjZt2ths/35+fnyj2wHPs/3wXNsHz7N98Dzbh63Os6kekTpMYCUiIiJZMRghIiIiWbl0MOLl5YWZM2dyUT4b43m2H55r++B5tg+eZ/tQwnl2iARWIiIicl4u3TNCRERE8mMwQkRERLJiMEJERESyYjBCREREsnLpYGThwoWIiIiAt7c3oqOjsWvXLrmbpBhbtmzBnXfeidatW0OlUuHXX3/Ve14QBMyYMQOtWrWCj48PYmNjceLECb1tLl26hLFjx8LPzw8BAQF4/PHHUVJSorfNgQMHMHjwYHh7eyM8PBzvvPNOg7b89NNP6Nq1K7y9vXHDDTdgzZo1kv++cklMTES/fv3QrFkzBAcHY8yYMUhLS9Pbpry8HJMnT0aLFi3QtGlT3HvvvcjJydHbJjMzE7fffjt8fX0RHByMl156CdXV1XrbbNq0CX369IGXlxc6deqEL7/8skF7nPUzsWjRIvTs2VNb1CkmJgZ//fWX9nmeY9uYO3cuVCoVnnvuOe1jPNfWe+ONN6BSqfR+unbtqn3eIc+x4KKWL18uqNVqYdmyZcLhw4eFCRMmCAEBAUJOTo7cTVOENWvWCK+++qrwyy+/CACEVatW6T0/d+5cwd/fX/j111+F/fv3C3fddZfQvn174cqVK9ptRo0aJfTq1UvYsWOHsHXrVqFTp07CQw89pH2+sLBQCAkJEcaOHSscOnRI+OGHHwQfHx/h008/1W6zfft2wd3dXXjnnXeEI0eOCK+99prg6ekpHDx40ObnwB5GjhwpfPHFF8KhQ4eE1NRU4bbbbhPatm0rlJSUaLeZOHGiEB4eLiQlJQl79uwRBgwYIAwcOFD7fHV1tdCjRw8hNjZW2Ldvn7BmzRohKChImD59unabU6dOCb6+vkJCQoJw5MgR4aOPPhLc3d2FtWvXardx5s/E77//LqxevVo4fvy4kJaWJrzyyiuCp6encOjQIUEQeI5tYdeuXUJERITQs2dPYerUqdrHea6tN3PmTKF79+7ChQsXtD95eXna5x3xHLtsMNK/f39h8uTJ2v/X1NQIrVu3FhITE2VslTLVD0Y0Go0QGhoqvPvuu9rHCgoKBC8vL+GHH34QBEEQjhw5IgAQdu/erd3mr7/+ElQqlXDu3DlBEAThk08+EQIDA4WKigrtNi+//LLQpUsX7f8feOAB4fbbb9drT3R0tPDUU09J+jsqRW5urgBA2Lx5syAItefV09NT+Omnn7TbHD16VAAgJCcnC4JQGzi6ubkJ2dnZ2m0WLVok+Pn5ac/tf//7X6F79+56x4qLixNGjhyp/b+rfSYCAwOFpUuX8hzbQHFxsXDdddcJ69evF4YOHaoNRniupTFz5kyhV69eBp9z1HPsksM0lZWVSElJQWxsrPYxNzc3xMbGIjk5WcaWOYbTp08jOztb7/z5+/sjOjpae/6Sk5MREBCAvn37areJjY2Fm5sbdu7cqd1myJAhUKvV2m1GjhyJtLQ0XL58WbuN7nHqtnHWv1NhYSEAoHnz5gCAlJQUVFVV6Z2Drl27om3btnrn+oYbbkBISIh2m5EjR6KoqAiHDx/WbmPqPLrSZ6KmpgbLly9HaWkpYmJieI5tYPLkybj99tsbnA+ea+mcOHECrVu3RocOHTB27FhkZmYCcNxz7JLBSH5+PmpqavT+EAAQEhKC7OxsmVrlOOrOkanzl52djeDgYL3nPTw80Lx5c71tDO1D9xjGtnHGv5NGo8Fzzz2HQYMGoUePHgBqf3+1Wo2AgAC9beufa0vPY1FREa5cueISn4mDBw+iadOm8PLywsSJE7Fq1Sp069aN51hiy5cvx969e5GYmNjgOZ5raURHR+PLL7/E2rVrsWjRIpw+fRqDBw9GcXGxw55jh1i1l8gVTJ48GYcOHcK2bdvkbopT6tKlC1JTU1FYWIiVK1di/Pjx2Lx5s9zNcipZWVmYOnUq1q9fD29vb7mb47RuvfVW7b979uyJ6OhotGvXDj/++CN8fHxkbJnlXLJnJCgoCO7u7g2yi3NychAaGipTqxxH3Tkydf5CQ0ORm5ur93x1dTUuXbqkt42hfegew9g2zvZ3mjJlCv78809s3LgRbdq00T4eGhqKyspKFBQU6G1f/1xbeh79/Pzg4+PjEp8JtVqNTp06ISoqComJiejVqxc++OADnmMJpaSkIDc3F3369IGHhwc8PDywefNmfPjhh/Dw8EBISAjPtQ0EBASgc+fOSE9Pd9j3s0sGI2q1GlFRUUhKStI+ptFokJSUhJiYGBlb5hjat2+P0NBQvfNXVFSEnTt3as9fTEwMCgoKkJKSot1mw4YN0Gg0iI6O1m6zZcsWVFVVabdZv349unTpgsDAQO02usep28ZZ/k6CIGDKlClYtWoVNmzYgPbt2+s9HxUVBU9PT71zkJaWhszMTL1zffDgQb3gb/369fDz80O3bt2025g6j674mdBoNKioqOA5ltDw4cNx8OBBpKaman/69u2LsWPHav/Ncy29kpISnDx5Eq1atXLc97PolFcnsXz5csHLy0v48ssvhSNHjghPPvmkEBAQoJdd7MqKi4uFffv2Cfv27RMACPPnzxf27dsnnDlzRhCE2qm9AQEBwm+//SYcOHBAGD16tMGpvb179xZ27twpbNu2Tbjuuuv0pvYWFBQIISEhwqOPPiocOnRIWL58ueDr69tgaq+Hh4cwb9484ejRo8LMmTOdamrvpEmTBH9/f2HTpk160/TKysq020ycOFFo27atsGHDBmHPnj1CTEyMEBMTo32+bpreLbfcIqSmpgpr164VWrZsaXCa3ksvvSQcPXpUWLhwocFpes76mZg2bZqwefNm4fTp08KBAweEadOmCSqVSvj7778FQeA5tiXd2TSCwHMthRdeeEHYtGmTcPr0aWH79u1CbGysEBQUJOTm5gqC4Jjn2GWDEUEQhI8++kho27atoFarhf79+ws7duyQu0mKsXHjRgFAg5/x48cLglA7vff1118XQkJCBC8vL2H48OFCWlqa3j4uXrwoPPTQQ0LTpk0FPz8/IT4+XiguLtbbZv/+/cKNN94oeHl5CWFhYcLcuXMbtOXHH38UOnfuLKjVaqF79+7C6tWrbfZ725uhcwxA+OKLL7TbXLlyRXj66aeFwMBAwdfXV7j77ruFCxcu6O0nIyNDuPXWWwUfHx8hKChIeOGFF4Sqqiq9bTZu3ChERkYKarVa6NChg94x6jjrZ+I///mP0K5dO0GtVgstW7YUhg8frg1EBIHn2JbqByM819aLi4sTWrVqJajVaiEsLEyIi4sT0tPTtc874jlWCYIgiO9PISIiIpKGS+aMEBERkXIwGCEiIiJZMRghIiIiWTEYISIiIlkxGCEiIiJZMRghIiIiWTEYISIiIlkxGCEiIiJZMRghIiIiWTEYISIiIlkxGCEiIiJZMRghIiIiWf0f6w6ycNj27yYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--TRAINING--#\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "EPOCHS=50000\n",
    "LR=0.1\n",
    "BATCH_SIZE=32\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # select a random batch and use that for fwd + backward for faster iteration\n",
    "    # batch_indices = torch.randperm(len(Xtr), generator=GEN)\n",
    "    # batch_indices = batch_indices[:BATCH_SIZE]\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (32,), generator=GEN)\n",
    "\n",
    "    emb = C[Xtr[batch_indices]] # lookup embeddings corresponding to each row -> (M, BLOCK_SIZE, 2) i.e one 2D vector per char in a row of X\n",
    "\n",
    "    # view shares the same underlying elements, just rearranges logically\n",
    "        # (number of examples, number of elems per example)\n",
    "    emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "    \n",
    "    # forward pass: get linear combs + bias for each row, activate with tanh\n",
    "    h = emb_view @ W\n",
    "    h = h + bias\n",
    "    h = h.tanh()\n",
    "    \n",
    "    # forward pass 2nd layer to get 27 outputs per row\n",
    "    probs = h @ W2 + b2\n",
    "    probs = probs + emb_view @ W3\n",
    "    \n",
    "    # this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "    nll_loss = F.cross_entropy(probs, Ytr[batch_indices])\n",
    "    \n",
    "    epochs.append(i)\n",
    "    losses.append(nll_loss.log10().item())\n",
    "    # --Update--#\n",
    "\n",
    "    nll_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "    \n",
    "    # for param in parameters:\n",
    "    #     param.grad = None\n",
    "    \n",
    "    # # backward pass\n",
    "    # nll_loss.backward()\n",
    "    \n",
    "    # # update\n",
    "    # with torch.no_grad():\n",
    "    #     for param in parameters:\n",
    "    #         param -= LR * param.grad\n",
    "            \n",
    "print(\"Loss:\", nll_loss.item())\n",
    "\n",
    "plt.plot(epochs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get loss for dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22655, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.1632, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "probs = probs + emb_view @ W3\n",
    "print(probs.shape)\n",
    "\n",
    "# same as cross_entropy nll_loss\n",
    "# t = probs.softmax(dim=1)\n",
    "# t = t[torch.arange(Xdev.shape[0]), Ydev]\n",
    "# t = -t.log().mean()\n",
    "# print(t)\n",
    "\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ydev)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1579, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtest]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "probs = probs + emb_view @ W3\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Ytest)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1105, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = build_dataset(names)\n",
    "\n",
    "emb = C[X]\n",
    "emb_view = emb.view((emb.shape[0], BLOCK_SIZE * EMBEDDING_SIZE))\n",
    "# forward pass: get linear combs + bias for each row, activate with tanh\n",
    "h = emb_view @ W\n",
    "h = h + bias\n",
    "h = h.tanh()\n",
    "\n",
    "# forward pass 2nd layer to get 27 outputs per row\n",
    "probs = h @ W2 + b2\n",
    "\n",
    "probs = probs + emb_view @ W3\n",
    "\n",
    "# this is the same as doing probs.softmax, selecting corresponding probabilities with [arange(M), Y], then doing NLL and mean. but better\n",
    "nll_loss = F.cross_entropy(probs, Y)\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : reysthilyn       | is in tr data: False\n",
      "name : syni       | is in tr data: False\n",
      "name : raha       | is in tr data: True\n",
      "name : kast       | is in tr data: False\n",
      "name : cemine       | is in tr data: False\n",
      "name : maru       | is in tr data: True\n",
      "name : warbelle       | is in tr data: False\n",
      "name : dumiyah       | is in tr data: False\n",
      "name : khnon       | is in tr data: False\n",
      "name : greus       | is in tr data: False\n",
      "name : eamanifa       | is in tr data: False\n",
      "name : broven       | is in tr data: False\n",
      "name : kaiyanna       | is in tr data: False\n",
      "name : ahap       | is in tr data: False\n",
      "name : shar       | is in tr data: False\n",
      "name : yulam       | is in tr data: False\n",
      "name : tiee       | is in tr data: False\n",
      "name : brigabo       | is in tr data: False\n",
      "name : vyoa       | is in tr data: False\n",
      "name : onna       | is in tr data: True\n"
     ]
    }
   ],
   "source": [
    "def load_params():\n",
    "    # parameters = [C, W, bias, W2, W3, b2]\n",
    "    loaded = torch.load('mlp.pt', weights_only=True)\n",
    "    C = loaded['C']\n",
    "    W = loaded['W']\n",
    "    bias = loaded['bias']\n",
    "    W2 = loaded['W2']\n",
    "    W3 = loaded['W3']\n",
    "    b2 = loaded['b2']\n",
    "\n",
    "    return C,W,bias,W2,W3,b2\n",
    "\n",
    "    \n",
    "def get_word(max_count=10):\n",
    "    current_block = [0] * BLOCK_SIZE\n",
    "    string = []\n",
    "\n",
    "    for i in range(max_count):\n",
    "        emb = C[current_block].view(1, BLOCK_SIZE * EMBEDDING_SIZE)\n",
    "        \n",
    "        # fwd pass\n",
    "        h = emb @ W + bias\n",
    "        h = h.tanh()\n",
    "    \n",
    "        probs = h @ W2 + b2\n",
    "        \n",
    "        probs = probs + emb @ W3\n",
    "        \n",
    "\n",
    "        # turn into probabilities\n",
    "        probs = probs.softmax(dim=1)\n",
    "        \n",
    "    \n",
    "        sampled = torch.multinomial(probs, num_samples=1, replacement=True, generator=GEN)\n",
    "        sampled_idx = sampled.item()\n",
    "        char = itos[sampled_idx]\n",
    "\n",
    "        if char == '.':\n",
    "            break\n",
    "\n",
    "        string.append(char)\n",
    "\n",
    "        current_block = current_block[1:] + [sampled_idx]\n",
    "\n",
    "\n",
    "    return ''.join(string)\n",
    "    \n",
    "\n",
    "C,W,bias,W2,W3,b2 = load_params()\n",
    "\n",
    "for i in range(20):\n",
    "    word = get_word()\n",
    "    s = f'name : {word}       | is in tr data: {word in names[:n_80]}'\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [C, W, bias, W2, W3, b2]\n",
    "names = ['C', 'W', 'bias', 'W2', 'W3', 'b2']\n",
    "param_dict = {}\n",
    "\n",
    "for name, param in zip(names, parameters):\n",
    "    param_dict[name] = param\n",
    "\n",
    "torch.save(param_dict, 'mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb       makemore_3.ipynb     names.txt\n",
      "makemore.ipynb       micrograd.ipynb\n",
      "makemore_2_mlp.ipynb mlp.pt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load('mlp.pt', weights_only=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQDGcWn074pA7NdAWB8vTp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env 28Sep)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
